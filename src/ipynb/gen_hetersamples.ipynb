{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 10:55:17,766 Note: NumExpr detected 56 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-10-21 10:55:17,767 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from lib.log import logger\n",
    "from utils import load_pickle, save_pickle, summarize_distribution, find_rt_bound\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个Sample子网中的每个用户随机生成其推文子网\n",
    "# NOTE: 对于不足N=20个推文邻居的用户节点, \n",
    "# 1) 取其所有的推文节点加入子网, 剩余不足的在最后用其他用户的剩余推文来补全;\n",
    "# 2) 如果此时仍然不够补全, 那么就舍弃这一用户子网(暂定)\n",
    "\n",
    "Ntimestages = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 13:29:35,950 tidx=0, len=1458\n",
      "2022-10-12 13:29:38,084 tidx=1, len=3025\n",
      "2022-10-12 13:29:42,386 tidx=2, len=5908\n",
      "2022-10-12 13:29:48,368 tidx=3, len=7939\n",
      "2022-10-12 13:29:53,162 tidx=4, len=6240\n",
      "2022-10-12 13:30:00,675 tidx=5, len=9509\n",
      "2022-10-12 13:30:11,960 tidx=6, len=14557\n",
      "2022-10-12 13:30:27,868 tidx=7, len=20605\n"
     ]
    }
   ],
   "source": [
    "for tidx in range(8):\n",
    "    hs_old = load_pickle(f\"../heter_samples_ratio1/{tidx}/heter_samples.p\")\n",
    "    logger.info(f\"tidx={tidx}, len={len(hs_old)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(Ntimestages):\n",
    "#     hs = load_pickle(f\"../heter_samples_ratio3/{idx}/heter_samples.p\")\n",
    "#     logger.info(f\"len={len(hs)}, bincount={np.bincount(hs.labels)}, class_weight={len(hs)/2/np.bincount(hs.labels)}\")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: 1:1\n",
    "2022-09-27 14:53:12,473 len=1458, bincount=[616 842], class_weight=[1.18344156 0.86579572]\n",
    "2022-09-27 14:53:13,098 len=3025, bincount=[1344 1681], class_weight=[1.12537202 0.89976205]\n",
    "2022-09-27 14:53:14,215 len=5908, bincount=[2686 3222], class_weight=[1.09977662 0.91682185]\n",
    "2022-09-27 14:53:15,656 len=7939, bincount=[3675 4264], class_weight=[1.08013605 0.9309334 ]\n",
    "2022-09-27 14:53:16,875 len=6240, bincount=[3193 3047], class_weight=[0.97713749 1.02395799]\n",
    "2022-09-27 14:53:18,682 len=9509, bincount=[5385 4124], class_weight=[0.88291551 1.15288555]\n",
    "2022-09-27 14:53:21,634 len=14557, bincount=[7816 6741], class_weight=[0.93123081 1.07973594]\n",
    "2022-09-27 14:53:28,913 len=20605, bincount=[10552 10053], class_weight=[0.97635519 1.02481846]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_ids Group by stage\n",
    "# diffusion_dict = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/ActionLog.p\")\n",
    "\n",
    "# upper_bs = {}\n",
    "# for hashtag, cascades in diffusion_dict.items():\n",
    "#     time_span = (cascades[-1][1]-cascades[0][1])/Ntimestages\n",
    "#     upper_bs[hashtag] = [i*time_span+cascades[0][1] for i in range(1,Ntimestages+1)]\n",
    "\n",
    "# graphid2tag = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/text/graphid2tag.p\")\n",
    "# graphid2timestamp = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/text/graphid2timestamp.p\")\n",
    "\n",
    "# text_ids = [set() for _ in range(Ntimestages)]\n",
    "# for text_id, tag in graphid2tag.items():\n",
    "#     timestamp = graphid2timestamp[text_id]\n",
    "#     idx = find_rt_bound(timestamp, upper_bs[tag])\n",
    "#     text_ids[idx].add(text_id)\n",
    "#     # print(f\"{timestamp:e}, {idx}, {upper_bs[tag]}\")\n",
    "# save_pickle(text_ids, \"/root/Lab_Related/data/Heter-GAT/Classic/text/textids_groupbystage.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: User can only contact those tweets within appropriate stages\n",
    "\n",
    "# user_texts = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/text/User-Text.p\")\n",
    "# textids_groupbystage = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/text/textids_groupbystage.p\")\n",
    "# text_ids = set()\n",
    "# ut_mp = [{} for _ in range(Ntimestages)]\n",
    "# for tidx in range(Ntimestages):\n",
    "#     text_ids |= textids_groupbystage[tidx] # accumulate seen text_ids by stage\n",
    "#     for user, texts in user_texts.items():\n",
    "#         ut_mp[tidx][user] = sorted(list(text_ids & set(list(texts.keys()))))\n",
    "# save_pickle(ut_mp, \"/root/Lab_Related/data/Heter-GAT/Classic/text/utmp_groupbystage.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build T->U mp\n",
    "# user_texts = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/text/User-Text.p\")\n",
    "\n",
    "# tweetid2userid_mp = {}\n",
    "# for user, texts in user_texts.items():\n",
    "#     for text_id in texts.keys():\n",
    "#         tweetid2userid_mp[text_id] = user\n",
    "# save_pickle(tweetid2userid_mp, \"/root/Lab_Related/data/Heter-GAT/Classic/text/tweetid2userid_mp.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 10:56:51,140 idx=10000, sample_ids=9430, tweet_ids=9430\n",
      "2022-10-21 10:57:04,385 idx=20000, sample_ids=18175, tweet_ids=18175\n",
      "2022-10-21 10:57:18,214 idx=30000, sample_ids=28041, tweet_ids=28041\n",
      "2022-10-21 10:57:30,377 idx=40000, sample_ids=37383, tweet_ids=37383\n",
      "2022-10-21 10:57:43,875 idx=50000, sample_ids=46835, tweet_ids=46835\n",
      "2022-10-21 10:57:57,680 idx=60000, sample_ids=56571, tweet_ids=56571\n",
      "2022-10-21 10:57:59,849 Finish Sampling Random Tweets... sample_ids=58185, tweet_ids=58185\n",
      "2022-10-21 10:58:18,422 Finish Saving pkl...\n"
     ]
    }
   ],
   "source": [
    "# Random Choose N=20 Tweets Per User\n",
    "import numpy as np\n",
    "from utils import SubGraphSample\n",
    "\n",
    "# from_dirname = \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20\"\n",
    "from_dirname = \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20\"\n",
    "# to_dirname   = \"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20\"\n",
    "to_dirname   = \"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20\"\n",
    "tweets_per_user = 5\n",
    "ut_mp = load_pickle(\"/root/data/HeterGAT/basic/text/utmp_groupbystage.p\")\n",
    "tweetid2userid_mp = load_pickle(\"/root/data/HeterGAT/basic/text/tweetid2userid_mp.p\")\n",
    "\n",
    "def read_subgraphsamples(samples_dirpath):\n",
    "    samples = SubGraphSample()\n",
    "    samples.adj_matrices = np.load(os.path.join(samples_dirpath, \"adjacency_matrix.npy\"))\n",
    "    samples.influence_features = np.load(os.path.join(samples_dirpath, \"influence_feature.npy\"))\n",
    "    samples.vertex_ids = np.load(os.path.join(samples_dirpath, \"vertex_id.npy\"))\n",
    "    samples.labels = np.load(os.path.join(samples_dirpath, \"label.npy\"))\n",
    "    samples.tags = np.load(os.path.join(samples_dirpath, \"hashtag.npy\"))\n",
    "    samples.time_stages = np.load(os.path.join(samples_dirpath, \"stage.npy\"))\n",
    "    return samples\n",
    "\n",
    "def gen_random_tweets(samples: SubGraphSample):\n",
    "    tweet_ids = []\n",
    "    sample_ids = []\n",
    "\n",
    "    for idx in range(len(samples.labels)):\n",
    "        if idx and idx % 10000 == 0:\n",
    "            logger.info(f\"idx={idx}, sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "        stage = samples.time_stages[idx]\n",
    "        selected_tweet_ids  = set()\n",
    "        candidate_tweet_ids = set()\n",
    "        for vertex_id in samples.vertex_ids[idx]:\n",
    "            available_tweet_ids = ut_mp[stage][vertex_id]\n",
    "            random_ids = np.random.choice(available_tweet_ids, size=min(tweets_per_user, len(available_tweet_ids)), replace=False)\n",
    "            selected_tweet_ids  |= set(random_ids)\n",
    "            candidate_tweet_ids |= set(available_tweet_ids)-set(random_ids)\n",
    "        candidate_tweet_ids -= selected_tweet_ids\n",
    "        # logger.info(f\"Length: sample={len(selected_tweet_ids)}, remain={len(candidate_tweet_ids)}, expected={len(samples.vertex_ids[idx])*tweets_per_user}\")\n",
    "\n",
    "        if len(selected_tweet_ids) != len(samples.vertex_ids[idx])*tweets_per_user:\n",
    "            diff = len(samples.vertex_ids[idx])*tweets_per_user - len(selected_tweet_ids)\n",
    "            if diff > len(candidate_tweet_ids):\n",
    "                continue\n",
    "            selected_tweet_ids |= set(np.random.choice(list(candidate_tweet_ids), size=diff, replace=False))\n",
    "        sample_ids.append(idx)\n",
    "        tweet_ids.append(selected_tweet_ids)\n",
    "    logger.info(f\"Finish Sampling Random Tweets... sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "\n",
    "    os.makedirs(to_dirname, exist_ok=True)\n",
    "    selected_samples = SubGraphSample(\n",
    "        adj_matrices=samples.adj_matrices[sample_ids],\n",
    "        influence_features=samples.influence_features[sample_ids],\n",
    "        vertex_ids=samples.vertex_ids[sample_ids],\n",
    "        labels=samples.labels[sample_ids],\n",
    "        tags=samples.tags[sample_ids],\n",
    "        time_stages=samples.time_stages[sample_ids],\n",
    "    )\n",
    "    save_pickle(sample_ids, f\"{to_dirname}/sample_ids.p\")\n",
    "    save_pickle(tweet_ids, f\"{to_dirname}/tweet_ids.p\")\n",
    "    save_pickle(selected_samples, f\"{to_dirname}/selected_samples.p\")\n",
    "    logger.info(\"Finish Saving pkl...\")\n",
    "\n",
    "# for tidx in range(Ntimestages):\n",
    "#     samples = read_subgraphsamples(f\"{sp_dirname}/{tidx}/\")\n",
    "#     gen_random_tweets(samples)\n",
    "\n",
    "samples = read_subgraphsamples(samples_dirpath=from_dirname)\n",
    "gen_random_tweets(samples=samples)\n",
    "\n",
    "\"\"\"\n",
    "2022-10-21 10:56:51,140 idx=10000, sample_ids=9430, tweet_ids=9430\n",
    "2022-10-21 10:57:04,385 idx=20000, sample_ids=18175, tweet_ids=18175\n",
    "2022-10-21 10:57:18,214 idx=30000, sample_ids=28041, tweet_ids=28041\n",
    "2022-10-21 10:57:30,377 idx=40000, sample_ids=37383, tweet_ids=37383\n",
    "2022-10-21 10:57:43,875 idx=50000, sample_ids=46835, tweet_ids=46835\n",
    "2022-10-21 10:57:57,680 idx=60000, sample_ids=56571, tweet_ids=56571\n",
    "2022-10-21 10:57:59,849 Finish Sampling Random Tweets... sample_ids=58185, tweet_ids=58185\n",
    "2022-10-21 10:58:18,422 Finish Saving pkl...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: 1:1\n",
    "2022-09-26 20:37:04,459 Length: users=9482, tweets=1458\n",
    "2022-09-26 20:37:27,938 Length: users=5201, tweets=3025\n",
    "2022-09-26 20:38:04,250 Length: users=7358, tweets=5908\n",
    "2022-09-26 20:39:01,396 Length: users=8091, tweets=7939\n",
    "2022-09-26 20:40:06,630 Length: users=6342, tweets=6240\n",
    "2022-09-26 20:41:22,722 Length: users=9583, tweets=9509\n",
    "2022-09-26 20:43:33,540 Length: users=14592, tweets=14557\n",
    "2022-09-26 20:47:39,127 Length: users=20605, tweets=20605\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "NOTE: 1:3\n",
    "2022-09-27 10:50:53,798 Length: users=18071, tweets=2705\n",
    "2022-09-27 10:51:22,941 Length: users=9698, tweets=5633\n",
    "2022-09-27 10:52:14,377 Length: users=13823, tweets=11092\n",
    "2022-09-27 10:53:32,810 Length: users=15253, tweets=14905\n",
    "2022-09-27 10:54:58,749 Length: users=12441, tweets=12184\n",
    "2022-09-27 10:56:45,734 Length: users=19229, tweets=19051\n",
    "2022-09-27 10:59:52,396 Length: users=29467, tweets=29380\n",
    "2022-09-27 11:05:41,109 Length: users=41437, tweets=41436\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "NOTE: stages_lab_try2_inf_199_898012_deg_18_9265_ego_50_neg_1_restart_20, ratio=1:1\n",
    "2022-10-10 09:40:33,418 idx=10000, tweet_ids len=9209\n",
    "2022-10-10 09:41:09,871 idx=20000, tweet_ids len=18038\n",
    "2022-10-10 09:42:02,105 idx=30000, tweet_ids len=27475\n",
    "2022-10-10 09:42:42,801 idx=40000, tweet_ids len=36228\n",
    "2022-10-10 09:43:25,821 idx=50000, tweet_ids len=44930\n",
    "2022-10-10 09:43:57,018 idx=60000, tweet_ids len=52328\n",
    "2022-10-10 09:44:37,991 idx=70000, tweet_ids len=62225\n",
    "2022-10-10 09:45:05,098 idx=80000, tweet_ids len=69763\n",
    "2022-10-10 09:45:50,042 idx=90000, tweet_ids len=79647\n",
    "2022-10-10 09:46:18,392 idx=100000, tweet_ids len=87068\n",
    "2022-10-10 09:47:12,400 idx=110000, tweet_ids len=96583\n",
    "2022-10-10 09:48:01,686 idx=120000, tweet_ids len=106062\n",
    "2022-10-10 09:48:36,812 idx=130000, tweet_ids len=113834\n",
    "2022-10-10 09:49:28,524 idx=140000, tweet_ids len=123752\n",
    "2022-10-10 09:50:05,030 idx=150000, tweet_ids len=132095\n",
    "2022-10-10 09:50:46,975 idx=160000, tweet_ids len=141435\n",
    "2022-10-10 09:51:23,820 idx=170000, tweet_ids len=150030\n",
    "2022-10-10 09:52:18,778 idx=180000, tweet_ids len=159629\n",
    "2022-10-10 09:52:44,353 idx=190000, tweet_ids len=167512\n",
    "2022-10-10 09:53:24,533 idx=200000, tweet_ids len=175910\n",
    "2022-10-10 09:54:02,472 idx=210000, tweet_ids len=185350\n",
    "2022-10-10 09:55:04,512 idx=220000, tweet_ids len=195349\n",
    "2022-10-10 09:55:44,417 idx=230000, tweet_ids len=203606\n",
    "2022-10-10 09:56:19,747 idx=240000, tweet_ids len=210787\n",
    "2022-10-10 09:57:00,054 idx=250000, tweet_ids len=219523\n",
    "2022-10-10 09:57:36,717 idx=260000, tweet_ids len=227820\n",
    "2022-10-10 09:58:16,037 idx=270000, tweet_ids len=236144\n",
    "2022-10-10 09:58:52,221 idx=280000, tweet_ids len=242976\n",
    "2022-10-10 09:59:22,469 idx=290000, tweet_ids len=251292\n",
    "2022-10-10 10:00:02,073 Length: users=297006, tweets=258046\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "NOTE: stages_lab_try2_inf_199_898012_deg_18_9265_ego_50_neg_1_restart_20, ratio=1:1, only stage-7\n",
    "2022-10-10 13:46:45,260 idx=40000, tweet_ids len=12203\n",
    "2022-10-10 13:47:21,965 idx=70000, tweet_ids len=18090\n",
    "2022-10-10 13:47:42,965 idx=90000, tweet_ids len=21608\n",
    "2022-10-10 13:48:08,042 idx=110000, tweet_ids len=25582\n",
    "2022-10-10 13:48:20,330 idx=120000, tweet_ids len=27482\n",
    "2022-10-10 13:49:10,263 idx=160000, tweet_ids len=35467\n",
    "2022-10-10 13:50:20,411 idx=220000, tweet_ids len=46816\n",
    "2022-10-10 13:51:47,310 Length: users=297006, tweets=60957\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 11:04:19,113 Finish Loading Data...\n",
      "2022-10-21 11:04:24,026 idx=1000\n",
      "2022-10-21 11:04:25,830 idx=2000\n",
      "2022-10-21 11:04:27,537 idx=3000\n",
      "2022-10-21 11:04:29,272 idx=4000\n",
      "2022-10-21 11:04:30,942 idx=5000\n",
      "2022-10-21 11:04:32,671 idx=6000\n",
      "2022-10-21 11:04:34,358 idx=7000\n",
      "2022-10-21 11:04:36,023 idx=8000\n",
      "2022-10-21 11:04:37,700 idx=9000\n",
      "2022-10-21 11:04:39,386 idx=10000\n",
      "2022-10-21 11:04:41,025 idx=11000\n",
      "2022-10-21 11:04:42,695 idx=12000\n",
      "2022-10-21 11:04:44,403 idx=13000\n",
      "2022-10-21 11:04:46,104 idx=14000\n",
      "2022-10-21 11:04:47,803 idx=15000\n",
      "2022-10-21 11:04:49,499 idx=16000\n",
      "2022-10-21 11:04:51,180 idx=17000\n",
      "2022-10-21 11:04:52,868 idx=18000\n",
      "2022-10-21 11:04:54,527 idx=19000\n",
      "2022-10-21 11:04:56,185 idx=20000\n",
      "2022-10-21 11:04:57,879 idx=21000\n",
      "2022-10-21 11:04:59,537 idx=22000\n",
      "2022-10-21 11:05:01,207 idx=23000\n",
      "2022-10-21 11:05:02,877 idx=24000\n",
      "2022-10-21 11:05:04,523 idx=25000\n",
      "2022-10-21 11:05:06,210 idx=26000\n",
      "2022-10-21 11:05:16,612 idx=27000\n",
      "2022-10-21 11:05:18,293 idx=28000\n",
      "2022-10-21 11:05:19,963 idx=29000\n",
      "2022-10-21 11:05:21,620 idx=30000\n",
      "2022-10-21 11:05:23,263 idx=31000\n",
      "2022-10-21 11:05:24,928 idx=32000\n",
      "2022-10-21 11:05:26,593 idx=33000\n",
      "2022-10-21 11:05:28,339 idx=34000\n",
      "2022-10-21 11:05:30,078 idx=35000\n",
      "2022-10-21 11:05:31,730 idx=36000\n",
      "2022-10-21 11:05:33,499 idx=37000\n",
      "2022-10-21 11:05:35,139 idx=38000\n",
      "2022-10-21 11:05:36,934 idx=39000\n",
      "2022-10-21 11:05:38,610 idx=40000\n",
      "2022-10-21 11:05:40,246 idx=41000\n",
      "2022-10-21 11:05:41,874 idx=42000\n",
      "2022-10-21 11:05:43,507 idx=43000\n",
      "2022-10-21 11:05:45,213 idx=44000\n",
      "2022-10-21 11:05:46,846 idx=45000\n",
      "2022-10-21 11:05:48,762 idx=46000\n",
      "2022-10-21 11:05:50,464 idx=47000\n",
      "2022-10-21 11:05:52,165 idx=48000\n",
      "2022-10-21 11:05:53,836 idx=49000\n",
      "2022-10-21 11:05:55,506 idx=50000\n",
      "2022-10-21 11:05:57,179 idx=51000\n",
      "2022-10-21 11:05:58,851 idx=52000\n",
      "2022-10-21 11:06:00,592 idx=53000\n",
      "2022-10-21 11:06:02,449 idx=54000\n",
      "2022-10-21 11:06:04,183 idx=55000\n",
      "2022-10-21 11:06:05,856 idx=56000\n",
      "2022-10-21 11:06:07,600 idx=57000\n",
      "2022-10-21 11:06:09,276 idx=58000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9718/2194956124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mhetersamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeterSubGraphSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mextend_adj_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetersamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mextend_initial_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetersamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Extend Vertex Indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9718/2194956124.py\u001b[0m in \u001b[0;36mextend_initial_features\u001b[0;34m(samples, heter_samples)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Build User Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# initial_features(user) = influence_features||Gra(hashtag,stage)||Exp(hashtag,stage)||Cas(hashtag,stage), dim=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mgra_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/root/data/HeterGAT/user_features/norm_gravity_feature/hashtag{hashtags[idx]}_t{stage}.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mexp_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/root/data/HeterGAT/user_features/norm_exptime_feature1/hashtag{hashtags[idx]}_t{stage}.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mcas_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/root/data/HeterGAT/user_features/norm_ce_feature/hashtag{hashtags[idx]}_t{stage}.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adj -> Heter-Adj\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from utils import SubGraphSample, HeterSubGraphSample\n",
    "import copy\n",
    "from scipy import sparse\n",
    "\n",
    "# from_dirname = \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20\"\n",
    "from_dirname = \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20\"\n",
    "# to_dirname   = \"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20\"\n",
    "to_dirname   = \"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20\"\n",
    "tweetid2userid_mp = load_pickle(\"/root/data/HeterGAT/basic/text/tweetid2userid_mp.p\")\n",
    "tweet_feat = np.array(load_pickle(\"/root/data/HeterGAT/basic/doc2topic_tweetfeat.p\"))\n",
    "tot_users_nb = 208894\n",
    "logger.info(\"Finish Loading Data...\")\n",
    "\n",
    "def extend_adj_matrices(samples: SubGraphSample, tweet_ids: List[List[int]], heter_samples: HeterSubGraphSample):\n",
    "    adj_matrices = samples.adj_matrices\n",
    "    vertex_ids   = samples.vertex_ids\n",
    "\n",
    "    for idx in range(len(samples.labels)):\n",
    "        if idx and idx % 1000 == 0:\n",
    "            logger.info(f\"idx={idx}\")\n",
    "        extend_subnetwork = np.array(\n",
    "            np.concatenate((vertex_ids[idx], np.array(list(tweet_ids[idx])))), \n",
    "            dtype=int\n",
    "        )\n",
    "        heter_samples.vertex_ids.append(extend_subnetwork) # (N,)\n",
    "        subnetwork_size, user_ids_nb = len(extend_subnetwork), len(vertex_ids[idx])\n",
    "        elem_idx_mp = {elem:idx for idx,elem in enumerate(extend_subnetwork)}\n",
    "\n",
    "        # Build U-U, U-T adj-matrices, respectively\n",
    "        heter_adj_matrices = []\n",
    "        uu_matrices = np.array([[0]*subnetwork_size for _ in range(subnetwork_size)], dtype=int)\n",
    "        uu_matrices[:user_ids_nb,:user_ids_nb] = adj_matrices[idx]\n",
    "        heter_adj_matrices.append(sparse.csr_matrix(uu_matrices)) # toarray()\n",
    "\n",
    "        # Get Corresponding User_id By Tweet_id, and then convert them into indexes in extend_subnetwork\n",
    "        ut_matrices = copy.deepcopy(uu_matrices)\n",
    "        for tweet_id in tweet_ids[idx]:\n",
    "            user_id = tweetid2userid_mp[tweet_id]\n",
    "            net_userid = elem_idx_mp[user_id]\n",
    "            net_tweetid = elem_idx_mp[tweet_id]\n",
    "            ut_matrices[net_userid][net_tweetid] = 1\n",
    "        heter_adj_matrices.append(sparse.csr_matrix(ut_matrices)) # toarray()\n",
    "        del uu_matrices, ut_matrices\n",
    "\n",
    "        heter_samples.heter_adj_matrices.append(heter_adj_matrices) # (|Rs|,N,N)\n",
    "    \n",
    "    heter_samples.labels = samples.labels\n",
    "\n",
    "def extend_initial_features(samples: SubGraphSample, heter_samples: HeterSubGraphSample):\n",
    "    influence_features = samples.influence_features\n",
    "    stage = samples.time_stages[0]\n",
    "    hashtags = samples.tags\n",
    "\n",
    "    for idx in range(len(samples)):\n",
    "        vertex_ids = heter_samples.vertex_ids[idx]\n",
    "        # Build User Features\n",
    "        # initial_features(user) = influence_features||Gra(hashtag,stage)||Exp(hashtag,stage)||Cas(hashtag,stage), dim=2\n",
    "        gra_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_gravity_feature/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "        exp_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_exptime_feature1/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "        cas_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_ce_feature/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "        user_ids = samples.vertex_ids[idx]\n",
    "        user_features = np.concatenate((influence_features[idx], gra_feat[user_ids], exp_feat[user_ids], cas_feat[user_ids]), axis=1)\n",
    "\n",
    "        # Build Tweet Features\n",
    "        tweet_ids = vertex_ids[len(user_ids):]\n",
    "        tweet_features = tweet_feat[tweet_ids]\n",
    "\n",
    "        # Extend Two Features with Padding zeros\n",
    "        initial_features = np.concatenate((\n",
    "            np.append(user_features,  [[0]*tweet_features.shape[1] for _ in range(user_features.shape[0])],  1),\n",
    "            np.append([[0]*user_features.shape[1]  for _ in range(tweet_features.shape[0])], tweet_features, 1)\n",
    "        ), axis=0)\n",
    "        heter_samples.initial_features.append(initial_features)\n",
    "\n",
    "# Main Solution\n",
    "tweetids = load_pickle(f\"{to_dirname}/tweet_ids.p\")\n",
    "samples  = load_pickle(f\"{to_dirname}/selected_samples.p\")\n",
    "hetersamples = HeterSubGraphSample()\n",
    "extend_adj_matrices(samples, tweetids, hetersamples)\n",
    "extend_initial_features(samples, hetersamples)\n",
    "\n",
    "# Extend Vertex Indices\n",
    "for idx in range(len(hetersamples)):\n",
    "    vertex_ids = hetersamples.vertex_ids[idx]\n",
    "    hetersamples.vertex_ids[idx] = [index if index<tot_users_nb else index+tot_users_nb for index in vertex_ids]\n",
    "\n",
    "# \n",
    "hetersamples.heter_adj_matrices = np.array(hetersamples.heter_adj_matrices)\n",
    "hetersamples.initial_features = np.array(hetersamples.initial_features)\n",
    "hetersamples.vertex_ids = np.array(hetersamples.vertex_ids)\n",
    "hetersamples.labels = np.array(hetersamples.labels)\n",
    "\n",
    "save_pickle(hetersamples, f\"{to_dirname}/heter_samples.p\")\n",
    "\n",
    "# for tidx in range(Ntimestages):\n",
    "#     tweetids = load_pickle(f\"{to_dirname}/{tidx}/tweet_ids.p\")\n",
    "#     samples  = load_pickle(f\"{to_dirname}/{tidx}/samples.p\")\n",
    "#     hetersamples = HeterSubGraphSample()\n",
    "#     extend_adj_matrices(samples, tweetids, hetersamples)\n",
    "#     extend_initial_features(samples, hetersamples)\n",
    "\n",
    "#     # Extend Vertex Indices\n",
    "#     for idx in range(len(hetersamples)):\n",
    "#         vertex_ids = hetersamples.vertex_ids[idx]\n",
    "#         hetersamples.vertex_ids[idx] = [index if index<tot_users_nb else index+tot_users_nb for index in vertex_ids]\n",
    "    \n",
    "#     # \n",
    "#     hetersamples.heter_adj_matrices = np.array(hetersamples.heter_adj_matrices)\n",
    "#     hetersamples.initial_features = np.array(hetersamples.initial_features)\n",
    "#     hetersamples.vertex_ids = np.array(hetersamples.vertex_ids)\n",
    "#     hetersamples.labels = np.array(hetersamples.labels)\n",
    "\n",
    "#     # save_pickle(hetersamples, f\"../{hetersp_dirname}/{tidx}/heter_samples.p\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Add initial_features in HeterSubGraphSample\n",
    "\n",
    "# import numpy as np\n",
    "# from utils import SubGraphSample, HeterSubGraphSample\n",
    "\n",
    "# tweet_feat = np.array(load_pickle(\"heter-training/doc2topic_tweetfeat.p\"))\n",
    "\n",
    "# def extend_initial_features(samples: SubGraphSample, heter_samples: HeterSubGraphSample):\n",
    "#     influence_features = samples.influence_features\n",
    "#     stage = samples.time_stages[0]\n",
    "#     hashtags = samples.tags\n",
    "\n",
    "#     if len(heter_samples.initial_features):\n",
    "#         heter_samples.initial_features = []\n",
    "\n",
    "#     for idx in range(len(samples)):\n",
    "#         vertex_ids = heter_samples.vertex_ids[idx]\n",
    "#         # Build User Features\n",
    "#         # initial_features(user) = influence_features||Gra(hashtag,stage)||Exp(hashtag,stage)||Cas(hashtag,stage), dim=2\n",
    "#         gra_feat = np.array(load_pickle(f\"user_features/norm_gravity_feature/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "#         exp_feat = np.array(load_pickle(f\"user_features/norm_exptime_feature1/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "#         cas_feat = np.array(load_pickle(f\"user_features/norm_ce_feature/hashtag{hashtags[idx]}_t{stage}.p\")).reshape(-1,1)\n",
    "#         user_ids = samples.vertex_ids[idx]\n",
    "#         user_features = np.concatenate((influence_features[idx], gra_feat[user_ids], exp_feat[user_ids], cas_feat[user_ids]), axis=1)\n",
    "\n",
    "#         # Build Tweet Features\n",
    "#         tweet_ids = vertex_ids[len(user_ids):]\n",
    "#         tweet_features = tweet_feat[tweet_ids]\n",
    "\n",
    "#         # Extend Two Features with Padding zeros\n",
    "#         initial_features = np.concatenate((\n",
    "#             np.append(user_features,  [[0]*tweet_features.shape[1] for _ in range(user_features.shape[0])],  1),\n",
    "#             np.append([[0]*user_features.shape[1]  for _ in range(tweet_features.shape[0])], tweet_features, 1)\n",
    "#         ), axis=0)\n",
    "#         heter_samples.initial_features.append(initial_features)\n",
    "    \n",
    "# for tidx in range(Ntimestages):\n",
    "#     samples = load_pickle(f\"heter_samples/{tidx}/samples.p\")\n",
    "#     heter_samples = load_pickle(f\"heter_samples/{tidx}/heter_samples_extendhadj.p\")\n",
    "#     extend_initial_features(samples, heter_samples)\n",
    "#     save_pickle(heter_samples, f\"heter_samples/{tidx}/heter_samples_extendfeat.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extend Hadj to Matrices\n",
    "# # 208894\n",
    "# import numpy as np\n",
    "\n",
    "# Ntimestages = 8\n",
    "# for tidx in range(Ntimestages):\n",
    "#     heter_samples = load_pickle(f\"heter_samples/{tidx}/heter_samples_extendfeat.p\")\n",
    "\n",
    "#     for idx in range(len(heter_samples)):\n",
    "#         vertex_ids = heter_samples.vertex_ids[idx]\n",
    "#         extended_vertex_ids = np.concatenate((vertex_ids[:50], np.array([elem+208894 for elem in vertex_ids[50:]])), axis=0)\n",
    "#         heter_samples.vertex_ids[idx] = extended_vertex_ids\n",
    "\n",
    "#     save_pickle(heter_samples, f\"heter_samples/{tidx}/heter_samples_extendidx.p\")\n",
    "\n",
    "# # NOTE: Time spent: 20min42.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(Ntimestages):\n",
    "#     heter_samples = load_pickle(f\"heter_samples/{idx}/heter_samples_extendidx.p\")\n",
    "#     heter_samples.heter_adj_matrices = np.array(heter_samples.heter_adj_matrices)\n",
    "#     heter_samples.initial_features = np.array(heter_samples.initial_features)\n",
    "#     heter_samples.vertex_ids = np.array(heter_samples.vertex_ids)\n",
    "#     save_pickle(heter_samples, f\"heter_samples/{idx}/heter_samples.p\")\n",
    "\n",
    "# NOTE: Time spent: 26min28.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Convert to Tensors\n",
    "\n",
    "# import torch\n",
    "\n",
    "# Ntimestages = 8\n",
    "# for tidx in range(Ntimestages):\n",
    "#     heter_samples = load_pickle(f\"heter_samples/{tidx}/heter_samples.p\")\n",
    "#     heter_samples.vertex_ids = torch.Tensor(heter_samples.vertex_ids)\n",
    "#     heter_samples.labels = torch.Tensor(heter_samples.labels)\n",
    "#     heter_samples.initial_features = torch.Tensor(heter_samples.initial_features)\n",
    "#     heter_samples.heter_adj_matrices = torch.stack([\n",
    "#         torch.stack([get_sparse_tensor(adj.tocoo()) for adj in heter_samples.heter_adj_matrices[:,hadj_idx]])\n",
    "#     for hadj_idx in range(heter_samples.heter_adj_matrices.shape[1])])\n",
    "\n",
    "#     save_pickle(heter_samples, f\"heter_samples/{tidx}/heter_samples_tensor.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [132641, 58655, 57581, 57830, 58903, 75070, 100318, 147189]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
