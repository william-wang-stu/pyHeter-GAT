{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\\n方法:\\n1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\\n2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\\n   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\\n3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\n",
    "方法:\n",
    "1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\n",
    "2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\n",
    "   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\n",
    "3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 10:11:03,676 Reading From config.ini... DATA_ROOTPATH=/remote-home/share/dmb_nas/wangzejian/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from lib.log import logger\n",
    "from lib.utils import get_sparse_tensor\n",
    "from utils import load_pickle, save_pickle, ChunkSampler, SubGraphSample, load_w2v_feature, sample_tweets_around_user, summarize_distribution\n",
    "from model import BatchdenseGAT, HeterdenseGAT\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from scipy import sparse\n",
    "from scipy import io as sio\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve\n",
    "from tensorboard_logger import tensorboard_logger\n",
    "from torch.utils.data import Dataset\n",
    "import configparser\n",
    "from dgl.data.utils import download, get_download_dir, _get_dgl_url\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import dgl\n",
    "import errno\n",
    "import pickle\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "DATA_ROOTPATH = config['DEFAULT']['DataRootPath']\n",
    "logger.info(f\"Reading From config.ini... DATA_ROOTPATH={DATA_ROOTPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deg_le483_df.p\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 22,\n",
       " 32,\n",
       " 33,\n",
       " 40,\n",
       " 43,\n",
       " 47,\n",
       " 69,\n",
       " 71,\n",
       " 80,\n",
       " 97,\n",
       " 99,\n",
       " 111,\n",
       " 126,\n",
       " 129,\n",
       " 140,\n",
       " 148,\n",
       " 151,\n",
       " 158,\n",
       " 164,\n",
       " 183,\n",
       " 189,\n",
       " 190,\n",
       " 197,\n",
       " 200,\n",
       " 203,\n",
       " 206,\n",
       " 210,\n",
       " 213,\n",
       " 263,\n",
       " 270,\n",
       " 305,\n",
       " 322,\n",
       " 329,\n",
       " 389,\n",
       " 389,\n",
       " 404,\n",
       " 425,\n",
       " 453,\n",
       " 478,\n",
       " 523,\n",
       " 538,\n",
       " 543,\n",
       " 623,\n",
       " 700,\n",
       " 743,\n",
       " 764,\n",
       " 778,\n",
       " 812,\n",
       " 819,\n",
       " 862,\n",
       " 1020,\n",
       " 1051,\n",
       " 1068,\n",
       " 1163,\n",
       " 1310,\n",
       " 1350,\n",
       " 1399,\n",
       " 1411,\n",
       " 1672,\n",
       " 1914,\n",
       " 2053,\n",
       " 2147,\n",
       " 2148,\n",
       " 2344,\n",
       " 2502,\n",
       " 3004,\n",
       " 3077,\n",
       " 3092,\n",
       " 3298,\n",
       " 3372,\n",
       " 3396,\n",
       " 3440,\n",
       " 5437,\n",
       " 5680,\n",
       " 6177,\n",
       " 7603,\n",
       " 8451,\n",
       " 8523,\n",
       " 10211,\n",
       " 10616,\n",
       " 10749,\n",
       " 10995,\n",
       " 13013,\n",
       " 13913,\n",
       " 14633,\n",
       " 15504,\n",
       " 15979,\n",
       " 18446,\n",
       " 20835,\n",
       " 21219,\n",
       " 23519,\n",
       " 28042,\n",
       " 30099,\n",
       " 30572,\n",
       " 31002,\n",
       " 33628,\n",
       " 34963,\n",
       " 36865,\n",
       " 38463,\n",
       " 40179,\n",
       " 59448,\n",
       " 59844,\n",
       " 62997,\n",
       " 66266,\n",
       " 67259,\n",
       " 69818,\n",
       " 74111,\n",
       " 74434,\n",
       " 82447,\n",
       " 105756,\n",
       " 117578,\n",
       " 118382,\n",
       " 140665,\n",
       " 177412,\n",
       " 188006,\n",
       " 213100,\n",
       " 281817,\n",
       " 403659,\n",
       " 459649,\n",
       " 480936,\n",
       " 1021587,\n",
       " 1117768,\n",
       " 1146336,\n",
       " 1342886,\n",
       " 1468046]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [len(elem) for elem in df.values()]\n",
    "sorted(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()\n",
    "\n",
    "def load_labels(args, hashtag, g, df):\n",
    "    labels = gen_labels(hashtag=hashtag, g=g, df=df)\n",
    "\n",
    "    float_mask = np.zeros(len(labels))\n",
    "    for label in [-1,1]:\n",
    "        ids = np.where(labels == label)[0]\n",
    "        if args.shuffle:\n",
    "            float_mask[ids] = np.random.permutation(np.linspace(1e-10,1,len(ids)))\n",
    "        else:\n",
    "            float_mask[ids] = np.linspace(1e-10,1,len(ids))\n",
    "    \n",
    "    train_ids = np.where((float_mask>0) & (float_mask<=args.train_ratio/100))[0]\n",
    "    val_ids   = np.where((float_mask>args.train_ratio/100) & (float_mask<=(args.train_ratio+args.valid_ratio)/100))[0]\n",
    "    test_ids  = np.where(float_mask>(args.train_ratio+args.valid_ratio)/100)[0]\n",
    "    logger.info(f\"train/valid/test={len(train_ids)},{len(val_ids)},{len(test_ids)}\")\n",
    "\n",
    "    num_user = g.vcount()\n",
    "    train_mask = get_binary_mask(num_user, train_ids)\n",
    "    val_mask   = get_binary_mask(num_user, val_ids)\n",
    "    test_mask  = get_binary_mask(num_user, test_ids)\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = train_mask.bool()\n",
    "        val_mask = val_mask.bool()\n",
    "        test_mask = test_mask.bool()\n",
    "    \n",
    "    labels[labels==-1] = 0\n",
    "    nb_classes = np.unique(labels).shape[0]\n",
    "    class_weight = torch.FloatTensor(len(labels) / (nb_classes*np.bincount(labels))) if args.class_weight_balanced else torch.ones(nb_classes)\n",
    "    \n",
    "    return labels, train_mask, val_mask, test_mask, nb_classes, class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 14:17:25,233 186\n"
     ]
    }
   ],
   "source": [
    "for hashtag in df:\n",
    "    logger.info(hashtag)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vcount = g.vcount()\n",
    "stage = 7\n",
    "\n",
    "labels = torch.zeros(graph_vcount)\n",
    "labels[list(pos_users[stage])] =  1\n",
    "arr1 = labels.int().numpy()\n",
    "logger.info(np.bincount(arr1))\n",
    "\n",
    "labels[list(neg_users[stage])] = -1\n",
    "\n",
    "arr1 = labels.int().numpy()\n",
    "logger.info(np.bincount(np.where(arr1==-1, 2, arr1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  957,   147, 43792])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = labels.int().numpy()\n",
    "np.bincount(np.where(arr1==-1, 2, arr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: tweets group by hashtag\n",
    "\n",
    "graphid2tag = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/graphid2tag.p\"))\n",
    "\n",
    "ts = [set() for _ in range(200)]\n",
    "for gid, tag in graphid2tag.items():\n",
    "    ts[tag].add(gid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: 目标是构建Hadjs和Feats, 同时生成必要的labels(for-loss)\n",
    "\n",
    "# # Total 44896 User Nodes\n",
    "# subgraph_deg483 = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deg_le483_subgraph.p\"))\n",
    "\n",
    "# # Find Tweet Nodes for each User Node\n",
    "# #   P.S. Dont Use Sampling, since we are not constructing subnetworks\n",
    "# # TODO: use text/utmp_groupbystage.p instead\n",
    "\n",
    "# ut_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/usertweet_mp.p\"))\n",
    "\n",
    "# # NOTE: Choose Part of the Tweets, 1kw is toooooo large!\n",
    "# max_user_tweets = 40\n",
    "# tweet_nodes = []\n",
    "# ut_edges    = []\n",
    "# for user in subgraph_deg483.vs[\"label\"]:\n",
    "#     selected_tweets = random.choices(ut_mp[user], k=min(len(ut_mp[user]), max_user_tweets))\n",
    "#     tweet_nodes.extend(selected_tweets)\n",
    "#     for tweet in selected_tweets:\n",
    "#         ut_edges.append((user, tweet))\n",
    "# logger.info(f\"Tweet Nodes={len(tweet_nodes)}, Edges={len(ut_edges)}\")\n",
    "\n",
    "# nodes = {}\n",
    "# node_indices = 0\n",
    "# # Users: 44896, Tweets: 10008103, Total: 10052999\n",
    "# for node in subgraph_deg483.vs[\"label\"]+[tweet+208894 for tweet in tweet_nodes]:\n",
    "#     nodes[node] = node_indices\n",
    "#     node_indices += 1\n",
    "\n",
    "# edges = [[], []]\n",
    "# for uu_edge in subgraph_deg483.es:\n",
    "#     source, target = subgraph_deg483.vs[uu_edge.source][\"label\"], subgraph_deg483.vs[uu_edge.target][\"label\"]\n",
    "#     edges[0].append([nodes[source], nodes[target]])\n",
    "\n",
    "# for from_, to_ in ut_edges:\n",
    "#     edges[1].append([nodes[from_], nodes[to_+208894]])\n",
    "\n",
    "# # Add self-loops\n",
    "# for node in range(len(subgraph_deg483.vs[\"label\"])):\n",
    "#     edges[0].append([node, node])\n",
    "# for node in range(len(subgraph_deg483.vs[\"label\"]), node_indices):\n",
    "#     edges[1].append([node,node])\n",
    "\n",
    "# logger.info(f\"{len(edges[0])}, {len(edges[1])}\")\n",
    "# # 2022-10-27 11:13:49,333 480540, 10008103\n",
    "# # 2022-10-27 11:14:40,915 525436, 20016206\n",
    "# # NOTE: 2022-10-27 13:54:47,112 525436, 7320540\n",
    "\n",
    "# def create_sparsemat_from_edgelist(edgelist, m, n):\n",
    "#     rows, cols = edgelist[:,0], edgelist[:,1]\n",
    "#     ones = np.ones(len(rows), np.uint8)\n",
    "#     mat = sparse.coo_matrix((ones, (rows, cols)), shape=(m, n))\n",
    "#     return mat.tocsr()\n",
    "\n",
    "# uu_mat = create_sparsemat_from_edgelist(np.array(edges[0]), node_indices, node_indices)\n",
    "# ut_mat = create_sparsemat_from_edgelist(np.array(edges[1]), node_indices, node_indices)\n",
    "# hadjs = [uu_mat, ut_mat]\n",
    "# save_pickle(hadjs, os.path.join(DATA_ROOTPATH, f\"HeterGAT/basic/deg_le483_hadjs_selfloop_max{max_user_tweets}tweet.p\"))\n",
    "\n",
    "# # TODO: vertices, stages, tags -> user_features[*]\n",
    "# # user_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/user_features/user_features.p\"))\n",
    "\n",
    "# user_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/user_features/user_features_avg.p\"))\n",
    "# deepwalk_feats = load_w2v_feature(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deepwalk/deepwalk_added.emb_64\"), 208894)\n",
    "# tweet_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/doc2topic_tweetfeat.p\"))\n",
    "# user_feats = np.concatenate((user_features[subgraph_deg483.vs[\"label\"]], deepwalk_feats[subgraph_deg483.vs[\"label\"]]), axis=1) \n",
    "# tweet_feats = tweet_features[tweet_nodes]\n",
    "# # logger.info(f\"{user_feats.shape}, {tweet_feats.shape}\")\n",
    "\n",
    "# feats = np.concatenate((\n",
    "#     np.append(user_feats, np.zeros(shape=(user_feats.shape[0], tweet_feats.shape[1])),  axis=1), \n",
    "#     np.append(np.zeros(shape=(tweet_feats.shape[0], user_feats.shape[1])), tweet_feats, axis=1), \n",
    "# ), axis=0)\n",
    "# logger.info(feats.shape)\n",
    "# save_pickle(feats, os.path.join(DATA_ROOTPATH, f\"HeterGAT/basic/deg_le483_feats_max{max_user_tweets}tweet.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_random_tweet_ids(samples: SubGraphSample, outdir: str, tweets_per_user:int=5):\n",
    "#     tweet_ids = []\n",
    "#     sample_ids = []\n",
    "#     ut_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/utmp_groupbystage.p\"))\n",
    "\n",
    "#     for idx in range(len(samples.labels)):\n",
    "#         if idx and idx % 10000 == 0:\n",
    "#             logger.info(f\"idx={idx}, sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "#         stage = samples.time_stages[idx]\n",
    "#         selected_tweet_ids  = set()\n",
    "#         candidate_tweet_ids = set()\n",
    "#         for vertex_id in samples.vertex_ids[idx]:\n",
    "#             available_tweet_ids = ut_mp[stage][vertex_id]\n",
    "#             random_ids = np.random.choice(available_tweet_ids, size=min(tweets_per_user, len(available_tweet_ids)), replace=False)\n",
    "#             selected_tweet_ids  |= set(random_ids)\n",
    "#             candidate_tweet_ids |= set(available_tweet_ids)-set(random_ids)\n",
    "#         candidate_tweet_ids -= selected_tweet_ids\n",
    "#         # logger.info(f\"Length: sample={len(selected_tweet_ids)}, remain={len(candidate_tweet_ids)}, expected={len(samples.vertex_ids[idx])*tweets_per_user}\")\n",
    "\n",
    "#         if len(selected_tweet_ids) != len(samples.vertex_ids[idx])*tweets_per_user:\n",
    "#             diff = len(samples.vertex_ids[idx])*tweets_per_user - len(selected_tweet_ids)\n",
    "#             if diff > len(candidate_tweet_ids):\n",
    "#                 continue\n",
    "#             selected_tweet_ids |= set(np.random.choice(list(candidate_tweet_ids), size=diff, replace=False))\n",
    "#         sample_ids.append(idx)\n",
    "#         tweet_ids.append(selected_tweet_ids)\n",
    "#     logger.info(f\"Finish Sampling Random Tweets... sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "\n",
    "#     os.makedirs(outdir, exist_ok=True)\n",
    "#     selected_samples = SubGraphSample(\n",
    "#         adj_matrices=samples.adj_matrices[sample_ids],\n",
    "#         influence_features=samples.influence_features[sample_ids],\n",
    "#         vertex_ids=samples.vertex_ids[sample_ids],\n",
    "#         labels=samples.labels[sample_ids],\n",
    "#         tags=samples.tags[sample_ids],\n",
    "#         time_stages=samples.time_stages[sample_ids],\n",
    "#     )\n",
    "#     save_pickle(sample_ids, os.path.join(outdir, \"sample_ids.p\"))\n",
    "#     save_pickle(tweet_ids, os.path.join(outdir, \"tweet_ids.p\"))\n",
    "#     save_pickle(selected_samples, os.path.join(outdir, \"selected_samples.p\"))\n",
    "#     logger.info(\"Finish Saving pkl...\")\n",
    "\n",
    "# def extend_subnetwork(file_dir: str):\n",
    "#     hs_filedir = os.path.join(DATA_ROOTPATH, file_dir).replace('stages_', 'hs_')\n",
    "#     samples = load_pickle(os.path.join(hs_filedir, \"selected_samples.p\"))\n",
    "#     tweet_ids = load_pickle(os.path.join(hs_filedir, \"tweet_ids.p\"))\n",
    "#     assert len(samples) == len(tweet_ids)\n",
    "\n",
    "#     tweetid2userid_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/tweetid2userid_mp.p\"))\n",
    "#     vertex_ids = samples.vertex_ids\n",
    "#     adjs       = samples.adj_matrices\n",
    "#     adjs[adjs != 0] = 1.0\n",
    "#     adjs = adjs.astype(np.dtype('B'))\n",
    "\n",
    "#     extended_vertices, extended_adjs = [], []\n",
    "#     for idx in range(len(samples)):\n",
    "#         subnetwork = np.array(np.concatenate((vertex_ids[idx], np.array(list(tweet_ids[idx])))), dtype=int)\n",
    "#         extended_vertices.append(subnetwork)\n",
    "\n",
    "#         subnetwork_size, num_users = len(subnetwork), len(vertex_ids[idx])\n",
    "#         elem_idx_mp = {elem:idx for idx,elem in enumerate(subnetwork)}\n",
    "#         uu_adj = np.array([[0]*subnetwork_size for _ in range(subnetwork_size)], dtype='B')\n",
    "#         uu_adj[:num_users,:num_users] = adjs[idx]\n",
    "#         # NOTE: Get Corresponding User_id By Tweet_id, and then convert them into indexes in extend_subnetwork\n",
    "#         ut_adj = copy.deepcopy(uu_adj)\n",
    "#         for tweet_id in tweet_ids[idx]:\n",
    "#             user_id = tweetid2userid_mp[tweet_id]\n",
    "#             net_userid = elem_idx_mp[user_id]\n",
    "#             net_tweetid = elem_idx_mp[tweet_id]\n",
    "#             ut_adj[net_userid][net_tweetid] = 1\n",
    "#         extended_adjs.append([uu_adj, ut_adj])\n",
    "#     extended_vertices, extended_adjs = np.array(extended_vertices), np.array(extended_adjs)\n",
    "#     save_pickle(extended_vertices, os.path.join(hs_filedir, \"extended_vertices.p\"))\n",
    "#     save_pickle(extended_adjs, os.path.join(hs_filedir, \"extended_adjs.p\"))\n",
    "\n",
    "# data_dirpath = os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n",
    "# samples = SubGraphSample(\n",
    "#     adj_matrices=np.load(os.path.join(data_dirpath, \"adjacency_matrix.npy\")),\n",
    "#     influence_features=np.load(os.path.join(data_dirpath, \"influence_feature.npy\")),\n",
    "#     vertex_ids=np.load(os.path.join(data_dirpath, \"vertex_id.npy\")),\n",
    "#     labels=np.load(os.path.join(data_dirpath, \"label.npy\")),\n",
    "#     tags=np.load(os.path.join(data_dirpath, \"hashtag.npy\")),\n",
    "#     time_stages=np.load(os.path.join(data_dirpath, \"stage.npy\"))\n",
    "# )\n",
    "# # gen_random_tweet_ids(samples, os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\"))\n",
    "# # extend_subnetwork(\"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n",
    "\n",
    "# def gen_user_emb(tot_user_num):\n",
    "#     # 208894*200*8*3\n",
    "#     user_feats = [[0.]*200*8*3 for _ in range(tot_user_num)]\n",
    "#     for tag in range(200):\n",
    "#         logger.info(f\"tag={tag}\")\n",
    "#         for stage in range(8):\n",
    "#             for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "#                 feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{tag}_t{stage}.p\")\n",
    "#                 for idx in range(tot_user_num):\n",
    "#                     user_feats[idx][tag*3*8+stage*3+feats_idx] = float(feats[idx])\n",
    "#     logger.info(f\"shape={user_feats.shape}\")\n",
    "#     return torch.FloatTensor(user_feats)\n",
    "\n",
    "# user_emb = gen_user_emb(208894)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('Heter-GAT-afYRWUpf': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d5a35b220c759fdf3543f8c8f7df3e7cf51856a17fdad347db68213ac2aaee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
