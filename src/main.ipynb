{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\\n方法:\\n1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\\n2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\\n   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\\n3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\n",
    "方法:\n",
    "1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\n",
    "2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\n",
    "   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\n",
    "3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.dirname(os.getcwd()))\n",
    "# from lib.log import logger\n",
    "# from utils import load_pickle, save_pickle, summarize_distribution, find_rt_bound, HeterSubGraphSample\n",
    "# from lib.utils import get_node_types, extend_edges, create_sparse, get_sparse_tensor\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from lib.log import logger\n",
    "from lib.utils import get_sparse_tensor\n",
    "from utils import HeterSubGraphSample, load_pickle, save_pickle, init_args, ChunkSampler, HeterGraphDataset, sparse_batch_collate\n",
    "from model import HeterGraphAttentionNetwork\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve\n",
    "from tensorboard_logger import tensorboard_logger\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 14:05:00,177 Loading...\n",
      "2022-10-22 14:06:55,587 Progress=0.016213499359566775\n",
      "2022-10-22 14:06:55,591 (1000, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: gen user feats\n",
    "from utils import SubGraphSample\n",
    "\n",
    "data_dirpath = \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\"\n",
    "samples = SubGraphSample(\n",
    "    adj_matrices=np.load(os.path.join(data_dirpath, \"adjacency_matrix.npy\")),\n",
    "    influence_features=np.load(os.path.join(data_dirpath, \"influence_feature.npy\")),\n",
    "    vertex_ids=np.load(os.path.join(data_dirpath, \"vertex_id.npy\")),\n",
    "    labels=np.load(os.path.join(data_dirpath, \"label.npy\")),\n",
    "    tags=np.load(os.path.join(data_dirpath, \"hashtag.npy\")),\n",
    "    time_stages=np.load(os.path.join(data_dirpath, \"stage.npy\"))\n",
    ")\n",
    "logger.info(\"Loading...\")\n",
    "\n",
    "inf_feats  = samples.influence_features\n",
    "hashtags   = samples.tags\n",
    "stages     = samples.time_stages\n",
    "\n",
    "user_feats = []\n",
    "for idx in range(len(samples)):\n",
    "    if idx and idx % 1000 == 0:\n",
    "        logger.info(f\"Progress={idx/len(samples)}\")\n",
    "        break\n",
    "    user_ids = samples.vertex_ids[idx]\n",
    "\n",
    "    gra_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_gravity_feature/hashtag{hashtags[idx]}_t{stages[idx]}.p\")).reshape(-1,1)\n",
    "    exp_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_exptime_feature1/hashtag{hashtags[idx]}_t{stages[idx]}.p\")).reshape(-1,1)\n",
    "    cas_feat = np.array(load_pickle(f\"/root/data/HeterGAT/user_features/norm_ce_feature/hashtag{hashtags[idx]}_t{stages[idx]}.p\")).reshape(-1,1)\n",
    "    user_feats.append(\n",
    "        np.concatenate((inf_feats[idx], gra_feat[user_ids], exp_feat[user_ids], cas_feat[user_ids]), axis=1)\n",
    "    )\n",
    "user_feats = np.array(user_feats)\n",
    "logger.info(f\"{user_feats.shape}\")\n",
    "# save_pickle(user_feats, os.path.join(data_dirpath, \"user_features.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 20:00:17,780 torch.Size([5, 2])\n",
      "2022-10-22 20:00:17,782 tensor([[0.7470, 0.4498],\n",
      "        [0.9738, 0.6716],\n",
      "        [0.2377, 0.2962],\n",
      "        [0.1450, 0.8016],\n",
      "        [0.7793, 0.2376]])\n",
      "2022-10-22 20:00:17,785 tensor([[0.9189, 0.4629, 0.1308, 0.1450, 0.1231],\n",
      "        [0.3367, 0.6372, 0.2932, 0.8016, 0.2547],\n",
      "        [0.0439, 0.9738, 0.0189, 0.4817, 0.8586],\n",
      "        [0.9848, 0.6716, 0.3356, 0.9293, 0.7409],\n",
      "        [0.7470, 0.2417, 0.4886, 0.5591, 0.6745],\n",
      "        [0.4498, 0.4378, 0.5871, 0.5634, 0.7223],\n",
      "        [0.1415, 0.3992, 0.2377, 0.5711, 0.5305],\n",
      "        [0.0782, 0.6359, 0.2962, 0.8852, 0.2720],\n",
      "        [0.0191, 0.8393, 0.8337, 0.0857, 0.7793],\n",
      "        [0.3462, 0.4402, 0.5805, 0.5101, 0.2376]])\n"
     ]
    }
   ],
   "source": [
    "N, bs, n = 10, 5, 2\n",
    "\n",
    "a = torch.rand((N, bs))\n",
    "indices = torch.LongTensor([[4,5], [2,3], [6,7], [0,1], [8,9]]) # (bs, n)\n",
    "# trg: (bs, n, 1)\n",
    "\n",
    "trg = a.T.gather(dim=1, index=indices)\n",
    "logger.info(trg.shape)\n",
    "logger.info(trg)\n",
    "logger.info(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen Hadjs\n",
    "tweet_ids = load_pickle(\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/tweet_ids.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 20:49:57,482 Error\n",
      "2022-10-22 20:49:57,485 Error\n",
      "2022-10-22 20:49:57,486 Error\n",
      "2022-10-22 20:49:57,487 Error\n",
      "2022-10-22 20:49:57,488 Error\n",
      "2022-10-22 20:49:57,489 Error\n",
      "2022-10-22 20:49:57,490 Error\n",
      "2022-10-22 20:49:57,490 Error\n",
      "2022-10-22 20:49:57,492 Error\n",
      "2022-10-22 20:49:57,493 Error\n",
      "2022-10-22 20:49:57,493 Error\n",
      "2022-10-22 20:49:57,495 Error\n",
      "2022-10-22 20:49:57,496 Error\n",
      "2022-10-22 20:49:57,496 Error\n",
      "2022-10-22 20:49:57,497 Error\n",
      "2022-10-22 20:49:57,498 Error\n",
      "2022-10-22 20:49:57,499 Error\n",
      "2022-10-22 20:49:57,500 Error\n",
      "2022-10-22 20:49:57,501 Error\n",
      "2022-10-22 20:49:57,502 Error\n",
      "2022-10-22 20:49:57,503 Error\n",
      "2022-10-22 20:49:57,503 Error\n",
      "2022-10-22 20:49:57,504 Error\n",
      "2022-10-22 20:49:57,505 Error\n",
      "2022-10-22 20:49:57,506 Error\n",
      "2022-10-22 20:49:57,507 Error\n",
      "2022-10-22 20:49:57,507 Error\n",
      "2022-10-22 20:49:57,508 Error\n",
      "2022-10-22 20:49:57,509 Error\n",
      "2022-10-22 20:49:57,510 Error\n",
      "2022-10-22 20:49:57,511 Error\n",
      "2022-10-22 20:49:57,512 Error\n",
      "2022-10-22 20:49:57,512 Error\n",
      "2022-10-22 20:49:57,513 Error\n",
      "2022-10-22 20:49:57,514 Error\n",
      "2022-10-22 20:49:57,514 Error\n",
      "2022-10-22 20:49:57,515 Error\n",
      "2022-10-22 20:49:57,515 Error\n",
      "2022-10-22 20:49:57,516 Error\n",
      "2022-10-22 20:49:57,517 Error\n",
      "2022-10-22 20:49:57,517 Error\n",
      "2022-10-22 20:49:57,518 Error\n",
      "2022-10-22 20:49:57,518 Error\n",
      "2022-10-22 20:49:57,519 Error\n",
      "2022-10-22 20:49:57,520 Error\n",
      "2022-10-22 20:49:57,521 Error\n",
      "2022-10-22 20:49:57,521 Error\n",
      "2022-10-22 20:49:57,522 Error\n",
      "2022-10-22 20:49:57,523 Error\n",
      "2022-10-22 20:49:57,524 Error\n",
      "2022-10-22 20:49:57,525 Error\n",
      "2022-10-22 20:49:57,526 Error\n",
      "2022-10-22 20:49:57,527 Error\n",
      "2022-10-22 20:49:57,528 Error\n",
      "2022-10-22 20:49:57,528 Error\n",
      "2022-10-22 20:49:57,529 Error\n",
      "2022-10-22 20:49:57,530 Error\n",
      "2022-10-22 20:49:57,531 Error\n",
      "2022-10-22 20:49:57,532 Error\n",
      "2022-10-22 20:49:57,533 Error\n",
      "2022-10-22 20:49:57,534 Error\n",
      "2022-10-22 20:49:57,535 Error\n",
      "2022-10-22 20:49:57,535 Error\n",
      "2022-10-22 20:49:57,536 Error\n",
      "2022-10-22 20:49:57,537 Error\n",
      "2022-10-22 20:49:57,537 Error\n",
      "2022-10-22 20:49:57,538 Error\n",
      "2022-10-22 20:49:57,539 Error\n",
      "2022-10-22 20:49:57,540 Error\n",
      "2022-10-22 20:49:57,540 Error\n",
      "2022-10-22 20:49:57,541 Error\n",
      "2022-10-22 20:49:57,542 Error\n",
      "2022-10-22 20:49:57,543 Error\n",
      "2022-10-22 20:49:57,543 Error\n",
      "2022-10-22 20:49:57,544 Error\n",
      "2022-10-22 20:49:57,545 Error\n",
      "2022-10-22 20:49:57,546 Error\n",
      "2022-10-22 20:49:57,546 Error\n",
      "2022-10-22 20:49:57,547 Error\n",
      "2022-10-22 20:49:57,548 Error\n",
      "2022-10-22 20:49:57,549 Error\n",
      "2022-10-22 20:49:57,550 Error\n",
      "2022-10-22 20:49:57,551 Error\n",
      "2022-10-22 20:49:57,551 Error\n",
      "2022-10-22 20:49:57,552 Error\n",
      "2022-10-22 20:49:57,552 Error\n",
      "2022-10-22 20:49:57,553 Error\n",
      "2022-10-22 20:49:57,553 Error\n",
      "2022-10-22 20:49:57,554 Error\n",
      "2022-10-22 20:49:57,555 Error\n",
      "2022-10-22 20:49:57,555 Error\n",
      "2022-10-22 20:49:57,556 Error\n",
      "2022-10-22 20:49:57,556 Error\n",
      "2022-10-22 20:49:57,557 Error\n",
      "2022-10-22 20:49:57,558 Error\n",
      "2022-10-22 20:49:57,558 Error\n",
      "2022-10-22 20:49:57,559 Error\n",
      "2022-10-22 20:49:57,560 Error\n",
      "2022-10-22 20:49:57,560 Error\n",
      "2022-10-22 20:49:57,561 Error\n",
      "2022-10-22 20:49:57,562 Error\n",
      "2022-10-22 20:49:57,563 Error\n",
      "2022-10-22 20:49:57,563 Error\n",
      "2022-10-22 20:49:57,564 Error\n",
      "2022-10-22 20:49:57,565 Error\n",
      "2022-10-22 20:49:57,565 Error\n",
      "2022-10-22 20:49:57,566 Error\n",
      "2022-10-22 20:49:57,567 Error\n",
      "2022-10-22 20:49:57,567 Error\n",
      "2022-10-22 20:49:57,568 Error\n",
      "2022-10-22 20:49:57,569 Error\n",
      "2022-10-22 20:49:57,570 Error\n",
      "2022-10-22 20:49:57,570 Error\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(tweet_ids)):\n",
    "    tids = tweet_ids[idx]\n",
    "    for tid in tids:\n",
    "        if tid >= 208894:\n",
    "            continue\n",
    "        logger.info(\"Error\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 15:00:38,171 tag=0\n",
      "2022-10-22 15:00:42,130 tag=1\n",
      "2022-10-22 15:00:45,979 tag=2\n",
      "2022-10-22 15:00:49,769 tag=3\n",
      "2022-10-22 15:00:53,529 tag=4\n",
      "2022-10-22 15:00:57,284 tag=5\n",
      "2022-10-22 15:01:01,043 tag=6\n",
      "2022-10-22 15:01:04,816 tag=7\n",
      "2022-10-22 15:01:08,604 tag=8\n",
      "2022-10-22 15:01:12,377 tag=9\n",
      "2022-10-22 15:01:16,140 tag=10\n",
      "2022-10-22 15:01:19,967 tag=11\n",
      "2022-10-22 15:01:23,934 tag=12\n",
      "2022-10-22 15:01:27,878 tag=13\n",
      "2022-10-22 15:02:34,273 tag=14\n",
      "2022-10-22 15:02:38,360 tag=15\n",
      "2022-10-22 15:02:42,337 tag=16\n",
      "2022-10-22 15:02:46,308 tag=17\n",
      "2022-10-22 15:02:50,373 tag=18\n",
      "2022-10-22 15:02:54,344 tag=19\n",
      "2022-10-22 15:02:58,307 tag=20\n",
      "2022-10-22 15:03:02,269 tag=21\n",
      "2022-10-22 15:03:06,204 tag=22\n",
      "2022-10-22 15:03:10,134 tag=23\n",
      "2022-10-22 15:03:14,114 tag=24\n",
      "2022-10-22 15:03:18,049 tag=25\n",
      "2022-10-22 15:03:21,998 tag=26\n",
      "2022-10-22 15:03:25,954 tag=27\n",
      "2022-10-22 15:03:29,903 tag=28\n",
      "2022-10-22 15:03:33,855 tag=29\n",
      "2022-10-22 15:03:37,908 tag=30\n",
      "2022-10-22 15:03:41,983 tag=31\n",
      "2022-10-22 15:03:46,094 tag=32\n",
      "2022-10-22 15:03:50,298 tag=33\n",
      "2022-10-22 15:03:54,490 tag=34\n",
      "2022-10-22 15:03:58,487 tag=35\n",
      "2022-10-22 15:04:02,498 tag=36\n",
      "2022-10-22 15:04:06,610 tag=37\n",
      "2022-10-22 15:04:10,634 tag=38\n",
      "2022-10-22 15:04:14,636 tag=39\n",
      "2022-10-22 15:04:18,635 tag=40\n",
      "2022-10-22 15:04:22,649 tag=41\n",
      "2022-10-22 15:04:26,692 tag=42\n",
      "2022-10-22 15:04:30,731 tag=43\n",
      "2022-10-22 15:04:34,697 tag=44\n",
      "2022-10-22 15:04:38,685 tag=45\n",
      "2022-10-22 15:04:42,667 tag=46\n",
      "2022-10-22 15:04:46,635 tag=47\n",
      "2022-10-22 15:04:50,610 tag=48\n",
      "2022-10-22 15:04:54,614 tag=49\n",
      "2022-10-22 15:04:58,635 tag=50\n",
      "2022-10-22 15:05:02,624 tag=51\n",
      "2022-10-22 15:05:06,610 tag=52\n",
      "2022-10-22 15:05:10,566 tag=53\n",
      "2022-10-22 15:05:14,540 tag=54\n",
      "2022-10-22 15:05:18,511 tag=55\n",
      "2022-10-22 15:05:22,496 tag=56\n",
      "2022-10-22 15:05:26,502 tag=57\n",
      "2022-10-22 15:05:30,578 tag=58\n",
      "2022-10-22 15:05:34,541 tag=59\n",
      "2022-10-22 15:05:38,521 tag=60\n",
      "2022-10-22 15:05:42,522 tag=61\n",
      "2022-10-22 15:05:46,496 tag=62\n",
      "2022-10-22 15:05:50,485 tag=63\n",
      "2022-10-22 15:05:54,496 tag=64\n",
      "2022-10-22 15:05:58,526 tag=65\n",
      "2022-10-22 15:06:02,483 tag=66\n",
      "2022-10-22 15:06:06,500 tag=67\n",
      "2022-10-22 15:06:10,510 tag=68\n",
      "2022-10-22 15:06:14,519 tag=69\n",
      "2022-10-22 15:06:18,398 tag=70\n",
      "2022-10-22 15:06:22,386 tag=71\n",
      "2022-10-22 15:06:26,422 tag=72\n",
      "2022-10-22 15:06:30,442 tag=73\n",
      "2022-10-22 15:06:34,410 tag=74\n",
      "2022-10-22 15:06:38,425 tag=75\n",
      "2022-10-22 15:06:42,391 tag=76\n",
      "2022-10-22 15:06:46,378 tag=77\n",
      "2022-10-22 15:06:50,398 tag=78\n",
      "2022-10-22 15:06:54,415 tag=79\n",
      "2022-10-22 15:06:58,486 tag=80\n",
      "2022-10-22 15:07:02,467 tag=81\n",
      "2022-10-22 15:07:06,454 tag=82\n",
      "2022-10-22 15:07:10,415 tag=83\n",
      "2022-10-22 15:07:14,394 tag=84\n",
      "2022-10-22 15:07:18,395 tag=85\n",
      "2022-10-22 15:07:22,399 tag=86\n",
      "2022-10-22 15:07:26,539 tag=87\n",
      "2022-10-22 15:07:30,609 tag=88\n",
      "2022-10-22 15:07:34,682 tag=89\n",
      "2022-10-22 15:07:38,777 tag=90\n",
      "2022-10-22 15:07:42,869 tag=91\n",
      "2022-10-22 15:07:46,966 tag=92\n",
      "2022-10-22 15:07:51,068 tag=93\n",
      "2022-10-22 15:07:55,169 tag=94\n",
      "2022-10-22 15:07:59,241 tag=95\n",
      "2022-10-22 15:08:03,313 tag=96\n",
      "2022-10-22 15:08:07,391 tag=97\n",
      "2022-10-22 15:08:11,441 tag=98\n",
      "2022-10-22 15:08:15,512 tag=99\n",
      "2022-10-22 15:08:19,550 tag=100\n",
      "2022-10-22 15:08:23,720 tag=101\n",
      "2022-10-22 15:08:27,822 tag=102\n",
      "2022-10-22 15:08:31,932 tag=103\n",
      "2022-10-22 15:08:36,009 tag=104\n",
      "2022-10-22 15:08:40,096 tag=105\n",
      "2022-10-22 15:08:44,150 tag=106\n",
      "2022-10-22 15:08:48,258 tag=107\n",
      "2022-10-22 15:08:52,344 tag=108\n",
      "2022-10-22 15:08:56,438 tag=109\n",
      "2022-10-22 15:09:00,502 tag=110\n",
      "2022-10-22 15:09:04,562 tag=111\n",
      "2022-10-22 15:09:08,669 tag=112\n",
      "2022-10-22 15:09:12,701 tag=113\n",
      "2022-10-22 15:09:16,764 tag=114\n",
      "2022-10-22 15:09:20,861 tag=115\n",
      "2022-10-22 15:09:24,967 tag=116\n",
      "2022-10-22 15:09:29,022 tag=117\n",
      "2022-10-22 15:09:33,113 tag=118\n",
      "2022-10-22 15:09:37,183 tag=119\n",
      "2022-10-22 15:09:41,266 tag=120\n",
      "2022-10-22 15:09:45,343 tag=121\n",
      "2022-10-22 15:09:49,465 tag=122\n",
      "2022-10-22 15:09:53,537 tag=123\n",
      "2022-10-22 15:09:57,613 tag=124\n",
      "2022-10-22 15:10:01,704 tag=125\n",
      "2022-10-22 15:10:05,800 tag=126\n",
      "2022-10-22 15:10:09,902 tag=127\n",
      "2022-10-22 15:10:13,995 tag=128\n",
      "2022-10-22 15:10:18,132 tag=129\n",
      "2022-10-22 15:10:22,210 tag=130\n",
      "2022-10-22 15:10:26,280 tag=131\n",
      "2022-10-22 15:10:30,370 tag=132\n",
      "2022-10-22 15:10:34,448 tag=133\n",
      "2022-10-22 15:10:38,486 tag=134\n",
      "2022-10-22 15:10:42,582 tag=135\n",
      "2022-10-22 15:10:46,699 tag=136\n",
      "2022-10-22 15:10:50,804 tag=137\n",
      "2022-10-22 15:10:54,865 tag=138\n",
      "2022-10-22 15:10:58,967 tag=139\n",
      "2022-10-22 15:11:03,041 tag=140\n",
      "2022-10-22 15:11:07,116 tag=141\n",
      "2022-10-22 15:11:11,206 tag=142\n",
      "2022-10-22 15:11:15,351 tag=143\n",
      "2022-10-22 15:11:19,460 tag=144\n",
      "2022-10-22 15:11:23,549 tag=145\n",
      "2022-10-22 15:11:27,654 tag=146\n",
      "2022-10-22 15:11:31,735 tag=147\n",
      "2022-10-22 15:11:35,783 tag=148\n",
      "2022-10-22 15:11:39,866 tag=149\n",
      "2022-10-22 15:11:44,019 tag=150\n",
      "2022-10-22 15:11:48,138 tag=151\n",
      "2022-10-22 15:11:52,206 tag=152\n",
      "2022-10-22 15:11:56,309 tag=153\n",
      "2022-10-22 15:12:00,396 tag=154\n",
      "2022-10-22 15:12:04,473 tag=155\n",
      "2022-10-22 15:12:08,567 tag=156\n",
      "2022-10-22 15:12:12,749 tag=157\n",
      "2022-10-22 15:12:16,837 tag=158\n",
      "2022-10-22 15:12:20,883 tag=159\n",
      "2022-10-22 15:12:25,034 tag=160\n",
      "2022-10-22 15:12:29,207 tag=161\n",
      "2022-10-22 15:12:33,447 tag=162\n",
      "2022-10-22 15:12:37,665 tag=163\n",
      "2022-10-22 15:12:41,919 tag=164\n",
      "2022-10-22 15:12:46,058 tag=165\n",
      "2022-10-22 15:12:50,248 tag=166\n",
      "2022-10-22 15:12:54,428 tag=167\n",
      "2022-10-22 15:12:58,601 tag=168\n",
      "2022-10-22 15:13:02,783 tag=169\n",
      "2022-10-22 15:13:06,960 tag=170\n",
      "2022-10-22 15:13:11,176 tag=171\n",
      "2022-10-22 15:13:15,293 tag=172\n",
      "2022-10-22 15:13:19,459 tag=173\n",
      "2022-10-22 15:13:23,624 tag=174\n",
      "2022-10-22 15:13:27,802 tag=175\n",
      "2022-10-22 15:13:31,972 tag=176\n",
      "2022-10-22 15:13:36,212 tag=177\n",
      "2022-10-22 15:13:40,379 tag=178\n",
      "2022-10-22 15:13:44,538 tag=179\n",
      "2022-10-22 15:13:48,703 tag=180\n",
      "2022-10-22 15:13:52,859 tag=181\n",
      "2022-10-22 15:13:56,994 tag=182\n",
      "2022-10-22 15:14:01,190 tag=183\n",
      "2022-10-22 15:14:05,458 tag=184\n",
      "2022-10-22 15:14:09,635 tag=185\n",
      "2022-10-22 15:14:13,788 tag=186\n",
      "2022-10-22 15:14:17,952 tag=187\n",
      "2022-10-22 15:14:22,115 tag=188\n",
      "2022-10-22 15:14:26,268 tag=189\n",
      "2022-10-22 15:14:30,426 tag=190\n",
      "2022-10-22 15:14:34,623 tag=191\n",
      "2022-10-22 15:14:38,780 tag=192\n",
      "2022-10-22 15:14:42,938 tag=193\n",
      "2022-10-22 15:14:47,090 tag=194\n",
      "2022-10-22 15:14:51,284 tag=195\n",
      "2022-10-22 15:14:55,434 tag=196\n",
      "2022-10-22 15:14:59,596 tag=197\n",
      "2022-10-22 15:15:03,805 tag=198\n",
      "2022-10-22 15:15:07,976 tag=199\n",
      "2022-10-22 15:19:54,739 shape=(208894, 4800)\n"
     ]
    }
   ],
   "source": [
    "# def gen_user_emb(tot_user_num):\n",
    "#     # 208894*200*8*3\n",
    "#     user_feats = [[0.]*200*8*3 for _ in range(tot_user_num)]\n",
    "#     for tag in range(200):\n",
    "#         logger.info(f\"tag={tag}\")\n",
    "#         for stage in range(8):\n",
    "#             for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "#                 feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{tag}_t{stage}.p\")\n",
    "#                 for idx in range(tot_user_num):\n",
    "#                     user_feats[idx][tag*3*8+stage*3+feats_idx] = float(feats[idx])\n",
    "#     logger.info(f\"shape={user_feats.shape}\")\n",
    "#     return torch.FloatTensor(user_feats)\n",
    "\n",
    "# user_emb = gen_user_emb(208894)\n",
    "\n",
    "# 208894*200*8*3\n",
    "tot_user_num = 208894\n",
    "user_feats = [[0.]*200*8*3 for _ in range(tot_user_num)]\n",
    "for tag in range(200):\n",
    "    logger.info(f\"tag={tag}\")\n",
    "    for stage in range(8):\n",
    "        for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "            feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{tag}_t{stage}.p\")\n",
    "            for idx in range(tot_user_num):\n",
    "                user_feats[idx][tag*3*8+stage*3+feats_idx] = float(feats[idx])\n",
    "user_feats = np.array(user_feats)\n",
    "logger.info(f\"shape={user_feats.shape}\")\n",
    "save_pickle(user_feats, \"/root/data/HeterGAT/user_features/user_features.p\")\n",
    "user_emb = torch.FloatTensor(user_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 14:39:09,428 0.6372319229258053\n",
      "2022-10-22 14:39:09,527 0.2223360655737705\n",
      "2022-10-22 14:39:09,532 0.5099874841101689\n"
     ]
    }
   ],
   "source": [
    "for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "    feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{0}_t{0}.p\")\n",
    "    logger.info(feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208894"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = load_pickle(\"/root/data/HeterGAT/user_features/norm_gravity_feature/hashtag0_t0.p\")\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61677"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = load_pickle(\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/heter_samples.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49926, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Fake Digg Heter Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from utils import SubGraphSample, load_w2v_feature\n",
    "\n",
    "class DiggDataset(Dataset):\n",
    "    def __init__(self, samples: SubGraphSample, embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.adjs = samples.adj_matrices\n",
    "        self.labels = samples.labels\n",
    "        self.feats = samples.influence_features\n",
    "        self.vertex_ids = samples.vertex_ids\n",
    "        self.concact_feats(embedding)\n",
    "    def concact_feats(self, embedding):\n",
    "        feats = []\n",
    "        for idx, vertex_ids in enumerate(self.vertex_ids):\n",
    "            emb_feats = [embedding[user] for user in vertex_ids]\n",
    "            feats.append(np.concatenate((self.feats[idx], emb_feats), axis=1))\n",
    "        self.feats = np.array(feats)\n",
    "        logger.info(self.feats.shape)\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.adjs[index], self.labels[index], self.feats[index]\n",
    "\n",
    "def collate_fn2(batch:list): \n",
    "    \"\"\"\n",
    "    Collate function which to transform scipy coo matrix to pytorch sparse tensor\n",
    "    \"\"\"\n",
    "    adjs_batch, labels_batch, feats_batch = zip(*batch)\n",
    "    adjs_batch = torch.FloatTensor(np.array(adjs_batch))\n",
    "    \n",
    "    if type(labels_batch[0]).__module__ == 'numpy':\n",
    "        # NOTE: https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "        labels_batch = torch.LongTensor(labels_batch)\n",
    "    \n",
    "    if type(feats_batch[0]).__module__ == 'numpy':\n",
    "        feats_batch = torch.FloatTensor(np.array(feats_batch))\n",
    "    return adjs_batch, labels_batch, feats_batch\n",
    "\n",
    "def digg_load_dataset(train_ratio=60, valid_ratio=20, batch_size=256):\n",
    "    embedding_path = \"/root/Lab_Related/data/Heter-GAT/Classic/deepwalk/deepwalk_added.emb_64\"\n",
    "    vertices = np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\")\n",
    "    max_vertex_idx = np.max(vertices)\n",
    "    embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
    "    # embedding = torch.FloatTensor(embedding)\n",
    "\n",
    "    samples = SubGraphSample(\n",
    "        adj_matrices=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/adjacency_matrix.npy\"),\n",
    "        influence_features=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/influence_feature.npy\"),\n",
    "        vertex_ids=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\"),\n",
    "        labels=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/label.npy\")\n",
    "    )\n",
    "    dataset = DiggDataset(samples, embedding)\n",
    "    nb_samples    = len(dataset)\n",
    "    \n",
    "    train_start,  valid_start, test_start = 0, int(nb_samples*train_ratio/100), int(nb_samples*(train_ratio+valid_ratio)/100)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(valid_start-train_start, 0), collate_fn=collate_fn2)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(test_start-valid_start, valid_start), collate_fn=collate_fn2)\n",
    "    test_loader  = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(nb_samples - test_start, test_start), collate_fn=collate_fn2)\n",
    "    logger.info(f\"Finish Loading Dataset... train={len(train_loader)}, valid={len(valid_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    return samples, train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 15:42:11,284 n=204955, d=64\n",
      "2022-10-12 15:42:21,005 (38152, 50, 66)\n",
      "2022-10-12 15:42:21,017 Finish Loading Dataset... train=90, valid=30, test=30\n",
      "2022-10-12 15:42:21,184 nb_samples=38152, class_weight=0.57:4.05, n_units=[66, 16, 16], n_heads=[8, 8]\n"
     ]
    }
   ],
   "source": [
    "GPU_MODEL = 'cuda:2'\n",
    "\n",
    "# 1. \n",
    "def load_dataset(data_filepath:str, train_ratio:float, valid_ratio:float, batch_size:int):\n",
    "    # heter_samples = load_pickle(os.path.join(data_dirpath, \"heter_samples_tensor.p\"))\n",
    "    heter_samples = load_pickle(data_filepath)\n",
    "    dataset       = HeterGraphDataset(heter_samples=heter_samples)\n",
    "    nb_samples    = len(dataset)\n",
    "    \n",
    "    train_start,  valid_start, test_start = 0, int(nb_samples*train_ratio/100), int(nb_samples*(train_ratio+valid_ratio)/100)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(valid_start-train_start, 0), collate_fn=sparse_batch_collate)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(test_start-valid_start, valid_start), collate_fn=sparse_batch_collate)\n",
    "    test_loader  = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(nb_samples - test_start, test_start), collate_fn=sparse_batch_collate)\n",
    "    logger.info(f\"Finish Loading Dataset... train={len(train_loader)}, valid={len(valid_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    return heter_samples, train_loader, valid_loader, test_loader\n",
    "\n",
    "# args = init_args()\n",
    "args = {\n",
    "    \"train_ratio\": 60,\n",
    "    \"valid_ratio\": 20,\n",
    "    \"batch\": 256,\n",
    "    # \"nb_user\": 50,\n",
    "    \"class_weight_balanced\": True,\n",
    "    \"hidden_units\": \"16,16\",\n",
    "    \"heads\": \"8,8\",\n",
    "    \"cuda\": True,\n",
    "    \"lr\": 0.1,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 100,\n",
    "    \"check_point\": 2,\n",
    "    # \"file_dir\": \"heter_samples_ratio1/0/heter_samples.p\",\n",
    "    \"file_dir\": \"hs_new_ratio1/heter_samples.p\",\n",
    "}\n",
    "# heter_samples, train_loader, valid_loader, test_loader = load_dataset(args[\"file_dir\"], args[\"train_ratio\"], args[\"valid_ratio\"], args[\"batch\"])\n",
    "samples, train_loader, valid_loader, test_loader = digg_load_dataset()\n",
    "nb_samples = len(samples)\n",
    "nb_classes = 2\n",
    "class_weight = torch.FloatTensor(nb_samples / (nb_classes*np.bincount(samples.labels))) if args[\"class_weight_balanced\"] else torch.ones(nb_classes)\n",
    "nb_user = 50\n",
    "n_units = [samples.influence_features.shape[2]+64]+[int(x) for x in args[\"hidden_units\"].strip().split(\",\")]\n",
    "n_heads = [int(x) for x in args[\"heads\"].strip().split(\",\")]\n",
    "logger.info(f\"nb_samples={nb_samples}, class_weight={class_weight[0]:.2f}:{class_weight[1]:.2f}, n_units={n_units}, n_heads={n_heads}\")\n",
    "# logger.info(\"class_weight=%.2f:%.2f\", class_weight[0], class_weight[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "import copy\n",
    "from model import BatchSparseMultiHeadGraphAttention, BatchMultiHeadGraphAttention, BatchAdditiveAttention\n",
    "\n",
    "class HeterGraphAttentionNetwork2(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_user, nb_node_kinds=2, nb_loop_nodes=[50,1050],\n",
    "        nb_classes=2, n_units=[25,64], n_heads=[3],\n",
    "        attn_dropout=0.5, dropout=0.1, \n",
    "        d2=64, gpu_device_ids=[] \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gpu_device_ids(List[int], default=[]): 采用Model Parallel方法, 主要使多头注意力可以在不同GPU上运行, \n",
    "                模型以数据所属GPU为例, 先在该参数指定的不同GPU上执行单一注意力头, 再在最后将所有注意力头的运行结果复制回数据所属的主GPU上\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layer = len(n_units) - 1\n",
    "        self.dropout = dropout\n",
    "        self.gpu_device_ids = gpu_device_ids\n",
    "\n",
    "        self.d = n_units[0]\n",
    "        self.d1 = n_units[1]\n",
    "        self.d2 = n_units[1]\n",
    "        self.n_user = n_user\n",
    "\n",
    "        self.layer_stack = nn.ModuleList()\n",
    "        for hidx in range(nb_node_kinds):\n",
    "            layer_stack = nn.ModuleList()\n",
    "            for i in range(self.n_layer):\n",
    "                # consider multi head from last layer\n",
    "                f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
    "                layer_stack.append(\n",
    "                    # BatchMultiHeadGraphAttention(nb_heads=n_heads[i], nb_in_feats=f_in, nb_out_feats=n_units[i + 1], \n",
    "                    #     nb_loop_nodes=nb_loop_nodes[hidx], attn_dropout=attn_dropout)\n",
    "                    BatchMultiHeadGraphAttention(n_head=n_heads[i], f_in=f_in, f_out=n_units[i+1], attn_dropout=attn_dropout)\n",
    "                )\n",
    "            self.layer_stack.append(layer_stack)\n",
    "        self.additive_attention = BatchAdditiveAttention(d=self.d, d1=self.d1, d2=self.d2)\n",
    "        self.fc_layer = nn.Linear(in_features=self.d1*(nb_node_kinds+1), out_features=nb_classes)\n",
    "    \n",
    "    def forward(self, h, hadj):\n",
    "        # NOTE: h: (bs, N, fin), hadj: (|Rs|, bs, N, N)\n",
    "        bs, n = h.shape[:2]\n",
    "        heter_embs = []\n",
    "        for heter_idx, layer_stack in enumerate(self.layer_stack):\n",
    "            x = copy.deepcopy(h)\n",
    "            for i, gat_layer in enumerate(layer_stack):\n",
    "                x = gat_layer(x, hadj[heter_idx]) # output: (bs, n_head, n, f_out)\n",
    "                if i + 1 == self.n_layer:\n",
    "                    x = x.mean(dim=-3) # (bs, n_head, n, f_out) -> (bs, n, f_out)\n",
    "                else:\n",
    "                    x = F.elu(x.reshape(bs, n, -1))\n",
    "                    x = F.dropout(x, self.dropout, training=self.training)\n",
    "            heter_embs.append(x[:,:self.n_user].unsqueeze(-2)) # (bs, Nu, 1, f_out)\n",
    "        type_aware_emb = torch.cat(heter_embs, dim=-2) # (bs, Nu, |Rs|, D')\n",
    "        type_fusion_emb = self.additive_attention(h[:,:self.n_user], type_aware_emb) # (bs, Nu, 1, D')\n",
    "        ret = self.fc_layer(\n",
    "            torch.cat((type_fusion_emb, type_aware_emb), dim=-2).reshape(bs, self.n_user,-1) # (bs, Nu, |Rs|+1, D') -> (bs, Nu, (|Rs|+1)*D')\n",
    "        ) #  (bs, Nu, nb_classes)\n",
    "        return F.log_softmax(ret, dim=-1)\n",
    "\n",
    "model = HeterGraphAttentionNetwork2(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    # model.cuda()\n",
    "    model.to(GPU_MODEL)\n",
    "    # class_weight = class_weight.cuda()\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "# params = [{'params': model.parameters()}]\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 15:46:18,982 train loss in this epoch 1.182631\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
    "    model.train()\n",
    "\n",
    "    loss = 0.\n",
    "    total = 0.\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        # if i_batch % 10 == 0:\n",
    "        #     logger.info(f\"i_batch={i_batch}\")\n",
    "        adjs, labels, feats = batch\n",
    "        bs = adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            adjs    = adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feats, torch.stack([adjs, adjs]))\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_train = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_train.item()\n",
    "        total += bs\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
    "\n",
    "train(0, train_loader, valid_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, batch in enumerate(train_loader):\n",
    "#     uu_adjs, ut_adjs, labels, feats = batch\n",
    "#     bs = uu_adjs.size(0)\n",
    "\n",
    "#     # feats = feats[:,:100]\n",
    "#     # labels = labels\n",
    "#     # nw_uu_adjs, nw_ut_adjs = [], []\n",
    "#     # for idx in range(len(uu_adjs)):\n",
    "#     #     nw_uu_adjs.append(torch.LongTensor(uu_adjs[idx].to_dense().numpy()[:100,:100]+np.eye(100)))\n",
    "#     #     nw_ut_adjs.append(torch.LongTensor(ut_adjs[idx].to_dense().numpy()[:100,:100]+np.eye(100)))\n",
    "#     # uu_adjs = torch.stack(nw_uu_adjs)\n",
    "#     # ut_adjs = torch.stack(nw_ut_adjs)\n",
    "\n",
    "#     if args[\"cuda\"]:\n",
    "#         uu_adjs, ut_adjs, labels, feats = uu_adjs.to(GPU_MODEL), ut_adjs.to(GPU_MODEL), labels.to(GPU_MODEL), feats.to(GPU_MODEL)\n",
    "#     break\n",
    "\n",
    "# model = HeterGraphAttentionNetwork(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "# if args[\"cuda\"]:\n",
    "#     model.to(GPU_MODEL)\n",
    "#     class_weight = class_weight.to(GPU_MODEL)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "# output = model(feats, torch.stack([uu_adjs, ut_adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "from model import BatchSparseMultiHeadGraphAttention, BatchAdditiveAttention, BatchMultiHeadGraphAttention\n",
    "import copy\n",
    "\n",
    "class HeterGraphAttentionNetwork2(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_user, nb_node_kinds=2, nb_loop_nodes=[50,1050],\n",
    "        nb_classes=2, n_units=[25,64], n_heads=[3],\n",
    "        attn_dropout=0.5, dropout=0.1, \n",
    "        d2=64, gpu_device_ids=[] \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layer = len(n_units) - 1\n",
    "        self.dropout = dropout\n",
    "        self.gpu_device_ids = gpu_device_ids\n",
    "\n",
    "        self.d = n_units[0]\n",
    "        self.d1 = n_units[1]\n",
    "        self.d2 = d2\n",
    "        self.n_user = n_user\n",
    "\n",
    "        self.layer_stack = nn.ModuleList()\n",
    "        for hidx in range(nb_node_kinds):\n",
    "            layer_stack = nn.ModuleList()\n",
    "            for i in range(self.n_layer):\n",
    "                # consider multi head from last layer\n",
    "                f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
    "                layer_stack.append(\n",
    "                    BatchMultiHeadGraphAttention(n_head=n_heads[i], f_in=f_in, f_out=n_units[i + 1], \n",
    "                        attn_dropout=attn_dropout)\n",
    "                )\n",
    "            self.layer_stack.append(layer_stack)\n",
    "        self.additive_attention = BatchAdditiveAttention(d=self.d, d1=self.d1, d2=self.d2)\n",
    "        self.fc_layer = nn.Linear(in_features=self.d1*(nb_node_kinds+1), out_features=nb_classes)\n",
    "    \n",
    "    def forward(self, h, hadj):\n",
    "        # NOTE: h: (bs, N, fin), hadj: (|Rs|, bs, N, N)\n",
    "        bs, n = h.shape[:2]\n",
    "        heter_embs = []\n",
    "        for heter_idx, layer_stack in enumerate(self.layer_stack):\n",
    "            x = copy.deepcopy(h)\n",
    "            for i, gat_layer in enumerate(layer_stack):\n",
    "                x = gat_layer(x, hadj[heter_idx]) # output: (bs, n_head, n, f_out)\n",
    "                if i + 1 == self.n_layer:\n",
    "                    x = x.mean(dim=-3) # (bs, n_head, n, f_out) -> (bs, n, f_out)\n",
    "                else:\n",
    "                    x = F.elu(x.reshape(bs, n, -1))\n",
    "                    x = F.dropout(x, self.dropout, training=self.training)\n",
    "            logger.info(x)\n",
    "            heter_embs.append(x[:,:self.n_user].unsqueeze(-2)) # (bs, Nu, 1, f_out)\n",
    "        type_aware_emb = torch.cat(heter_embs, dim=-2) # (bs, Nu, |Rs|, D')\n",
    "        type_fusion_emb = self.additive_attention(h[:,:self.n_user], type_aware_emb) # (bs, Nu, 1, D')\n",
    "        ret = self.fc_layer(\n",
    "            torch.cat((type_fusion_emb, type_aware_emb), dim=-2).reshape(bs, self.n_user,-1) # (bs, Nu, |Rs|+1, D') -> (bs, Nu, (|Rs|+1)*D')\n",
    "        ) #  (bs, Nu, nb_classes)\n",
    "        return F.log_softmax(ret, dim=-1)\n",
    "\n",
    "model = HeterGraphAttentionNetwork2(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    model.to(GPU_MODEL)\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "output = model(feats, torch.stack([uu_adjs, ut_adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "model = HeterGraphAttentionNetwork(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    # model.cuda()\n",
    "    model.to(GPU_MODEL)\n",
    "    # class_weight = class_weight.cuda()\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "# params = [{'params': model.parameters()}]\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 12:14:19,308 out Allocated: 0.001953125\n"
     ]
    }
   ],
   "source": [
    "# logger.info(f\"out Allocated: {torch.cuda.memory_reserved(int(GPU_MODEL[-1]))/1024**3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 12:42:28,618 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:28,620 torch.Size([256, 2])\n",
      "2022-09-22 12:42:29,528 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:29,529 torch.Size([256, 2])\n",
      "2022-09-22 12:42:30,180 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:30,181 torch.Size([256, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7338/3924587083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#     train(epoch, train_loader, valid_loader, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbest_thr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_best_thr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_desc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_thr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_desc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \"\"\"\n",
      "\u001b[0;32m/tmp/ipykernel_7338/3924587083.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(epoch, loader, thr, return_best_thr, log_desc)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeats\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPU_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muu_adjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mut_adjs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# choose last user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Heter-GAT/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, hadj)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgat_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgat_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhadj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheter_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output: (bs, n_head, n, f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (bs, n_head, n, f_out) -> (bs, n, f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Heter-GAT/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mh_prime_lifted\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mh_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (E,k,f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_src_lifted\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mattn_trg_lifted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mneigh_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_neigh_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mneigh_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate(epoch, loader, thr=None, return_best_thr=False, log_desc='valid_'):\n",
    "    model.eval()\n",
    "    total = 0.\n",
    "    loss, prec, rec, f1 = 0., 0., 0., 0.\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "    for i_batch, batch in enumerate(loader):\n",
    "        uu_adjs, ut_adjs, labels, feats = batch\n",
    "        bs = uu_adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            uu_adjs = uu_adjs.to(GPU_MODEL)\n",
    "            ut_adjs = ut_adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        output = model(feats, torch.stack([uu_adjs, ut_adjs]))\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_batch = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_batch.item()\n",
    "\n",
    "        y_true += labels.data.tolist()\n",
    "        # 返回output中每行最大值的索引\n",
    "        y_pred += output.max(1)[1].data.tolist()\n",
    "        y_score += output[:, 1].data.tolist()\n",
    "        total += bs\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if thr is not None:\n",
    "        logger.info(\"using threshold %.4f\", thr)\n",
    "        y_score = np.array(y_score)\n",
    "        y_pred = np.zeros_like(y_score)\n",
    "        y_pred[y_score > thr] = 1\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    logger.info(\"%sloss: %.4f AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f\",\n",
    "            log_desc, loss / total, auc, prec, rec, f1)\n",
    "\n",
    "    if return_best_thr:\n",
    "        precs, recs, thrs = precision_recall_curve(y_true, y_score)\n",
    "        f1s = 2 * precs * recs / (precs + recs)\n",
    "        f1s = f1s[:-1]\n",
    "        thrs = thrs[~np.isnan(f1s)]\n",
    "        f1s = f1s[~np.isnan(f1s)]\n",
    "        best_thr = thrs[np.argmax(f1s)]\n",
    "        logger.info(\"best threshold=%4f, f1=%.4f\", best_thr, np.max(f1s))\n",
    "        return best_thr\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
    "    model.train()\n",
    "\n",
    "    loss = 0.\n",
    "    total = 0.\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        # if i_batch % 10 == 0:\n",
    "        #     logger.info(f\"i_batch={i_batch}\")\n",
    "        uu_adjs, ut_adjs, labels, feats = batch\n",
    "        bs = uu_adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            uu_adjs = uu_adjs.to(GPU_MODEL)\n",
    "            ut_adjs = ut_adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.rand((feats.shape)).to(GPU_MODEL), torch.stack([uu_adjs, ut_adjs]))\n",
    "        logger.info(output.shape)\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_train = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_train.item()\n",
    "        total += bs\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
    "    if (epoch + 1) % args[\"check_point\"] == 0:\n",
    "        logger.info(\"epoch %d, checkpoint!\", epoch)\n",
    "        best_thr = evaluate(epoch, valid_loader, return_best_thr=True, log_desc='valid_')\n",
    "        evaluate(epoch, test_loader, thr=best_thr, log_desc='test_')\n",
    "\n",
    "# for epoch in range(args[\"epochs\"]):\n",
    "#     train(epoch, train_loader, valid_loader, test_loader)\n",
    "\n",
    "best_thr = evaluate(0, valid_loader, return_best_thr=True, log_desc='valid_')\n",
    "evaluate(0, test_loader, thr=best_thr, log_desc='test_')\n",
    "\"\"\"\n",
    "2022-09-19 18:36:01,818 train loss in this epoch 0.304292\n",
    "2022-09-19 18:45:58,481 train loss in this epoch 0.096567\n",
    "2022-09-19 18:45:58,484 epoch 1, checkpoint!\n",
    "2022-09-19 18:47:17,276 valid_loss: 0.0495 AUC: 0.9972 Prec: 0.9972 Rec: 1.0000 F1: 0.9986\n",
    "/tmp/ipykernel_30944/2709401001.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
    "  f1s = 2 * precs * recs / (precs + recs)\n",
    "2022-09-19 18:47:17,301 best threshold=-0.040288, f1=0.9990\n",
    "2022-09-19 18:48:35,552 using threshold -0.0403\n",
    "2022-09-19 18:48:35,601 test_loss: 0.0501 AUC: 0.9976 Prec: 0.9975 Rec: 1.0000 F1: 0.9988\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
