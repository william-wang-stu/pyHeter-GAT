{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\\n方法:\\n1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\\n2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\\n   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\\n3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\n",
    "方法:\n",
    "1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\n",
    "2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\n",
    "   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\n",
    "3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 15:19:52,519 Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-10-25 15:19:52,520 NumExpr defaulting to 8 threads.\n",
      "2022-10-25 07:19:53.304131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-10-25 15:19:54,658 Reading From config.ini... DATA_ROOTPATH=/remote-home/share/dmb_nas/wangzejian\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from lib.log import logger\n",
    "from utils import load_pickle, save_pickle, ChunkSampler, SubGraphSample, load_w2v_feature\n",
    "from model import BatchdenseGAT, HeterdenseGAT\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve\n",
    "from tensorboard_logger import tensorboard_logger\n",
    "from torch.utils.data import Dataset\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "DATA_ROOTPATH = config['DEFAULT']['DataRootPath']\n",
    "logger.info(f\"Reading From config.ini... DATA_ROOTPATH={DATA_ROOTPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_feats = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/doc2topic_tweetfeat.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_feats.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ids = load_pickle(os.path.join(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20\", \"extended_vertices.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20903095, 0.00625   , 0.44792733, ..., 0.00625   , 0.00625   ,\n",
       "        0.00625   ],\n",
       "       [0.005     , 0.005     , 0.48467028, ..., 0.005     , 0.005     ,\n",
       "        0.005     ],\n",
       "       [0.00714286, 0.00714286, 0.53198843, ..., 0.00714286, 0.00714286,\n",
       "        0.00714286],\n",
       "       ...,\n",
       "       [0.00384615, 0.00384615, 0.92692308, ..., 0.00384615, 0.00384615,\n",
       "        0.00384615],\n",
       "       [0.00454545, 0.00454545, 0.79811734, ..., 0.00454545, 0.00454545,\n",
       "        0.00454545],\n",
       "       [0.00454545, 0.00454545, 0.64677627, ..., 0.00454545, 0.00454545,\n",
       "        0.00454545]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58185, 100, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_feats[vertex_ids[:,20:]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_random_tweet_ids(samples: SubGraphSample, outdir: str, tweets_per_user:int=5):\n",
    "#     tweet_ids = []\n",
    "#     sample_ids = []\n",
    "#     ut_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/utmp_groupbystage.p\"))\n",
    "\n",
    "#     for idx in range(len(samples.labels)):\n",
    "#         if idx and idx % 10000 == 0:\n",
    "#             logger.info(f\"idx={idx}, sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "#         stage = samples.time_stages[idx]\n",
    "#         selected_tweet_ids  = set()\n",
    "#         candidate_tweet_ids = set()\n",
    "#         for vertex_id in samples.vertex_ids[idx]:\n",
    "#             available_tweet_ids = ut_mp[stage][vertex_id]\n",
    "#             random_ids = np.random.choice(available_tweet_ids, size=min(tweets_per_user, len(available_tweet_ids)), replace=False)\n",
    "#             selected_tweet_ids  |= set(random_ids)\n",
    "#             candidate_tweet_ids |= set(available_tweet_ids)-set(random_ids)\n",
    "#         candidate_tweet_ids -= selected_tweet_ids\n",
    "#         # logger.info(f\"Length: sample={len(selected_tweet_ids)}, remain={len(candidate_tweet_ids)}, expected={len(samples.vertex_ids[idx])*tweets_per_user}\")\n",
    "\n",
    "#         if len(selected_tweet_ids) != len(samples.vertex_ids[idx])*tweets_per_user:\n",
    "#             diff = len(samples.vertex_ids[idx])*tweets_per_user - len(selected_tweet_ids)\n",
    "#             if diff > len(candidate_tweet_ids):\n",
    "#                 continue\n",
    "#             selected_tweet_ids |= set(np.random.choice(list(candidate_tweet_ids), size=diff, replace=False))\n",
    "#         sample_ids.append(idx)\n",
    "#         tweet_ids.append(selected_tweet_ids)\n",
    "#     logger.info(f\"Finish Sampling Random Tweets... sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "\n",
    "#     os.makedirs(outdir, exist_ok=True)\n",
    "#     selected_samples = SubGraphSample(\n",
    "#         adj_matrices=samples.adj_matrices[sample_ids],\n",
    "#         influence_features=samples.influence_features[sample_ids],\n",
    "#         vertex_ids=samples.vertex_ids[sample_ids],\n",
    "#         labels=samples.labels[sample_ids],\n",
    "#         tags=samples.tags[sample_ids],\n",
    "#         time_stages=samples.time_stages[sample_ids],\n",
    "#     )\n",
    "#     save_pickle(sample_ids, os.path.join(outdir, \"sample_ids.p\"))\n",
    "#     save_pickle(tweet_ids, os.path.join(outdir, \"tweet_ids.p\"))\n",
    "#     save_pickle(selected_samples, os.path.join(outdir, \"selected_samples.p\"))\n",
    "#     logger.info(\"Finish Saving pkl...\")\n",
    "\n",
    "# def extend_subnetwork(file_dir: str):\n",
    "#     hs_filedir = os.path.join(DATA_ROOTPATH, file_dir).replace('stages_', 'hs_')\n",
    "#     samples = load_pickle(os.path.join(hs_filedir, \"selected_samples.p\"))\n",
    "#     tweet_ids = load_pickle(os.path.join(hs_filedir, \"tweet_ids.p\"))\n",
    "#     assert len(samples) == len(tweet_ids)\n",
    "\n",
    "#     tweetid2userid_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/tweetid2userid_mp.p\"))\n",
    "#     vertex_ids = samples.vertex_ids\n",
    "#     adjs       = samples.adj_matrices\n",
    "#     adjs[adjs != 0] = 1.0\n",
    "#     adjs = adjs.astype(np.dtype('B'))\n",
    "\n",
    "#     extended_vertices, extended_adjs = [], []\n",
    "#     for idx in range(len(samples)):\n",
    "#         subnetwork = np.array(np.concatenate((vertex_ids[idx], np.array(list(tweet_ids[idx])))), dtype=int)\n",
    "#         extended_vertices.append(subnetwork)\n",
    "\n",
    "#         subnetwork_size, num_users = len(subnetwork), len(vertex_ids[idx])\n",
    "#         elem_idx_mp = {elem:idx for idx,elem in enumerate(subnetwork)}\n",
    "#         uu_adj = np.array([[0]*subnetwork_size for _ in range(subnetwork_size)], dtype='B')\n",
    "#         uu_adj[:num_users,:num_users] = adjs[idx]\n",
    "#         # NOTE: Get Corresponding User_id By Tweet_id, and then convert them into indexes in extend_subnetwork\n",
    "#         ut_adj = copy.deepcopy(uu_adj)\n",
    "#         for tweet_id in tweet_ids[idx]:\n",
    "#             user_id = tweetid2userid_mp[tweet_id]\n",
    "#             net_userid = elem_idx_mp[user_id]\n",
    "#             net_tweetid = elem_idx_mp[tweet_id]\n",
    "#             ut_adj[net_userid][net_tweetid] = 1\n",
    "#         extended_adjs.append([uu_adj, ut_adj])\n",
    "#     extended_vertices, extended_adjs = np.array(extended_vertices), np.array(extended_adjs)\n",
    "#     save_pickle(extended_vertices, os.path.join(hs_filedir, \"extended_vertices.p\"))\n",
    "#     save_pickle(extended_adjs, os.path.join(hs_filedir, \"extended_adjs.p\"))\n",
    "\n",
    "# data_dirpath = os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n",
    "# samples = SubGraphSample(\n",
    "#     adj_matrices=np.load(os.path.join(data_dirpath, \"adjacency_matrix.npy\")),\n",
    "#     influence_features=np.load(os.path.join(data_dirpath, \"influence_feature.npy\")),\n",
    "#     vertex_ids=np.load(os.path.join(data_dirpath, \"vertex_id.npy\")),\n",
    "#     labels=np.load(os.path.join(data_dirpath, \"label.npy\")),\n",
    "#     tags=np.load(os.path.join(data_dirpath, \"hashtag.npy\")),\n",
    "#     time_stages=np.load(os.path.join(data_dirpath, \"stage.npy\"))\n",
    "# )\n",
    "# # gen_random_tweet_ids(samples, os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\"))\n",
    "# # extend_subnetwork(\"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_user_emb(tot_user_num):\n",
    "#     # 208894*200*8*3\n",
    "#     user_feats = [[0.]*200*8*3 for _ in range(tot_user_num)]\n",
    "#     for tag in range(200):\n",
    "#         logger.info(f\"tag={tag}\")\n",
    "#         for stage in range(8):\n",
    "#             for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "#                 feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{tag}_t{stage}.p\")\n",
    "#                 for idx in range(tot_user_num):\n",
    "#                     user_feats[idx][tag*3*8+stage*3+feats_idx] = float(feats[idx])\n",
    "#     logger.info(f\"shape={user_feats.shape}\")\n",
    "#     return torch.FloatTensor(user_feats)\n",
    "\n",
    "# user_emb = gen_user_emb(208894)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Fake Digg Heter Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from utils import SubGraphSample, load_w2v_feature\n",
    "\n",
    "class DiggDataset(Dataset):\n",
    "    def __init__(self, samples: SubGraphSample, embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.adjs = samples.adj_matrices\n",
    "        self.labels = samples.labels\n",
    "        self.feats = samples.influence_features\n",
    "        self.vertex_ids = samples.vertex_ids\n",
    "        self.concact_feats(embedding)\n",
    "    def concact_feats(self, embedding):\n",
    "        feats = []\n",
    "        for idx, vertex_ids in enumerate(self.vertex_ids):\n",
    "            emb_feats = [embedding[user] for user in vertex_ids]\n",
    "            feats.append(np.concatenate((self.feats[idx], emb_feats), axis=1))\n",
    "        self.feats = np.array(feats)\n",
    "        logger.info(self.feats.shape)\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.adjs[index], self.labels[index], self.feats[index]\n",
    "\n",
    "def collate_fn2(batch:list): \n",
    "    \"\"\"\n",
    "    Collate function which to transform scipy coo matrix to pytorch sparse tensor\n",
    "    \"\"\"\n",
    "    adjs_batch, labels_batch, feats_batch = zip(*batch)\n",
    "    adjs_batch = torch.FloatTensor(np.array(adjs_batch))\n",
    "    \n",
    "    if type(labels_batch[0]).__module__ == 'numpy':\n",
    "        # NOTE: https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "        labels_batch = torch.LongTensor(labels_batch)\n",
    "    \n",
    "    if type(feats_batch[0]).__module__ == 'numpy':\n",
    "        feats_batch = torch.FloatTensor(np.array(feats_batch))\n",
    "    return adjs_batch, labels_batch, feats_batch\n",
    "\n",
    "def digg_load_dataset(train_ratio=60, valid_ratio=20, batch_size=256):\n",
    "    embedding_path = \"/root/Lab_Related/data/Heter-GAT/Classic/deepwalk/deepwalk_added.emb_64\"\n",
    "    vertices = np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\")\n",
    "    max_vertex_idx = np.max(vertices)\n",
    "    embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
    "    # embedding = torch.FloatTensor(embedding)\n",
    "\n",
    "    samples = SubGraphSample(\n",
    "        adj_matrices=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/adjacency_matrix.npy\"),\n",
    "        influence_features=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/influence_feature.npy\"),\n",
    "        vertex_ids=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\"),\n",
    "        labels=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/label.npy\")\n",
    "    )\n",
    "    dataset = DiggDataset(samples, embedding)\n",
    "    nb_samples    = len(dataset)\n",
    "    \n",
    "    train_start,  valid_start, test_start = 0, int(nb_samples*train_ratio/100), int(nb_samples*(train_ratio+valid_ratio)/100)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(valid_start-train_start, 0), collate_fn=collate_fn2)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(test_start-valid_start, valid_start), collate_fn=collate_fn2)\n",
    "    test_loader  = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(nb_samples - test_start, test_start), collate_fn=collate_fn2)\n",
    "    logger.info(f\"Finish Loading Dataset... train={len(train_loader)}, valid={len(valid_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    return samples, train_loader, valid_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('Heter-GAT-afYRWUpf': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d5a35b220c759fdf3543f8c8f7df3e7cf51856a17fdad347db68213ac2aaee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
