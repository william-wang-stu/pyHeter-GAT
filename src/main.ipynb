{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\\n方法:\\n1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\\n2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\\n   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\\n3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\n",
    "方法:\n",
    "1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\n",
    "2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\n",
    "   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\n",
    "3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.dirname(os.getcwd()))\n",
    "# from lib.log import logger\n",
    "# from utils import load_pickle, save_pickle, summarize_distribution, find_rt_bound, HeterSubGraphSample\n",
    "# from lib.utils import get_node_types, extend_edges, create_sparse, get_sparse_tensor\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 18:23:26,046 Note: NumExpr detected 56 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-10-21 18:23:26,048 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from lib.log import logger\n",
    "from lib.utils import get_sparse_tensor\n",
    "from utils import HeterSubGraphSample, load_pickle, save_pickle, init_args, ChunkSampler, HeterGraphDataset, sparse_batch_collate\n",
    "from model import HeterGraphAttentionNetwork\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve\n",
    "from tensorboard_logger import tensorboard_logger\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = load_pickle(\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/heter_samples.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  39949,   23831,   15772,    4542,   21571,   25925,   40133,\n",
       "         20049,   16095,   36448,   20447,    1637,   38896,    3573,\n",
       "         12535,   35704,   15481,   15994,    2427,   40452, 4369418,\n",
       "       4369419, 4369420, 4369423, 4531220, 4531221, 4531222, 4531225,\n",
       "       4531226, 4531227, 4369443, 4369444, 4531243, 4531244, 4531246,\n",
       "       4531247, 4369461, 4531253, 4531265, 4503658, 4369483, 4369484,\n",
       "       4369487, 4503633, 4503634, 4531284, 4503636, 4531285, 4503639,\n",
       "       4503640, 4503641, 4503642, 4531291, 4503644, 4503645, 4503643,\n",
       "       5553759, 5553760, 4503648, 4503650, 5553763, 5553764, 4503653,\n",
       "       4503652, 4531298, 5553768, 4503657, 5553770, 4503659, 4503651,\n",
       "       5553762, 4503655, 4503663, 4503662, 4503656, 4503666, 4503665,\n",
       "       5553771, 4503669, 4503661, 5553775, 4503672, 5553776, 4503674,\n",
       "       4503673, 4503676, 4503675, 4503664, 4503679, 4503677, 4503681,\n",
       "       4503667, 5553806, 4503668, 5553808, 5553810, 5553812, 5553772,\n",
       "       4503670, 5553821, 5553773, 5553826, 5553774, 5553829, 5553856,\n",
       "       5553866, 5660501, 5660502, 4503407, 4503429, 4503434, 4531158,\n",
       "       4531160, 9603056, 9603057, 9603058, 9603059, 4369403, 4369404,\n",
       "       4369405])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.vertex_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = load_pickle(\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/heter_samples.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49926, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Fake Digg Heter Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from utils import SubGraphSample, load_w2v_feature\n",
    "\n",
    "class DiggDataset(Dataset):\n",
    "    def __init__(self, samples: SubGraphSample, embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.adjs = samples.adj_matrices\n",
    "        self.labels = samples.labels\n",
    "        self.feats = samples.influence_features\n",
    "        self.vertex_ids = samples.vertex_ids\n",
    "        self.concact_feats(embedding)\n",
    "    def concact_feats(self, embedding):\n",
    "        feats = []\n",
    "        for idx, vertex_ids in enumerate(self.vertex_ids):\n",
    "            emb_feats = [embedding[user] for user in vertex_ids]\n",
    "            feats.append(np.concatenate((self.feats[idx], emb_feats), axis=1))\n",
    "        self.feats = np.array(feats)\n",
    "        logger.info(self.feats.shape)\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.adjs[index], self.labels[index], self.feats[index]\n",
    "\n",
    "def collate_fn2(batch:list): \n",
    "    \"\"\"\n",
    "    Collate function which to transform scipy coo matrix to pytorch sparse tensor\n",
    "    \"\"\"\n",
    "    adjs_batch, labels_batch, feats_batch = zip(*batch)\n",
    "    adjs_batch = torch.FloatTensor(np.array(adjs_batch))\n",
    "    \n",
    "    if type(labels_batch[0]).__module__ == 'numpy':\n",
    "        # NOTE: https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "        labels_batch = torch.LongTensor(labels_batch)\n",
    "    \n",
    "    if type(feats_batch[0]).__module__ == 'numpy':\n",
    "        feats_batch = torch.FloatTensor(np.array(feats_batch))\n",
    "    return adjs_batch, labels_batch, feats_batch\n",
    "\n",
    "def digg_load_dataset(train_ratio=60, valid_ratio=20, batch_size=256):\n",
    "    embedding_path = \"/root/Lab_Related/data/Heter-GAT/Classic/deepwalk/deepwalk_added.emb_64\"\n",
    "    vertices = np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\")\n",
    "    max_vertex_idx = np.max(vertices)\n",
    "    embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
    "    # embedding = torch.FloatTensor(embedding)\n",
    "\n",
    "    samples = SubGraphSample(\n",
    "        adj_matrices=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/adjacency_matrix.npy\"),\n",
    "        influence_features=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/influence_feature.npy\"),\n",
    "        vertex_ids=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/vertex_id.npy\"),\n",
    "        labels=np.load(\"/root/TR-pptusn/DeepInf-preprocess/preprocess/stages_op_inf_100_1k/label.npy\")\n",
    "    )\n",
    "    dataset = DiggDataset(samples, embedding)\n",
    "    nb_samples    = len(dataset)\n",
    "    \n",
    "    train_start,  valid_start, test_start = 0, int(nb_samples*train_ratio/100), int(nb_samples*(train_ratio+valid_ratio)/100)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(valid_start-train_start, 0), collate_fn=collate_fn2)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(test_start-valid_start, valid_start), collate_fn=collate_fn2)\n",
    "    test_loader  = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(nb_samples - test_start, test_start), collate_fn=collate_fn2)\n",
    "    logger.info(f\"Finish Loading Dataset... train={len(train_loader)}, valid={len(valid_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    return samples, train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 15:42:11,284 n=204955, d=64\n",
      "2022-10-12 15:42:21,005 (38152, 50, 66)\n",
      "2022-10-12 15:42:21,017 Finish Loading Dataset... train=90, valid=30, test=30\n",
      "2022-10-12 15:42:21,184 nb_samples=38152, class_weight=0.57:4.05, n_units=[66, 16, 16], n_heads=[8, 8]\n"
     ]
    }
   ],
   "source": [
    "GPU_MODEL = 'cuda:2'\n",
    "\n",
    "# 1. \n",
    "def load_dataset(data_filepath:str, train_ratio:float, valid_ratio:float, batch_size:int):\n",
    "    # heter_samples = load_pickle(os.path.join(data_dirpath, \"heter_samples_tensor.p\"))\n",
    "    heter_samples = load_pickle(data_filepath)\n",
    "    dataset       = HeterGraphDataset(heter_samples=heter_samples)\n",
    "    nb_samples    = len(dataset)\n",
    "    \n",
    "    train_start,  valid_start, test_start = 0, int(nb_samples*train_ratio/100), int(nb_samples*(train_ratio+valid_ratio)/100)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(valid_start-train_start, 0), collate_fn=sparse_batch_collate)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(test_start-valid_start, valid_start), collate_fn=sparse_batch_collate)\n",
    "    test_loader  = DataLoader(dataset, batch_size=batch_size, sampler=ChunkSampler(nb_samples - test_start, test_start), collate_fn=sparse_batch_collate)\n",
    "    logger.info(f\"Finish Loading Dataset... train={len(train_loader)}, valid={len(valid_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    return heter_samples, train_loader, valid_loader, test_loader\n",
    "\n",
    "# args = init_args()\n",
    "args = {\n",
    "    \"train_ratio\": 60,\n",
    "    \"valid_ratio\": 20,\n",
    "    \"batch\": 256,\n",
    "    # \"nb_user\": 50,\n",
    "    \"class_weight_balanced\": True,\n",
    "    \"hidden_units\": \"16,16\",\n",
    "    \"heads\": \"8,8\",\n",
    "    \"cuda\": True,\n",
    "    \"lr\": 0.1,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 100,\n",
    "    \"check_point\": 2,\n",
    "    # \"file_dir\": \"heter_samples_ratio1/0/heter_samples.p\",\n",
    "    \"file_dir\": \"hs_new_ratio1/heter_samples.p\",\n",
    "}\n",
    "# heter_samples, train_loader, valid_loader, test_loader = load_dataset(args[\"file_dir\"], args[\"train_ratio\"], args[\"valid_ratio\"], args[\"batch\"])\n",
    "samples, train_loader, valid_loader, test_loader = digg_load_dataset()\n",
    "nb_samples = len(samples)\n",
    "nb_classes = 2\n",
    "class_weight = torch.FloatTensor(nb_samples / (nb_classes*np.bincount(samples.labels))) if args[\"class_weight_balanced\"] else torch.ones(nb_classes)\n",
    "nb_user = 50\n",
    "n_units = [samples.influence_features.shape[2]+64]+[int(x) for x in args[\"hidden_units\"].strip().split(\",\")]\n",
    "n_heads = [int(x) for x in args[\"heads\"].strip().split(\",\")]\n",
    "logger.info(f\"nb_samples={nb_samples}, class_weight={class_weight[0]:.2f}:{class_weight[1]:.2f}, n_units={n_units}, n_heads={n_heads}\")\n",
    "# logger.info(\"class_weight=%.2f:%.2f\", class_weight[0], class_weight[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "import copy\n",
    "from model import BatchSparseMultiHeadGraphAttention, BatchMultiHeadGraphAttention, BatchAdditiveAttention\n",
    "\n",
    "class HeterGraphAttentionNetwork2(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_user, nb_node_kinds=2, nb_loop_nodes=[50,1050],\n",
    "        nb_classes=2, n_units=[25,64], n_heads=[3],\n",
    "        attn_dropout=0.5, dropout=0.1, \n",
    "        d2=64, gpu_device_ids=[] \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gpu_device_ids(List[int], default=[]): 采用Model Parallel方法, 主要使多头注意力可以在不同GPU上运行, \n",
    "                模型以数据所属GPU为例, 先在该参数指定的不同GPU上执行单一注意力头, 再在最后将所有注意力头的运行结果复制回数据所属的主GPU上\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layer = len(n_units) - 1\n",
    "        self.dropout = dropout\n",
    "        self.gpu_device_ids = gpu_device_ids\n",
    "\n",
    "        self.d = n_units[0]\n",
    "        self.d1 = n_units[1]\n",
    "        self.d2 = n_units[1]\n",
    "        self.n_user = n_user\n",
    "\n",
    "        self.layer_stack = nn.ModuleList()\n",
    "        for hidx in range(nb_node_kinds):\n",
    "            layer_stack = nn.ModuleList()\n",
    "            for i in range(self.n_layer):\n",
    "                # consider multi head from last layer\n",
    "                f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
    "                layer_stack.append(\n",
    "                    # BatchMultiHeadGraphAttention(nb_heads=n_heads[i], nb_in_feats=f_in, nb_out_feats=n_units[i + 1], \n",
    "                    #     nb_loop_nodes=nb_loop_nodes[hidx], attn_dropout=attn_dropout)\n",
    "                    BatchMultiHeadGraphAttention(n_head=n_heads[i], f_in=f_in, f_out=n_units[i+1], attn_dropout=attn_dropout)\n",
    "                )\n",
    "            self.layer_stack.append(layer_stack)\n",
    "        self.additive_attention = BatchAdditiveAttention(d=self.d, d1=self.d1, d2=self.d2)\n",
    "        self.fc_layer = nn.Linear(in_features=self.d1*(nb_node_kinds+1), out_features=nb_classes)\n",
    "    \n",
    "    def forward(self, h, hadj):\n",
    "        # NOTE: h: (bs, N, fin), hadj: (|Rs|, bs, N, N)\n",
    "        bs, n = h.shape[:2]\n",
    "        heter_embs = []\n",
    "        for heter_idx, layer_stack in enumerate(self.layer_stack):\n",
    "            x = copy.deepcopy(h)\n",
    "            for i, gat_layer in enumerate(layer_stack):\n",
    "                x = gat_layer(x, hadj[heter_idx]) # output: (bs, n_head, n, f_out)\n",
    "                if i + 1 == self.n_layer:\n",
    "                    x = x.mean(dim=-3) # (bs, n_head, n, f_out) -> (bs, n, f_out)\n",
    "                else:\n",
    "                    x = F.elu(x.reshape(bs, n, -1))\n",
    "                    x = F.dropout(x, self.dropout, training=self.training)\n",
    "            heter_embs.append(x[:,:self.n_user].unsqueeze(-2)) # (bs, Nu, 1, f_out)\n",
    "        type_aware_emb = torch.cat(heter_embs, dim=-2) # (bs, Nu, |Rs|, D')\n",
    "        type_fusion_emb = self.additive_attention(h[:,:self.n_user], type_aware_emb) # (bs, Nu, 1, D')\n",
    "        ret = self.fc_layer(\n",
    "            torch.cat((type_fusion_emb, type_aware_emb), dim=-2).reshape(bs, self.n_user,-1) # (bs, Nu, |Rs|+1, D') -> (bs, Nu, (|Rs|+1)*D')\n",
    "        ) #  (bs, Nu, nb_classes)\n",
    "        return F.log_softmax(ret, dim=-1)\n",
    "\n",
    "model = HeterGraphAttentionNetwork2(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    # model.cuda()\n",
    "    model.to(GPU_MODEL)\n",
    "    # class_weight = class_weight.cuda()\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "# params = [{'params': model.parameters()}]\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 15:46:18,982 train loss in this epoch 1.182631\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
    "    model.train()\n",
    "\n",
    "    loss = 0.\n",
    "    total = 0.\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        # if i_batch % 10 == 0:\n",
    "        #     logger.info(f\"i_batch={i_batch}\")\n",
    "        adjs, labels, feats = batch\n",
    "        bs = adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            adjs    = adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feats, torch.stack([adjs, adjs]))\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_train = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_train.item()\n",
    "        total += bs\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
    "\n",
    "train(0, train_loader, valid_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, batch in enumerate(train_loader):\n",
    "#     uu_adjs, ut_adjs, labels, feats = batch\n",
    "#     bs = uu_adjs.size(0)\n",
    "\n",
    "#     # feats = feats[:,:100]\n",
    "#     # labels = labels\n",
    "#     # nw_uu_adjs, nw_ut_adjs = [], []\n",
    "#     # for idx in range(len(uu_adjs)):\n",
    "#     #     nw_uu_adjs.append(torch.LongTensor(uu_adjs[idx].to_dense().numpy()[:100,:100]+np.eye(100)))\n",
    "#     #     nw_ut_adjs.append(torch.LongTensor(ut_adjs[idx].to_dense().numpy()[:100,:100]+np.eye(100)))\n",
    "#     # uu_adjs = torch.stack(nw_uu_adjs)\n",
    "#     # ut_adjs = torch.stack(nw_ut_adjs)\n",
    "\n",
    "#     if args[\"cuda\"]:\n",
    "#         uu_adjs, ut_adjs, labels, feats = uu_adjs.to(GPU_MODEL), ut_adjs.to(GPU_MODEL), labels.to(GPU_MODEL), feats.to(GPU_MODEL)\n",
    "#     break\n",
    "\n",
    "# model = HeterGraphAttentionNetwork(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "# if args[\"cuda\"]:\n",
    "#     model.to(GPU_MODEL)\n",
    "#     class_weight = class_weight.to(GPU_MODEL)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "# output = model(feats, torch.stack([uu_adjs, ut_adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "from model import BatchSparseMultiHeadGraphAttention, BatchAdditiveAttention, BatchMultiHeadGraphAttention\n",
    "import copy\n",
    "\n",
    "class HeterGraphAttentionNetwork2(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_user, nb_node_kinds=2, nb_loop_nodes=[50,1050],\n",
    "        nb_classes=2, n_units=[25,64], n_heads=[3],\n",
    "        attn_dropout=0.5, dropout=0.1, \n",
    "        d2=64, gpu_device_ids=[] \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layer = len(n_units) - 1\n",
    "        self.dropout = dropout\n",
    "        self.gpu_device_ids = gpu_device_ids\n",
    "\n",
    "        self.d = n_units[0]\n",
    "        self.d1 = n_units[1]\n",
    "        self.d2 = d2\n",
    "        self.n_user = n_user\n",
    "\n",
    "        self.layer_stack = nn.ModuleList()\n",
    "        for hidx in range(nb_node_kinds):\n",
    "            layer_stack = nn.ModuleList()\n",
    "            for i in range(self.n_layer):\n",
    "                # consider multi head from last layer\n",
    "                f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
    "                layer_stack.append(\n",
    "                    BatchMultiHeadGraphAttention(n_head=n_heads[i], f_in=f_in, f_out=n_units[i + 1], \n",
    "                        attn_dropout=attn_dropout)\n",
    "                )\n",
    "            self.layer_stack.append(layer_stack)\n",
    "        self.additive_attention = BatchAdditiveAttention(d=self.d, d1=self.d1, d2=self.d2)\n",
    "        self.fc_layer = nn.Linear(in_features=self.d1*(nb_node_kinds+1), out_features=nb_classes)\n",
    "    \n",
    "    def forward(self, h, hadj):\n",
    "        # NOTE: h: (bs, N, fin), hadj: (|Rs|, bs, N, N)\n",
    "        bs, n = h.shape[:2]\n",
    "        heter_embs = []\n",
    "        for heter_idx, layer_stack in enumerate(self.layer_stack):\n",
    "            x = copy.deepcopy(h)\n",
    "            for i, gat_layer in enumerate(layer_stack):\n",
    "                x = gat_layer(x, hadj[heter_idx]) # output: (bs, n_head, n, f_out)\n",
    "                if i + 1 == self.n_layer:\n",
    "                    x = x.mean(dim=-3) # (bs, n_head, n, f_out) -> (bs, n, f_out)\n",
    "                else:\n",
    "                    x = F.elu(x.reshape(bs, n, -1))\n",
    "                    x = F.dropout(x, self.dropout, training=self.training)\n",
    "            logger.info(x)\n",
    "            heter_embs.append(x[:,:self.n_user].unsqueeze(-2)) # (bs, Nu, 1, f_out)\n",
    "        type_aware_emb = torch.cat(heter_embs, dim=-2) # (bs, Nu, |Rs|, D')\n",
    "        type_fusion_emb = self.additive_attention(h[:,:self.n_user], type_aware_emb) # (bs, Nu, 1, D')\n",
    "        ret = self.fc_layer(\n",
    "            torch.cat((type_fusion_emb, type_aware_emb), dim=-2).reshape(bs, self.n_user,-1) # (bs, Nu, |Rs|+1, D') -> (bs, Nu, (|Rs|+1)*D')\n",
    "        ) #  (bs, Nu, nb_classes)\n",
    "        return F.log_softmax(ret, dim=-1)\n",
    "\n",
    "model = HeterGraphAttentionNetwork2(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    model.to(GPU_MODEL)\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "output = model(feats, torch.stack([uu_adjs, ut_adjs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "model = HeterGraphAttentionNetwork(n_user=nb_user, n_units=n_units, nb_classes=nb_classes, n_heads=n_heads, dropout=args[\"dropout\"])\n",
    "if args[\"cuda\"]:\n",
    "    # model.cuda()\n",
    "    model.to(GPU_MODEL)\n",
    "    # class_weight = class_weight.cuda()\n",
    "    class_weight = class_weight.to(GPU_MODEL)\n",
    "# params = [{'params': model.parameters()}]\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 12:14:19,308 out Allocated: 0.001953125\n"
     ]
    }
   ],
   "source": [
    "# logger.info(f\"out Allocated: {torch.cuda.memory_reserved(int(GPU_MODEL[-1]))/1024**3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 12:42:28,618 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:28,620 torch.Size([256, 2])\n",
      "2022-09-22 12:42:29,528 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:29,529 torch.Size([256, 2])\n",
      "2022-09-22 12:42:30,180 torch.Size([256, 50, 2])\n",
      "2022-09-22 12:42:30,181 torch.Size([256, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7338/3924587083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#     train(epoch, train_loader, valid_loader, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbest_thr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_best_thr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_desc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_thr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_desc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \"\"\"\n",
      "\u001b[0;32m/tmp/ipykernel_7338/3924587083.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(epoch, loader, thr, return_best_thr, log_desc)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeats\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPU_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muu_adjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mut_adjs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# choose last user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Heter-GAT/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, hadj)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgat_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgat_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhadj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheter_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output: (bs, n_head, n, f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (bs, n_head, n, f_out) -> (bs, n, f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Heter-GAT/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mh_prime_lifted\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mh_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (E,k,f_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_src_lifted\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mattn_trg_lifted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mneigh_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_neigh_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mneigh_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate(epoch, loader, thr=None, return_best_thr=False, log_desc='valid_'):\n",
    "    model.eval()\n",
    "    total = 0.\n",
    "    loss, prec, rec, f1 = 0., 0., 0., 0.\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "    for i_batch, batch in enumerate(loader):\n",
    "        uu_adjs, ut_adjs, labels, feats = batch\n",
    "        bs = uu_adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            uu_adjs = uu_adjs.to(GPU_MODEL)\n",
    "            ut_adjs = ut_adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        output = model(feats, torch.stack([uu_adjs, ut_adjs]))\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_batch = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_batch.item()\n",
    "\n",
    "        y_true += labels.data.tolist()\n",
    "        # 返回output中每行最大值的索引\n",
    "        y_pred += output.max(1)[1].data.tolist()\n",
    "        y_score += output[:, 1].data.tolist()\n",
    "        total += bs\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if thr is not None:\n",
    "        logger.info(\"using threshold %.4f\", thr)\n",
    "        y_score = np.array(y_score)\n",
    "        y_pred = np.zeros_like(y_score)\n",
    "        y_pred[y_score > thr] = 1\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    logger.info(\"%sloss: %.4f AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f\",\n",
    "            log_desc, loss / total, auc, prec, rec, f1)\n",
    "\n",
    "    if return_best_thr:\n",
    "        precs, recs, thrs = precision_recall_curve(y_true, y_score)\n",
    "        f1s = 2 * precs * recs / (precs + recs)\n",
    "        f1s = f1s[:-1]\n",
    "        thrs = thrs[~np.isnan(f1s)]\n",
    "        f1s = f1s[~np.isnan(f1s)]\n",
    "        best_thr = thrs[np.argmax(f1s)]\n",
    "        logger.info(\"best threshold=%4f, f1=%.4f\", best_thr, np.max(f1s))\n",
    "        return best_thr\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
    "    model.train()\n",
    "\n",
    "    loss = 0.\n",
    "    total = 0.\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        # if i_batch % 10 == 0:\n",
    "        #     logger.info(f\"i_batch={i_batch}\")\n",
    "        uu_adjs, ut_adjs, labels, feats = batch\n",
    "        bs = uu_adjs.size(0)\n",
    "\n",
    "        if args[\"cuda\"]:\n",
    "            uu_adjs = uu_adjs.to(GPU_MODEL)\n",
    "            ut_adjs = ut_adjs.to(GPU_MODEL)\n",
    "            labels  = labels.to(GPU_MODEL)\n",
    "            feats   = feats.to(GPU_MODEL)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.rand((feats.shape)).to(GPU_MODEL), torch.stack([uu_adjs, ut_adjs]))\n",
    "        logger.info(output.shape)\n",
    "        output = output[:,-1,:] # choose last user\n",
    "\n",
    "        loss_train = F.nll_loss(output, labels, class_weight)\n",
    "        loss += bs * loss_train.item()\n",
    "        total += bs\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
    "    if (epoch + 1) % args[\"check_point\"] == 0:\n",
    "        logger.info(\"epoch %d, checkpoint!\", epoch)\n",
    "        best_thr = evaluate(epoch, valid_loader, return_best_thr=True, log_desc='valid_')\n",
    "        evaluate(epoch, test_loader, thr=best_thr, log_desc='test_')\n",
    "\n",
    "# for epoch in range(args[\"epochs\"]):\n",
    "#     train(epoch, train_loader, valid_loader, test_loader)\n",
    "\n",
    "best_thr = evaluate(0, valid_loader, return_best_thr=True, log_desc='valid_')\n",
    "evaluate(0, test_loader, thr=best_thr, log_desc='test_')\n",
    "\"\"\"\n",
    "2022-09-19 18:36:01,818 train loss in this epoch 0.304292\n",
    "2022-09-19 18:45:58,481 train loss in this epoch 0.096567\n",
    "2022-09-19 18:45:58,484 epoch 1, checkpoint!\n",
    "2022-09-19 18:47:17,276 valid_loss: 0.0495 AUC: 0.9972 Prec: 0.9972 Rec: 1.0000 F1: 0.9986\n",
    "/tmp/ipykernel_30944/2709401001.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
    "  f1s = 2 * precs * recs / (precs + recs)\n",
    "2022-09-19 18:47:17,301 best threshold=-0.040288, f1=0.9990\n",
    "2022-09-19 18:48:35,552 using threshold -0.0403\n",
    "2022-09-19 18:48:35,601 test_loss: 0.0501 AUC: 0.9976 Prec: 0.9975 Rec: 1.0000 F1: 0.9988\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
