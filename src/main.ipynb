{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\\n方法:\\n1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\\n2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\\n   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\\n3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "思路: 在TR模型的基础上, 融合用户的文本特征, 以异质图Heter-GAT的方式融合用户特征, 做信息传播预测任务\n",
    "方法:\n",
    "1. 整理原始数据, 构建用户有向关联网络, 并根据原始文本内容计算用户文本嵌入向量\n",
    "2. 考虑节点类型为User和Tweet, 边类型为U-U和U-T, 分别从用户特征和文本特征的角度通过GAT网络融合邻域节点特征;\n",
    "   Heter-GAT模型的输出为(N,|Rs|+1,D')维度, 模型后面需要接一个全连接层FC=(|Rs|+1)*D'->2, 损失函数保持为NLL-Loss\n",
    "3. 可视化局部邻域, 观察不同注意力头、不同异质图邻域卷积的偏向\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 20:49:01,861 Reading From config.ini... DATA_ROOTPATH=/remote-home/share/dmb_nas/wangzejian\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from lib.log import logger\n",
    "from lib.utils import get_sparse_tensor\n",
    "from utils import load_pickle, save_pickle, ChunkSampler, SubGraphSample, load_w2v_feature\n",
    "from model import BatchdenseGAT, HeterdenseGAT\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from scipy import sparse\n",
    "from scipy import io as sio\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve\n",
    "from tensorboard_logger import tensorboard_logger\n",
    "from torch.utils.data import Dataset\n",
    "import configparser\n",
    "from dgl.data.utils import download, get_download_dir, _get_dgl_url\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import dgl\n",
    "import errno\n",
    "import pickle\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "DATA_ROOTPATH = config['DEFAULT']['DataRootPath']\n",
    "logger.info(f\"Reading From config.ini... DATA_ROOTPATH={DATA_ROOTPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.dgl/ACM3025.pkl from https://data.dgl.ai/dataset/ACM3025.pkl...\n",
      "dataset loaded\n",
      "{'dataset': 'ACM',\n",
      " 'test': 0.7024793388429752,\n",
      " 'train': 0.19834710743801653,\n",
      " 'val': 0.09917355371900827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24338/3715326685.py:12: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()\n",
    "\n",
    "def load_acm(remove_self_loop):\n",
    "    url = 'dataset/ACM3025.pkl'\n",
    "    data_path = get_download_dir() + '/ACM3025.pkl'\n",
    "    download(_get_dgl_url(url), path=data_path)\n",
    "\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    labels, features = torch.from_numpy(data['label'].todense()).long(), \\\n",
    "                       torch.from_numpy(data['feature'].todense()).float()\n",
    "    num_classes = labels.shape[1]\n",
    "    labels = labels.nonzero()[:, 1]\n",
    "\n",
    "    if remove_self_loop:\n",
    "        num_nodes = data['label'].shape[0]\n",
    "        data['PAP'] = sparse.csr_matrix(data['PAP'] - np.eye(num_nodes))\n",
    "        data['PLP'] = sparse.csr_matrix(data['PLP'] - np.eye(num_nodes))\n",
    "\n",
    "    # Adjacency matrices for meta path based neighbors\n",
    "    # (Mufei): I verified both of them are binary adjacency matrices with self loops\n",
    "    author_g = dgl.from_scipy(data['PAP'])\n",
    "    subject_g = dgl.from_scipy(data['PLP'])\n",
    "    gs = [author_g, subject_g]\n",
    "\n",
    "    train_idx = torch.from_numpy(data['train_idx']).long().squeeze(0)\n",
    "    val_idx = torch.from_numpy(data['val_idx']).long().squeeze(0)\n",
    "    test_idx = torch.from_numpy(data['test_idx']).long().squeeze(0)\n",
    "\n",
    "    num_nodes = author_g.number_of_nodes()\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "\n",
    "    print('dataset loaded')\n",
    "    pprint({\n",
    "        'dataset': 'ACM',\n",
    "        'train': train_mask.sum().item() / num_nodes,\n",
    "        'val': val_mask.sum().item() / num_nodes,\n",
    "        'test': test_mask.sum().item() / num_nodes\n",
    "    })\n",
    "\n",
    "    return gs, features, labels, num_classes, train_idx, val_idx, test_idx, \\\n",
    "           train_mask, val_mask, test_mask\n",
    "\n",
    "g, features, labels, num_classes, train_idx, val_idx, test_idx, train_mask, val_mask, test_mask = load_acm(remove_self_loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.dgl/ACM.mat from https://data.dgl.ai/dataset/ACM.mat...\n"
     ]
    }
   ],
   "source": [
    "def load_acm_raw(remove_self_loop):\n",
    "    assert not remove_self_loop\n",
    "    url = 'dataset/ACM.mat'\n",
    "    data_path = get_download_dir() + '/ACM.mat'\n",
    "    download(_get_dgl_url(url), path=data_path)\n",
    "\n",
    "    data = sio.loadmat(data_path)\n",
    "    p_vs_l = data['PvsL']       # paper-field?\n",
    "    p_vs_a = data['PvsA']       # paper-author\n",
    "    p_vs_t = data['PvsT']       # paper-term, bag of words\n",
    "    p_vs_c = data['PvsC']       # paper-conference, labels come from that\n",
    "\n",
    "    # We assign\n",
    "    # (1) KDD papers as class 0 (data mining),\n",
    "    # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "    # (3) SIGCOMM and MOBICOMM papers as class 2 (communication)\n",
    "    conf_ids = [0, 1, 9, 10, 13]\n",
    "    label_ids = [0, 1, 2, 2, 1]\n",
    "\n",
    "    p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "    p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "    p_vs_l = p_vs_l[p_selected]\n",
    "    p_vs_a = p_vs_a[p_selected]\n",
    "    p_vs_t = p_vs_t[p_selected]\n",
    "    p_vs_c = p_vs_c[p_selected]\n",
    "\n",
    "    hg = dgl.heterograph({\n",
    "        ('paper', 'pa', 'author'): p_vs_a.nonzero(),\n",
    "        ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "        ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "        ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero()\n",
    "    })\n",
    "\n",
    "    features = torch.FloatTensor(p_vs_t.toarray())\n",
    "\n",
    "    pc_p, pc_c = p_vs_c.nonzero()\n",
    "    labels = np.zeros(len(p_selected), dtype=np.int64)\n",
    "    for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "        labels[pc_p[pc_c == conf_id]] = label_id\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    float_mask = np.zeros(len(pc_p))\n",
    "    for conf_id in conf_ids:\n",
    "        pc_c_mask = (pc_c == conf_id)\n",
    "        float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))\n",
    "    train_idx = np.where(float_mask <= 0.2)[0]\n",
    "    val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "    test_idx = np.where(float_mask > 0.3)[0]\n",
    "\n",
    "    num_nodes = hg.number_of_nodes('paper')\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "\n",
    "    return hg, features, labels, num_classes, train_idx, val_idx, test_idx, \\\n",
    "            train_mask, val_mask, test_mask\n",
    "\n",
    "g, features, labels, n_classes, train_nid, val_nid, test_nid, train_mask, val_mask, test_mask = load_acm_raw(remove_self_loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.sampling import RandomWalkNeighborSampler\n",
    "\n",
    "sampler_list = []\n",
    "for metapath in [['pa', 'ap'], ['pf', 'fp']]:\n",
    "    sampler_list.append(\n",
    "        RandomWalkNeighborSampler(G=g, num_traversals=1, termination_prob=0, num_random_walks=20, num_neighbors=20, metapath=metapath)\n",
    "    )\n",
    "\n",
    "def sample_blocks(seeds):\n",
    "    block_list = []\n",
    "\n",
    "    for sampler in sampler_list:\n",
    "        frontier = sampler(seeds)\n",
    "        logger.info(type(frontier))\n",
    "        # add self loop\n",
    "        frontier = dgl.remove_self_loop(frontier)\n",
    "        frontier.add_edges(torch.tensor(seeds), torch.tensor(seeds))\n",
    "        block = dgl.to_block(frontier, seeds)\n",
    "        block_list.append(block)\n",
    "\n",
    "    return seeds, block_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_nid,\n",
    "    batch_size=1024,\n",
    "    collate_fn=sample_blocks,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)\n",
    "\n",
    "for idx, (seeds, blocks) in enumerate(dataloader):\n",
    "    logger.info(f\"{len(seeds)}, {blocks}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 14:11:07,047 525436, 1679466\n"
     ]
    }
   ],
   "source": [
    "# # NOTE: 目标是构建Hadjs和Feats, 同时生成必要的labels(for-loss)\n",
    "\n",
    "# # Total 44896 User Nodes\n",
    "# subgraph_deg483 = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deg_le483_subgraph.p\"))\n",
    "\n",
    "# # Find Tweet Nodes for each User Node\n",
    "# #   P.S. Dont Use Sampling, since we are not constructing subnetworks\n",
    "# # TODO: use text/utmp_groupbystage.p instead\n",
    "\n",
    "# ut_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/usertweet_mp.p\"))\n",
    "\n",
    "# # NOTE: Choose Part of the Tweets, 1kw is toooooo large!\n",
    "# tweet_nodes = []\n",
    "# ut_edges    = []\n",
    "# for user in subgraph_deg483.vs[\"label\"]:\n",
    "#     selected_tweets = random.choices(ut_mp[user], k=min(len(ut_mp[user]), 20))\n",
    "#     tweet_nodes.extend(selected_tweets)\n",
    "#     for tweet in selected_tweets:\n",
    "#         ut_edges.append((user, tweet))\n",
    "# logger.info(f\"Tweet Nodes={len(tweet_nodes)}, Edges={len(ut_edges)}\")\n",
    "\n",
    "# nodes = {}\n",
    "# node_indices = 0\n",
    "# # Users: 44896, Tweets: 10008103, Total: 10052999\n",
    "# for node in subgraph_deg483.vs[\"label\"]+[tweet+208894 for tweet in tweet_nodes]:\n",
    "#     nodes[node] = node_indices\n",
    "#     node_indices += 1\n",
    "\n",
    "# edges = [[], []]\n",
    "# for uu_edge in subgraph_deg483.es:\n",
    "#     source, target = subgraph_deg483.vs[uu_edge.source][\"label\"], subgraph_deg483.vs[uu_edge.target][\"label\"]\n",
    "#     edges[0].append([nodes[source], nodes[target]])\n",
    "\n",
    "# for from_, to_ in ut_edges:\n",
    "#     edges[1].append([nodes[from_], nodes[to_+208894]])\n",
    "\n",
    "# # Add self-loops\n",
    "# for node in range(len(subgraph_deg483.vs[\"label\"])):\n",
    "#     edges[0].append([node, node])\n",
    "# for node in range(len(subgraph_deg483.vs[\"label\"]), node_indices):\n",
    "#     edges[1].append([node,node])\n",
    "\n",
    "# logger.info(f\"{len(edges[0])}, {len(edges[1])}\")\n",
    "# # 2022-10-27 11:13:49,333 480540, 10008103\n",
    "# # 2022-10-27 11:14:40,915 525436, 20016206\n",
    "# # NOTE: 2022-10-27 13:54:47,112 525436, 7320540\n",
    "\n",
    "# def create_sparsemat_from_edgelist(edgelist, m, n):\n",
    "#     rows, cols = edgelist[:,0], edgelist[:,1]\n",
    "#     ones = np.ones(len(rows), np.uint8)\n",
    "#     mat = sparse.coo_matrix((ones, (rows, cols)), shape=(m, n))\n",
    "#     return mat.tocsr()\n",
    "\n",
    "# uu_mat = create_sparsemat_from_edgelist(np.array(edges[0]), node_indices, node_indices)\n",
    "# ut_mat = create_sparsemat_from_edgelist(np.array(edges[1]), node_indices, node_indices)\n",
    "# hadjs = [uu_mat, ut_mat]\n",
    "# save_pickle(hadjs, os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deg_le483_hadjs_selfloop_max20tweet.p\"))\n",
    "\n",
    "# # TODO: vertices, stages, tags -> user_features[*]\n",
    "# # user_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/user_features/user_features.p\"))\n",
    "\n",
    "# user_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/user_features/user_features_avg.p\"))\n",
    "# deepwalk_feats = load_w2v_feature(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deepwalk/deepwalk_added.emb_64\"), 208894)\n",
    "# tweet_features = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/doc2topic_tweetfeat.p\"))\n",
    "# user_feats = np.concatenate((user_features[subgraph_deg483.vs[\"label\"]], deepwalk_feats[subgraph_deg483.vs[\"label\"]]), axis=1) \n",
    "# tweet_feats = tweet_features[tweet_nodes]\n",
    "# # logger.info(f\"{user_feats.shape}, {tweet_feats.shape}\")\n",
    "\n",
    "# feats = np.concatenate((\n",
    "#     np.append(user_feats, np.zeros(shape=(user_feats.shape[0], tweet_feats.shape[1])),  axis=1), \n",
    "#     np.append(np.zeros(shape=(tweet_feats.shape[0], user_feats.shape[1])), tweet_feats, axis=1), \n",
    "# ), axis=0)\n",
    "# logger.info(feats.shape)\n",
    "# save_pickle(feats, os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/deg_le483_feats_max20tweet.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_random_tweet_ids(samples: SubGraphSample, outdir: str, tweets_per_user:int=5):\n",
    "#     tweet_ids = []\n",
    "#     sample_ids = []\n",
    "#     ut_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/utmp_groupbystage.p\"))\n",
    "\n",
    "#     for idx in range(len(samples.labels)):\n",
    "#         if idx and idx % 10000 == 0:\n",
    "#             logger.info(f\"idx={idx}, sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "#         stage = samples.time_stages[idx]\n",
    "#         selected_tweet_ids  = set()\n",
    "#         candidate_tweet_ids = set()\n",
    "#         for vertex_id in samples.vertex_ids[idx]:\n",
    "#             available_tweet_ids = ut_mp[stage][vertex_id]\n",
    "#             random_ids = np.random.choice(available_tweet_ids, size=min(tweets_per_user, len(available_tweet_ids)), replace=False)\n",
    "#             selected_tweet_ids  |= set(random_ids)\n",
    "#             candidate_tweet_ids |= set(available_tweet_ids)-set(random_ids)\n",
    "#         candidate_tweet_ids -= selected_tweet_ids\n",
    "#         # logger.info(f\"Length: sample={len(selected_tweet_ids)}, remain={len(candidate_tweet_ids)}, expected={len(samples.vertex_ids[idx])*tweets_per_user}\")\n",
    "\n",
    "#         if len(selected_tweet_ids) != len(samples.vertex_ids[idx])*tweets_per_user:\n",
    "#             diff = len(samples.vertex_ids[idx])*tweets_per_user - len(selected_tweet_ids)\n",
    "#             if diff > len(candidate_tweet_ids):\n",
    "#                 continue\n",
    "#             selected_tweet_ids |= set(np.random.choice(list(candidate_tweet_ids), size=diff, replace=False))\n",
    "#         sample_ids.append(idx)\n",
    "#         tweet_ids.append(selected_tweet_ids)\n",
    "#     logger.info(f\"Finish Sampling Random Tweets... sample_ids={len(sample_ids)}, tweet_ids={len(tweet_ids)}\")\n",
    "\n",
    "#     os.makedirs(outdir, exist_ok=True)\n",
    "#     selected_samples = SubGraphSample(\n",
    "#         adj_matrices=samples.adj_matrices[sample_ids],\n",
    "#         influence_features=samples.influence_features[sample_ids],\n",
    "#         vertex_ids=samples.vertex_ids[sample_ids],\n",
    "#         labels=samples.labels[sample_ids],\n",
    "#         tags=samples.tags[sample_ids],\n",
    "#         time_stages=samples.time_stages[sample_ids],\n",
    "#     )\n",
    "#     save_pickle(sample_ids, os.path.join(outdir, \"sample_ids.p\"))\n",
    "#     save_pickle(tweet_ids, os.path.join(outdir, \"tweet_ids.p\"))\n",
    "#     save_pickle(selected_samples, os.path.join(outdir, \"selected_samples.p\"))\n",
    "#     logger.info(\"Finish Saving pkl...\")\n",
    "\n",
    "# def extend_subnetwork(file_dir: str):\n",
    "#     hs_filedir = os.path.join(DATA_ROOTPATH, file_dir).replace('stages_', 'hs_')\n",
    "#     samples = load_pickle(os.path.join(hs_filedir, \"selected_samples.p\"))\n",
    "#     tweet_ids = load_pickle(os.path.join(hs_filedir, \"tweet_ids.p\"))\n",
    "#     assert len(samples) == len(tweet_ids)\n",
    "\n",
    "#     tweetid2userid_mp = load_pickle(os.path.join(DATA_ROOTPATH, \"HeterGAT/basic/text/tweetid2userid_mp.p\"))\n",
    "#     vertex_ids = samples.vertex_ids\n",
    "#     adjs       = samples.adj_matrices\n",
    "#     adjs[adjs != 0] = 1.0\n",
    "#     adjs = adjs.astype(np.dtype('B'))\n",
    "\n",
    "#     extended_vertices, extended_adjs = [], []\n",
    "#     for idx in range(len(samples)):\n",
    "#         subnetwork = np.array(np.concatenate((vertex_ids[idx], np.array(list(tweet_ids[idx])))), dtype=int)\n",
    "#         extended_vertices.append(subnetwork)\n",
    "\n",
    "#         subnetwork_size, num_users = len(subnetwork), len(vertex_ids[idx])\n",
    "#         elem_idx_mp = {elem:idx for idx,elem in enumerate(subnetwork)}\n",
    "#         uu_adj = np.array([[0]*subnetwork_size for _ in range(subnetwork_size)], dtype='B')\n",
    "#         uu_adj[:num_users,:num_users] = adjs[idx]\n",
    "#         # NOTE: Get Corresponding User_id By Tweet_id, and then convert them into indexes in extend_subnetwork\n",
    "#         ut_adj = copy.deepcopy(uu_adj)\n",
    "#         for tweet_id in tweet_ids[idx]:\n",
    "#             user_id = tweetid2userid_mp[tweet_id]\n",
    "#             net_userid = elem_idx_mp[user_id]\n",
    "#             net_tweetid = elem_idx_mp[tweet_id]\n",
    "#             ut_adj[net_userid][net_tweetid] = 1\n",
    "#         extended_adjs.append([uu_adj, ut_adj])\n",
    "#     extended_vertices, extended_adjs = np.array(extended_vertices), np.array(extended_adjs)\n",
    "#     save_pickle(extended_vertices, os.path.join(hs_filedir, \"extended_vertices.p\"))\n",
    "#     save_pickle(extended_adjs, os.path.join(hs_filedir, \"extended_adjs.p\"))\n",
    "\n",
    "# data_dirpath = os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n",
    "# samples = SubGraphSample(\n",
    "#     adj_matrices=np.load(os.path.join(data_dirpath, \"adjacency_matrix.npy\")),\n",
    "#     influence_features=np.load(os.path.join(data_dirpath, \"influence_feature.npy\")),\n",
    "#     vertex_ids=np.load(os.path.join(data_dirpath, \"vertex_id.npy\")),\n",
    "#     labels=np.load(os.path.join(data_dirpath, \"label.npy\")),\n",
    "#     tags=np.load(os.path.join(data_dirpath, \"hashtag.npy\")),\n",
    "#     time_stages=np.load(os.path.join(data_dirpath, \"stage.npy\"))\n",
    "# )\n",
    "# # gen_random_tweet_ids(samples, os.path.join(DATA_ROOTPATH, \"HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\"))\n",
    "# # extend_subnetwork(\"HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/\")\n",
    "\n",
    "# def gen_user_emb(tot_user_num):\n",
    "#     # 208894*200*8*3\n",
    "#     user_feats = [[0.]*200*8*3 for _ in range(tot_user_num)]\n",
    "#     for tag in range(200):\n",
    "#         logger.info(f\"tag={tag}\")\n",
    "#         for stage in range(8):\n",
    "#             for feats_idx, feats in enumerate([\"norm_gravity_feature\", \"norm_exptime_feature1\", \"norm_ce_feature\"]):\n",
    "#                 feats = load_pickle(f\"/root/data/HeterGAT/user_features/{feats}/hashtag{tag}_t{stage}.p\")\n",
    "#                 for idx in range(tot_user_num):\n",
    "#                     user_feats[idx][tag*3*8+stage*3+feats_idx] = float(feats[idx])\n",
    "#     logger.info(f\"shape={user_feats.shape}\")\n",
    "#     return torch.FloatTensor(user_feats)\n",
    "\n",
    "# user_emb = gen_user_emb(208894)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('Heter-GAT-afYRWUpf': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d5a35b220c759fdf3543f8c8f7df3e7cf51856a17fdad347db68213ac2aaee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
