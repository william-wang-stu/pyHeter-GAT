{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 18:23:08,093 Note: NumExpr detected 56 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-10-21 18:23:08,095 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from lib.log import logger\n",
    "from src.utils import load_pickle, save_pickle, get_sparse_tensor\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import igraph\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# args = {\n",
    "#     \"min_active_neighbor\": 3,\n",
    "#     \"min_inf\": 100,\n",
    "#     \"max_inf\": 1000,\n",
    "#     \"min_deg\": 3,\n",
    "#     \"max_deg\": 31,\n",
    "#     \"ego_size\": 49,\n",
    "#     \"negative\": 1,\n",
    "#     \"restart_prob\": 0.2,\n",
    "#     \"walk_length\": 1000,\n",
    "#     \"output\": \"stages_op_inf_100_1k\",\n",
    "#     \"graph_file\": \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_friends.csv\",\n",
    "#     \"vote_file\":  \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_votes1.csv\",\n",
    "# }\n",
    "\n",
    "# os.makedirs(args[\"output\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11286/2430548899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/heter_samples.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Heter-GAT/src/utils.py\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "samples = load_pickle(\"/root/data/HeterGAT/stages/hs_subg483_inf_40_1718027_deg_18_483_ego_20_neg_1_restart_20/heter_samples.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "arr = [[1,2],[3,4]]\n",
    "mat = sparse.csr_matrix(arr)\n",
    "mat.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"min_active_neighbor\": 3,\n",
    "    \"min_inf\": 40,\n",
    "    \"max_inf\": 1718027,\n",
    "    \"min_deg\": 18,\n",
    "    \"max_deg\": 483,\n",
    "    \"ego_size\": 549,\n",
    "    \"negative\": 1,\n",
    "    \"restart_prob\": 0.2,\n",
    "    \"walk_length\": 10000,\n",
    "    \"output\": \"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_550_neg_1_restart_20\",\n",
    "    # \"graph_file\": \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_friends.csv\",\n",
    "    # \"vote_file\":  \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_votes1.csv\",\n",
    "}\n",
    "os.makedirs(args[\"output\"], exist_ok=True)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "graph = load_pickle(\"/root/data/HeterGAT/basic/deg_le483_subgraph.p\")\n",
    "diffusion = load_pickle(\"/root/data/HeterGAT/basic/deg_le483_df.p\")\n",
    "degree = graph.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.57948065, 0.81248999, 0.92561276, 0.95240949],\n",
       "        [0.14611328, 0.40065262, 0.10707053, 0.36104975],\n",
       "        [0.18617519, 0.67105101, 0.43146114, 0.67858838]],\n",
       "\n",
       "       [[0.75436873, 0.71014746, 0.40175647, 0.59489344],\n",
       "        [0.52436996, 0.24665542, 0.8337823 , 0.50637502],\n",
       "        [0.10458612, 0.44269392, 0.91112531, 0.39180159]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs = np.load(\"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/adjacency_matrix.npy\")\n",
    "feats = np.load(\"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/influence_feature.npy\")\n",
    "labels = np.load(\"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/label.npy\")\n",
    "vertex_ids = np.load(\"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/vertex_id.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(10)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61801, 50)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile([1,2], 4).reshape(4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(0, 2, (20, 20))\n",
    "a ^= a.T\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 10)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.repeat(a[:,:,np.newaxis], 10, axis=2)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61801,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61801, 50, 2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61801, 50, 50)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 21:00:21,828 Dump data ...\n",
      "2022-10-20 21:08:20,141 Collected 10000 instances\n",
      "2022-10-20 21:16:16,360 Collected 20000 instances\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6348/2532629226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6348/2532629226.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_deg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_deg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;31m# create positive case for user u, photo p, time t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcascade_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_affected_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muser_affected_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6348/2532629226.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, u, p, t, label, user_affected_now)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msubgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mego\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplementation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"create_from_scratch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0madjacency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0madjacency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def random_walk_with_restart(g, start, restart_prob):\n",
    "    current = random.choice(start)\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        stop = yield current\n",
    "        current = random.choice(start) if random.random() < restart_prob or g.degree(current)==0 \\\n",
    "                else random.choice(g.neighbors(current))\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.adj_matrices = []\n",
    "        self.features = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        self.hashtags = []\n",
    "        self.stages = []\n",
    "\n",
    "    def create(self, u, p, t, label, user_affected_now):\n",
    "        active_neighbor, inactive_neighbor = [], []\n",
    "\n",
    "        for v in graph.neighbors(u):\n",
    "            if v in user_affected_now:\n",
    "                active_neighbor.append(v)\n",
    "            else:\n",
    "                inactive_neighbor.append(v)\n",
    "        if len(active_neighbor) < args[\"min_active_neighbor\"]:\n",
    "            return\n",
    "\n",
    "        n = args[\"ego_size\"] + 1\n",
    "        n_active = 0\n",
    "        ego = []\n",
    "        if len(active_neighbor) < args[\"ego_size\"]:\n",
    "            # we should sample some inactive neighbors\n",
    "            n_active = len(active_neighbor)\n",
    "            ego = set(active_neighbor)\n",
    "            for v in itertools.islice(random_walk_with_restart(graph,\n",
    "                start=active_neighbor + [u,], restart_prob=args[\"restart_prob\"]), args[\"walk_length\"]):\n",
    "                if v!=u and v not in ego:\n",
    "                    ego.add(v)\n",
    "                    if len(ego) == args[\"ego_size\"]:\n",
    "                        break\n",
    "            ego = list(ego)\n",
    "            if len(ego) < args[\"ego_size\"]:\n",
    "                return\n",
    "        else:\n",
    "            n_active = args[\"ego_size\"]\n",
    "            samples = np.random.choice(active_neighbor,\n",
    "                    size=args[\"ego_size\"],\n",
    "                    replace=False)\n",
    "            ego += samples.tolist()\n",
    "        ego.append(u)\n",
    "\n",
    "        order = np.argsort(ego)\n",
    "        ranks = np.argsort(order)\n",
    "\n",
    "        subgraph = graph.subgraph(ego, implementation=\"create_from_scratch\")\n",
    "        adjacency = np.array(subgraph.get_adjacency().data)\n",
    "        adjacency = adjacency[ranks][:, ranks]\n",
    "        self.adj_matrices.append(adjacency)\n",
    "\n",
    "        feature = np.zeros((n,2))\n",
    "        for idx, v in enumerate(ego[:-1]):\n",
    "            if v in user_affected_now:\n",
    "                feature[idx, 0] = 1\n",
    "        feature[n-1, 1] = 1\n",
    "        self.features.append(feature)\n",
    "        self.vertices.append(np.array(ego, dtype=int))\n",
    "        self.labels.append(label)\n",
    "        self.hashtags.append(p)\n",
    "        self.stages.append(t)\n",
    "\n",
    "        # circle = subgraph.subgraph(ranks[:n_active], implementation=\"create_from_scratch\")\n",
    "\n",
    "        if len(self.labels) % 10000 == 0:\n",
    "            logger.info(\"Collected %d instances\", len(self.labels))\n",
    "\n",
    "    def dump_data(self):\n",
    "        self.adj_matrices = np.array(self.adj_matrices)\n",
    "        self.features = np.array(self.features)\n",
    "        self.vertices = np.array(self.vertices)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.hashtags = np.array(self.hashtags)\n",
    "        self.stages = np.array(self.stages)\n",
    "\n",
    "        output_dir = args[\"output\"]\n",
    "        with open(os.path.join(output_dir, \"adjacency_matrix.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.adj_matrices)\n",
    "        with open(os.path.join(output_dir, \"influence_feature.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.features)\n",
    "        with open(os.path.join(output_dir, \"vertex_id.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.vertices)\n",
    "        with open(os.path.join(output_dir, \"label.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.labels)\n",
    "        with open(os.path.join(output_dir, \"hashtag.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.hashtags)\n",
    "        with open(os.path.join(output_dir, \"stage.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.stages)\n",
    "\n",
    "        logger.info(\"Dump %d instances in total\" % (len(self.labels)))\n",
    "\n",
    "        self.adj_matrices = []\n",
    "        self.features = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        self.hashtags = []\n",
    "        self.stages = []\n",
    "\n",
    "    def dump(self):\n",
    "        logger.info(\"Dump data ...\")\n",
    "\n",
    "        nu = 0\n",
    "        for cascade_idx, cascade in diffusion.items():\n",
    "            nu += 1\n",
    "            if nu % 100 == 0:\n",
    "                logger.info(\"%d (%.2f percent) diffusion processed\" % (nu, 100.*nu/len(diffusion)))\n",
    "\n",
    "            if len(cascade)<args[\"min_inf\"] or len(cascade)>=args[\"max_inf\"]:\n",
    "                continue\n",
    "            user_affected_all = set([item[0] for item in cascade])\n",
    "            user_affected_now = set()\n",
    "            last = 0\n",
    "            #infected = set((cas[0][0],))\n",
    "            for item in cascade[1:]:\n",
    "                u, t = item\n",
    "                while last < len(cascade) and cascade[last][1] < t:\n",
    "                    user_affected_now.add(cascade[last][0])\n",
    "                    last += 1\n",
    "                if len(user_affected_now) == 0:\n",
    "                    continue\n",
    "                if u in user_affected_now:\n",
    "                    continue\n",
    "                stage = (t-cascade[0][1])*8 // (cascade[-1][1]-cascade[0][1])\n",
    "                if stage < 0:\n",
    "                    stage = 0\n",
    "                if stage > 7:\n",
    "                    stage = 7\n",
    "                if degree[u]>=args[\"min_deg\"] and degree[u]<args[\"max_deg\"]:\n",
    "                    # create positive case for user u, photo p, time t\n",
    "                    self.create(u, cascade_idx, stage, 1, user_affected_now)\n",
    "\n",
    "                negative = list(set(graph.neighbors(u)) - user_affected_all)\n",
    "                negative = [v for v in negative \\\n",
    "                        if degree[v]>=args[\"min_deg\"] \\\n",
    "                        and degree[v]<args[\"max_deg\"]]\n",
    "                if len(negative) == 0:\n",
    "                    continue\n",
    "                negative_sample = np.random.choice(negative,\n",
    "                        size=min(args[\"negative\"], len(negative)), replace=False)\n",
    "                for v in negative_sample:\n",
    "                    # create negative case for user v photo p, time t\n",
    "                    self.create(v, cascade_idx, stage, 0, user_affected_now)\n",
    "        if len(self.labels) > 0:\n",
    "            self.dump_data()\n",
    "\n",
    "data = Data()\n",
    "data.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# adjs = np.load(\"/root/data/HeterGAT/stages/stages_subg483_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20/adjacency_matrix.npy\")\n",
    "\n",
    "# sparse_adjs = []\n",
    "# for adj in adjs:\n",
    "#     sparse_adjs.append(sparse.csr_matrix(adj).astype(np.float64))\n",
    "# sparse_adjs = np.array(sparse_adjs)\n",
    "\n",
    "# save_pickle(sparse_adjs, \"sparse_adjs.pkl\")\n",
    "sparse_adjs = load_pickle(\"sparse_adjs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((1024, 50, 25))\n",
    "\n",
    "torch.bmm(torch.stack([get_sparse_tensor(adj.tocoo()) for adj in sparse_adjs[:1024]]), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class SpGATLayer(nn.Module):\n",
    "    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=False):\n",
    "        super(SpGATLayer, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "\n",
    "        self.w = nn.Parameter(torch.Tensor(n_head, f_in, f_out))\n",
    "        self.attn_src = nn.Parameter(torch.Tensor(n_head, f_out, 1))\n",
    "        self.attn_trg = nn.Parameter(torch.Tensor(n_head, f_out, 1))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout = nn.Dropout(p=attn_dropout)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(f_out))\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        nn.init.xavier_uniform_(self.attn_src)\n",
    "        nn.init.xavier_uniform_(self.attn_trg)\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        \"\"\"from https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb\"\"\"\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "        \n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "    \n",
    "    def special_spmm(self, other, trg_indices, size):\n",
    "        ret = torch.zeros(size=size, dtype=other.dtype, device=other.device)\n",
    "        trg_indices_broadcast = self.explicit_broadcast(trg_indices, other)\n",
    "        ret.scatter_add_(dim=0, index=trg_indices_broadcast, src=other)\n",
    "        return ret\n",
    "    \n",
    "    def calc_neigh_attn(self, e, trg_indices, nb_nodes):\n",
    "        e = e - e.max()\n",
    "        exp_e = e.exp()\n",
    "        neigh_attn = exp_e / (self.special_spmm(other=exp_e, trg_indices=trg_indices, size=(nb_nodes, self.n_head)).index_select(0, trg_indices)+1e-16)\n",
    "        return neigh_attn.unsqueeze(-1) # (E,k)->(E,k,1)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj: torch.Tensor):\n",
    "        # NOTE: h: (n, fin), adj: (n, n)\n",
    "        n = h.shape[0]\n",
    "        h_prime  = torch.matmul(h.unsqueeze(0), self.w) # (k,n,f_out)\n",
    "        h_prime  = self.dropout(h_prime)\n",
    "        attn_src = torch.matmul(h_prime, self.attn_src).sum(dim=-1).permute(1,0) # (n,k)\n",
    "        attn_trg = torch.matmul(h_prime, self.attn_trg).sum(dim=-1).permute(1,0) # (n,k)\n",
    "\n",
    "        src_indices, trg_indices = adj._indices()[0], adj._indices()[1]\n",
    "        attn_src_lifted, attn_trg_lifted, h_prime_lifted = attn_src[src_indices], attn_trg[trg_indices], h_prime.permute(1,0,2)[src_indices]\n",
    "        e = self.leaky_relu(attn_src_lifted+attn_trg_lifted) # (E,k)\n",
    "\n",
    "        neigh_attn = self.calc_neigh_attn(e=e, trg_indices=trg_indices, nb_nodes=n) # (E,k,1)\n",
    "        neigh_attn = self.dropout(neigh_attn)\n",
    "\n",
    "        h_weighted = h_prime_lifted*neigh_attn # (E,k,f_out)\n",
    "        out = self.special_spmm(h_weighted, trg_indices=trg_indices, size=(n, self.n_head, self.f_out)) # (n,k,f_out)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return out + self.bias\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "from src.model import BatchSparseMultiHeadGraphAttention\n",
    "\n",
    "layer = SpGATLayer(n_head=8, f_in=20, f_out=16, attn_dropout=0.2)\n",
    "layer2 = BatchSparseMultiHeadGraphAttention(nb_heads=8, nb_in_feats=20, nb_out_feats=16, nb_loop_nodes=[50,50], attn_dropout=0.2)\n",
    "feats = torch.rand((1024, 50, 20))\n",
    "adjs = torch.stack([get_sparse_tensor(adj.tocoo()) for adj in sparse_adjs[:1024]])\n",
    "\n",
    "output1 = layer(feats[0], adjs[0])\n",
    "output2 = layer2(feats, adjs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1544e-01, -5.4057e-02, -2.4879e-01,  ..., -6.7029e-02,\n",
       "          -9.3592e-02,  6.8765e-01],\n",
       "         [-2.4823e-01, -1.3386e-01, -3.4142e-01,  ..., -8.2601e-02,\n",
       "          -6.7905e-02,  8.0942e-01],\n",
       "         [-2.3460e-01, -9.1109e-02, -2.9905e-01,  ..., -2.5372e-02,\n",
       "          -4.4732e-02,  7.2911e-01],\n",
       "         ...,\n",
       "         [-1.6695e-01, -4.7715e-01, -4.5230e-01,  ..., -1.2416e-01,\n",
       "           1.0839e-01,  3.7785e-01],\n",
       "         [-2.2112e-01, -2.5405e-01, -3.9294e-01,  ..., -1.0668e-01,\n",
       "           4.5095e-03,  7.4820e-01],\n",
       "         [-1.2201e-01, -1.3804e-01, -2.4096e-01,  ..., -8.4257e-02,\n",
       "          -9.5241e-02,  7.8978e-01]],\n",
       "\n",
       "        [[ 1.0376e-01,  1.7119e-01, -2.9007e-01,  ...,  3.2059e-01,\n",
       "           2.1262e-01, -2.2682e-01],\n",
       "         [ 1.5674e-01,  2.1932e-01, -4.4006e-01,  ...,  4.3963e-01,\n",
       "           2.1299e-01, -4.1362e-01],\n",
       "         [ 1.7116e-01,  1.8441e-01, -2.9506e-01,  ...,  3.5907e-01,\n",
       "           1.1020e-01, -2.0907e-01],\n",
       "         ...,\n",
       "         [ 2.3499e-01,  2.1439e-01, -3.8722e-01,  ...,  3.4734e-01,\n",
       "           2.1909e-01, -5.3640e-01],\n",
       "         [ 1.5918e-01,  1.4551e-01, -3.6462e-01,  ...,  4.0294e-01,\n",
       "           2.0510e-01, -3.1617e-01],\n",
       "         [ 2.7774e-01,  3.7725e-02, -4.1823e-01,  ...,  4.5912e-01,\n",
       "           1.3895e-01, -3.7417e-01]],\n",
       "\n",
       "        [[-3.7783e-01, -3.1939e-01,  1.2838e-01,  ..., -8.8279e-01,\n",
       "          -4.8446e-01, -3.4798e-01],\n",
       "         [-3.3750e-01, -2.3510e-01,  1.3436e-01,  ..., -6.4139e-01,\n",
       "          -2.7586e-01, -2.9331e-01],\n",
       "         [-4.2730e-01, -2.7595e-01,  6.5252e-02,  ..., -7.5993e-01,\n",
       "          -4.1748e-01, -3.4002e-01],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -2.5300e-01,  7.7692e-02,  ..., -6.0022e-01,\n",
       "           0.0000e+00, -2.9435e-03],\n",
       "         [-3.9923e-01, -3.0931e-01,  1.3104e-01,  ..., -8.6249e-01,\n",
       "          -2.9276e-01, -3.5731e-01],\n",
       "         [-2.4758e-01, -2.5383e-01,  1.8572e-01,  ..., -7.7110e-01,\n",
       "          -4.4482e-01, -3.8103e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.9984e-02,  4.3524e-01,  1.8234e-01,  ..., -3.5083e-04,\n",
       "          -4.4763e-01,  1.9030e-01],\n",
       "         [ 3.9005e-02,  3.8225e-01,  1.7683e-01,  ...,  4.6968e-02,\n",
       "          -4.8380e-01,  2.2208e-01],\n",
       "         [-2.4340e-03,  3.2312e-01,  1.9412e-01,  ..., -5.3725e-02,\n",
       "          -4.9012e-01,  8.7000e-02],\n",
       "         ...,\n",
       "         [ 1.1864e-01,  4.6227e-01,  4.0438e-01,  ..., -7.2726e-02,\n",
       "          -7.3904e-01, -2.8473e-01],\n",
       "         [ 1.6284e-01,  3.5775e-01,  2.4906e-01,  ...,  2.0380e-01,\n",
       "          -3.9730e-01,  1.1101e-01],\n",
       "         [ 2.4135e-02,  3.2872e-03,  2.0447e-01,  ...,  1.2983e-01,\n",
       "          -3.3889e-01,  2.3647e-01]],\n",
       "\n",
       "        [[-8.1554e-02,  1.5753e-01,  1.5513e-01,  ...,  4.3826e-01,\n",
       "           9.1006e-03,  5.6325e-01],\n",
       "         [ 1.0110e-02,  1.9128e-01,  3.0145e-01,  ...,  4.2419e-01,\n",
       "           3.3489e-02,  6.0021e-01],\n",
       "         [ 5.8646e-02,  1.0550e-01,  1.1166e-01,  ...,  4.2833e-01,\n",
       "           4.2085e-02,  5.9961e-01],\n",
       "         ...,\n",
       "         [ 3.5270e-02,  9.4355e-02,  4.4021e-01,  ...,  2.5723e-01,\n",
       "          -6.6687e-02,  3.5733e-01],\n",
       "         [-7.9900e-02,  1.7676e-02,  6.2248e-02,  ...,  3.0616e-01,\n",
       "          -2.9831e-03,  5.2225e-01],\n",
       "         [-6.3260e-03,  1.4696e-01,  1.3777e-01,  ...,  4.1325e-01,\n",
       "          -7.5515e-02,  6.0093e-01]],\n",
       "\n",
       "        [[-1.4874e-01, -1.9569e-01,  2.8450e-02,  ..., -4.8109e-01,\n",
       "           6.8725e-02, -3.1399e-01],\n",
       "         [-1.8762e-01, -6.5327e-02,  1.5856e-01,  ..., -5.6579e-01,\n",
       "           1.3019e-01, -3.0856e-01],\n",
       "         [-2.2215e-01, -2.4326e-01,  1.0436e-01,  ..., -5.9659e-01,\n",
       "           1.8724e-01, -3.6584e-01],\n",
       "         ...,\n",
       "         [-1.1949e-01, -2.7757e-01, -1.7412e-01,  ..., -3.1307e-01,\n",
       "          -4.2207e-01, -4.9018e-01],\n",
       "         [-9.1484e-02, -1.1721e-01,  8.5050e-02,  ..., -4.7805e-01,\n",
       "           1.4487e-02, -2.1249e-01],\n",
       "         [-1.0187e-01, -1.8909e-01,  6.6887e-02,  ..., -4.3713e-01,\n",
       "           1.3925e-01, -2.1318e-01]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.permute(1,0,2)-output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import BatchSparseMultiHeadGraphAttention\n",
    "\n",
    "layer = SpGATLayer(n_head=8, f_in=20, f_out=16, attn_dropout=0.2)\n",
    "layer2 = BatchSparseMultiHeadGraphAttention(nb_heads=8, nb_in_feats=20, nb_out_feats=16, attn_dropout=0.2)\n",
    "feats = torch.rand((1024, 50, 20))\n",
    "adjs = torch.stack([get_sparse_tensor(adj.tocoo()) for adj in sparse_adjs])\n",
    "\n",
    "output1 = layer(feats[0], adjs[0])\n",
    "output2 = layer2(feats, adjs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0709, 4.7018, 4.9718,  ..., 5.2282, 5.5877, 5.1165],\n",
       "         [4.5599, 5.6851, 5.3550,  ..., 7.2261, 6.0412, 5.2741],\n",
       "         [5.6862, 6.0142, 5.3410,  ..., 5.4852, 5.6655, 5.0347],\n",
       "         ...,\n",
       "         [4.7047, 4.5633, 6.3992,  ..., 4.3762, 6.4680, 4.4079],\n",
       "         [4.5754, 4.2937, 5.5518,  ..., 4.2582, 4.2420, 5.2090],\n",
       "         [5.4328, 4.8921, 5.6502,  ..., 4.2890, 5.8278, 5.0578]],\n",
       "\n",
       "        [[4.0160, 4.0266, 4.2205,  ..., 5.2469, 4.5562, 4.7742],\n",
       "         [4.9410, 4.8675, 4.8066,  ..., 6.0297, 4.7501, 4.8685],\n",
       "         [5.1416, 4.9092, 4.1807,  ..., 5.1767, 5.0153, 4.3268],\n",
       "         ...,\n",
       "         [5.2034, 5.5113, 5.5583,  ..., 4.1603, 6.1777, 4.7432],\n",
       "         [4.0258, 3.7083, 3.8873,  ..., 3.7321, 3.7415, 3.6337],\n",
       "         [5.3826, 4.1181, 4.8410,  ..., 5.2146, 5.6606, 4.1128]],\n",
       "\n",
       "        [[4.5096, 4.3108, 5.2446,  ..., 5.7763, 5.1839, 5.5337],\n",
       "         [5.0791, 6.0706, 5.4059,  ..., 6.9042, 5.4360, 4.4933],\n",
       "         [5.9228, 5.4491, 5.6753,  ..., 5.4754, 4.9459, 5.2702],\n",
       "         ...,\n",
       "         [5.6304, 5.0965, 6.1267,  ..., 3.9857, 6.1313, 5.6289],\n",
       "         [4.1360, 4.2607, 5.0606,  ..., 3.7950, 4.1570, 4.4804],\n",
       "         [5.2905, 4.7511, 5.4766,  ..., 4.9725, 6.3748, 4.7879]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[4.2815, 4.8433, 5.1621,  ..., 5.4903, 5.6307, 5.0904],\n",
       "         [4.3383, 5.5895, 5.0583,  ..., 6.5080, 5.2415, 5.0257],\n",
       "         [5.2030, 5.0988, 4.5576,  ..., 5.3062, 5.3134, 4.8613],\n",
       "         ...,\n",
       "         [5.2270, 5.0545, 5.3956,  ..., 4.3000, 6.2068, 4.7829],\n",
       "         [3.5733, 3.9478, 4.6304,  ..., 3.7150, 4.0399, 3.9424],\n",
       "         [5.2967, 4.5106, 4.7599,  ..., 4.5938, 5.7261, 5.0132]],\n",
       "\n",
       "        [[3.4268, 4.9005, 4.0350,  ..., 4.3952, 4.9715, 4.7846],\n",
       "         [4.2754, 5.2904, 4.0222,  ..., 6.4273, 5.4200, 3.9891],\n",
       "         [4.8496, 5.5580, 5.1509,  ..., 5.4567, 4.5555, 4.6329],\n",
       "         ...,\n",
       "         [3.6456, 4.8290, 5.8336,  ..., 3.8476, 4.9034, 4.3264],\n",
       "         [4.0052, 3.4861, 3.8101,  ..., 4.2140, 4.2729, 3.7748],\n",
       "         [5.2330, 3.7143, 5.5590,  ..., 3.7855, 4.8679, 3.5291]],\n",
       "\n",
       "        [[4.4194, 5.3659, 5.3017,  ..., 6.4532, 5.7290, 6.3555],\n",
       "         [6.1138, 6.2545, 5.5172,  ..., 7.0772, 6.5105, 5.4877],\n",
       "         [6.8991, 6.8031, 6.0754,  ..., 6.7261, 6.2716, 6.0158],\n",
       "         ...,\n",
       "         [5.0284, 6.4792, 7.7978,  ..., 5.1104, 6.9301, 6.3783],\n",
       "         [4.6454, 4.7015, 4.4425,  ..., 4.9320, 5.2364, 4.7506],\n",
       "         [6.8315, 6.1175, 6.7596,  ..., 5.9198, 6.4291, 5.3117]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((50,20))\n",
    "b = torch.rand((8,20,16))\n",
    "torch.einsum('ab,cbd->acd', [a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5274/71767201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGATv2Conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/gatv2_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx_r\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mx_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch\n",
    "\n",
    "feats = torch.rand((1024, 50, 20))\n",
    "adjs  = torch.stack([get_sparse_tensor(adj.tocoo()) for adj in sparse_adjs[:1024]])\n",
    "\n",
    "layer = GATv2Conv(in_channels=128, out_channels=16, heads=8)\n",
    "layer(feats, adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61440, 50, 50])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61801"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sparse_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Gen Digg Subsamples\n",
    "\n",
    "args = {\n",
    "    \"min_active_neighbor\": 3,\n",
    "    \"min_inf\": 199,\n",
    "    \"max_inf\": 898012,\n",
    "    \"min_deg\": 18,\n",
    "    \"max_deg\": 9265,\n",
    "    \"ego_size\": 49,\n",
    "    \"negative\": 1,\n",
    "    \"restart_prob\": 0.2,\n",
    "    \"walk_length\": 1000,\n",
    "    # \"output\": \"stages_lab_try2_inf_199_898012_deg_18_9265_ego_50_neg_1_restart_20\",\n",
    "    \"graph_file\": \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_friends.csv\",\n",
    "    \"vote_file\":  \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_votes1.csv\",\n",
    "}\n",
    "\n",
    "v2id = {}\n",
    "\n",
    "def get_vid(u, readonly=False):\n",
    "    if u not in v2id:\n",
    "        if readonly:\n",
    "            return -1\n",
    "        newid = len(v2id)\n",
    "        v2id[u] = newid\n",
    "    return v2id[u]\n",
    "\n",
    "def extract_edge_list():\n",
    "    edgelist = []\n",
    "    logger.info(\"Extrack edges from %s\", args[\"graph_file\"])\n",
    "    with open(args[\"graph_file\"], \"r\") as f:\n",
    "        nu = 0\n",
    "        for line in f:\n",
    "            nu += 1\n",
    "            if (nu+1) % 100000 == 0:\n",
    "                logger.info(\"%d user profile proccessed\" % (nu+1))\n",
    "            content = line.strip().split(',')\n",
    "            u, v = int(content[-2][1:-1]), int(content[-1][1:-1])\n",
    "            u, v = get_vid(u, readonly=False), get_vid(v, readonly=False)\n",
    "            edgelist.append((u,v))\n",
    "\n",
    "    return edgelist\n",
    "\n",
    "def load_graph():\n",
    "    edgelist = extract_edge_list()\n",
    "    logger.info(f\"Tot {len(edgelist)} Edges\")\n",
    "    n_vertices = len(v2id)\n",
    "    graph = igraph.Graph(len(v2id), directed=False)\n",
    "    graph.add_edges(edgelist)\n",
    "    graph.to_undirected()\n",
    "    graph.simplify(multiple=True, loops=True)\n",
    "\n",
    "    # edgelist_path = os.path.join(args[\"output\"], \"graph.edgelist\")\n",
    "    # with open(edgelist_path, \"w\") as f:\n",
    "    #     graph.write(f, format=\"edgelist\")\n",
    "\n",
    "    # add some fake vertices\n",
    "    graph.add_vertices(args[\"ego_size\"])\n",
    "    degree = graph.degree()\n",
    "\n",
    "    return n_vertices, graph, degree\n",
    "\n",
    "n_vertices, graph, degree = load_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 15:19:25,548 Load digg vote ...\n"
     ]
    }
   ],
   "source": [
    "diffusion = {}\n",
    "\n",
    "def add_action(user, action, t):\n",
    "    if action not in diffusion:\n",
    "        diffusion[action] = []\n",
    "    diffusion[action].append((user, t))\n",
    "\n",
    "def load_vote():\n",
    "    logger.info(\"Load digg vote ...\")\n",
    "\n",
    "    with open(args[\"vote_file\"], \"r\") as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split(',')\n",
    "            t, u, vote = [int(x[1:-1]) for x in content]\n",
    "            u = get_vid(u, readonly=True)\n",
    "            if u > -1:\n",
    "                add_action(u, vote, t)\n",
    "\n",
    "load_vote()\n",
    "for k in diffusion:\n",
    "    diffusion[k] = sorted(diffusion[k], key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diffusion():\n",
    "    diffusion_size = [len(v) for v in diffusion.values()]\n",
    "    logger.info(\"mean diffusion length %.2f\", np.mean(diffusion_size))\n",
    "    logger.info(\"max diffusion length %.2f\", np.max(diffusion_size))\n",
    "    logger.info(\"min diffusion length %.2f\", np.min(diffusion_size))\n",
    "    for i in range(1, 10):\n",
    "        logger.info(\"%d-th percentile of diffusion length %.2f\", i*10,\n",
    "                np.percentile(diffusion_size, i*10))\n",
    "    logger.info(\"95-th percentile of diffusion length %.2f\",\n",
    "            np.percentile(diffusion_size, 95))\n",
    "\n",
    "def summarize_graph():\n",
    "    logger.info(\"mean degree %.2f\", np.mean(degree))\n",
    "    logger.info(\"max degree %.2f\", np.max(degree))\n",
    "    logger.info(\"min degree %.2f\", np.min(degree))\n",
    "    for i in range(1, 10):\n",
    "        logger.info(\"%d-th percentile of degree %.2f\", i*10,\n",
    "                np.percentile(degree, i*10))\n",
    "    logger.info(\"95-th percentile of degree %.2f\",\n",
    "                np.percentile(degree, 95))\n",
    "\n",
    "def compute_structural_features():\n",
    "    logger.info(\"Computing rarity (reciprocal of degree)\")\n",
    "    degree = np.array(graph.degree())\n",
    "    degree[degree==0] = 1\n",
    "    rarity = 1. / degree\n",
    "    logger.info(\"Computing clustering coefficient..\")\n",
    "    cc = graph.transitivity_local_undirected(mode=\"zero\")\n",
    "    logger.info(\"Computing pagerank...\")\n",
    "    pagerank = graph.pagerank(directed=False)\n",
    "    logger.info(\"Computing constraint...\")\n",
    "    \"\"\"\n",
    "    constraint = graph.constraint()\n",
    "    logger.info(\"Computing closeness...\")\n",
    "    closeness = graph.closeness(cutoff=3)\n",
    "    logger.info(\"Computing betweenness...\")\n",
    "    betweenness = graph.betweenness(cutoff=3, directed=False)\n",
    "    logger.info(\"Computing authority_score...\")\n",
    "    \"\"\"\n",
    "    authority_score = graph.authority_score()\n",
    "    logger.info(\"Computing hub_score...\")\n",
    "    hub_score = graph.hub_score()\n",
    "    logger.info(\"Computing evcent...\")\n",
    "    evcent = graph.evcent(directed=False)\n",
    "    logger.info(\"Computing coreness...\")\n",
    "    coreness = graph.coreness()\n",
    "    logger.info(\"Structural feature computation done!\")\n",
    "    structural_features = np.column_stack(\n",
    "            (rarity, cc, pagerank,\n",
    "                #constraint, closeness, betweenness,\n",
    "                authority_score, hub_score, evcent, coreness))\n",
    "\n",
    "    with open(os.path.join(args[\"output\"], \"vertex_feature.npy\"), \"wb\") as f:\n",
    "        np.save(f, structural_features)\n",
    "\n",
    "# logger.info(len(diffusion))\n",
    "# summarize_diffusion()\n",
    "# summarize_graph()\n",
    "# compute_structural_features()\n",
    "\n",
    "def summarize_distribution(data):\n",
    "    logger.info(f\"length={len(data)}\")\n",
    "    logger.info(\"mean diffusion length %.2f\", np.mean(data))\n",
    "    logger.info(\"max diffusion length %.2f\", np.max(data))\n",
    "    logger.info(\"min diffusion length %.2f\", np.min(data))\n",
    "    for i in range(1, 10):\n",
    "        logger.info(\"%d-th percentile of diffusion length %.2f\", i*10,\n",
    "                np.percentile(data, i*10))\n",
    "    logger.info(\"95-th percentile of diffusion length %.2f\",\n",
    "            np.percentile(data, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 14:36:00,807 maxdeg=483, length=21336\n",
      "2022-10-15 14:36:00,811 maxdeg=1144, length=41057\n",
      "2022-10-15 14:36:00,813 maxdeg=1476, length=47293\n",
      "2022-10-15 14:36:00,815 maxdeg=1907, length=53623\n",
      "2022-10-15 14:36:00,817 maxdeg=5676, length=70964\n",
      "2022-10-15 14:36:00,820 maxdeg=9265, length=74369\n"
     ]
    }
   ],
   "source": [
    "for maxdeg in [483, 1144, 1476, 1907, 5676, 9265]:\n",
    "    labels = np.load(f\"../../DeepInf-preprocess/preprocess/stages_lab_inf_40_172140_deg_18_{maxdeg}_ego_50_neg_1_restart_20/label.npy\")\n",
    "    logger.info(f\"maxdeg={maxdeg}, length={len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 15:59:22,611 0=40.0\n",
      "2022-10-15 15:59:22,614 1=57.88\n",
      "2022-10-15 15:59:22,615 2=71.94\n",
      "2022-10-15 15:59:22,617 3=112.7\n",
      "2022-10-15 15:59:22,618 4=125.48\n",
      "2022-10-15 15:59:22,619 5=141.35000000000002\n",
      "2022-10-15 15:59:22,621 6=159.7\n",
      "2022-10-15 15:59:22,622 7=170.0\n",
      "2022-10-15 15:59:22,623 8=189.68\n",
      "2022-10-15 15:59:22,625 9=198.82\n",
      "2022-10-15 15:59:22,626 10=199.0\n",
      "2022-10-15 15:59:22,627 11=200.0\n",
      "2022-10-15 15:59:22,628 12=210.28\n",
      "2022-10-15 15:59:22,630 13=227.66000000000003\n",
      "2022-10-15 15:59:22,631 14=248.76000000000005\n",
      "2022-10-15 15:59:22,632 15=297.8499999999999\n",
      "2022-10-15 15:59:22,633 16=321.96\n",
      "2022-10-15 15:59:22,634 17=357.47\n",
      "2022-10-15 15:59:22,636 18=371.66\n",
      "2022-10-15 15:59:22,637 19=409.24\n",
      "2022-10-15 15:59:22,638 20=435.40000000000003\n",
      "2022-10-15 15:59:22,640 21=463.58\n",
      "2022-10-15 15:59:22,641 22=532.8800000000001\n",
      "2022-10-15 15:59:22,642 23=585.4000000000001\n",
      "2022-10-15 15:59:22,643 24=611.24\n",
      "2022-10-15 15:59:22,645 25=635.25\n",
      "2022-10-15 15:59:22,646 26=688.0600000000001\n",
      "2022-10-15 15:59:22,647 27=732.9000000000001\n",
      "2022-10-15 15:59:22,648 28=753.76\n",
      "2022-10-15 15:59:22,649 29=764.0\n",
      "2022-10-15 15:59:22,650 30=781.0\n",
      "2022-10-15 15:59:22,651 31=834.1499999999999\n",
      "2022-10-15 15:59:22,652 32=1047.0\n",
      "2022-10-15 15:59:22,653 33=1140.8400000000001\n",
      "2022-10-15 15:59:22,654 34=1269.6400000000006\n",
      "2022-10-15 15:59:22,655 35=1542.9499999999985\n",
      "2022-10-15 15:59:22,656 36=1623.12\n",
      "2022-10-15 15:59:22,658 37=1691.9399999999998\n",
      "2022-10-15 15:59:22,659 38=1757.6000000000001\n",
      "2022-10-15 15:59:22,660 39=1839.42\n",
      "2022-10-15 15:59:22,661 40=2049.400000000003\n",
      "2022-10-15 15:59:22,662 41=2265.95\n",
      "2022-10-15 15:59:22,663 42=2521.52\n",
      "2022-10-15 15:59:22,664 43=2586.14\n",
      "2022-10-15 15:59:22,665 44=2658.4\n",
      "2022-10-15 15:59:22,666 45=2805.2\n",
      "2022-10-15 15:59:22,667 46=2982.78\n",
      "2022-10-15 15:59:22,669 47=3489.77\n",
      "2022-10-15 15:59:22,670 48=3666.4399999999996\n",
      "2022-10-15 15:59:22,671 49=3876.2600000000007\n",
      "2022-10-15 15:59:22,672 50=4097.0\n",
      "2022-10-15 15:59:22,673 51=4559.36\n",
      "2022-10-15 15:59:22,674 52=5006.960000000001\n",
      "2022-10-15 15:59:22,675 53=5206.82\n",
      "2022-10-15 15:59:22,676 54=5559.88\n",
      "2022-10-15 15:59:22,677 55=5735.550000000001\n",
      "2022-10-15 15:59:22,678 56=5986.360000000001\n",
      "2022-10-15 15:59:22,679 57=6343.289999999998\n",
      "2022-10-15 15:59:22,680 58=6610.019999999997\n",
      "2022-10-15 15:59:22,681 59=7753.549999999999\n",
      "2022-10-15 15:59:22,682 60=7948.999999999999\n",
      "2022-10-15 15:59:22,683 61=8596.31\n",
      "2022-10-15 15:59:22,684 62=9085.539999999999\n",
      "2022-10-15 15:59:22,685 63=12330.090000000015\n",
      "2022-10-15 15:59:22,686 64=15159.96\n",
      "2022-10-15 15:59:22,688 65=22214.6\n",
      "2022-10-15 15:59:22,689 66=23917.84\n",
      "2022-10-15 15:59:22,690 67=28218.560000000012\n",
      "2022-10-15 15:59:22,691 68=29689.520000000008\n",
      "2022-10-15 15:59:22,692 69=31840.61\n",
      "2022-10-15 15:59:22,693 70=35035.19999999993\n",
      "2022-10-15 15:59:22,694 71=49364.99999999983\n",
      "2022-10-15 15:59:22,696 72=67236.72\n",
      "2022-10-15 15:59:22,697 73=78332.96000000002\n",
      "2022-10-15 15:59:22,698 74=86690.83999999998\n",
      "2022-10-15 15:59:22,699 75=90623.25\n",
      "2022-10-15 15:59:22,701 76=100951.16000000016\n",
      "2022-10-15 15:59:22,703 77=116060.64999999997\n",
      "2022-10-15 15:59:22,704 78=120934.16\n",
      "2022-10-15 15:59:22,706 79=141595.91000000012\n",
      "2022-10-15 15:59:22,707 80=172140.4000000004\n",
      "2022-10-15 15:59:22,708 81=201678.77999999997\n",
      "2022-10-15 15:59:22,710 82=228838.49999999968\n",
      "2022-10-15 15:59:22,711 83=304112.6299999998\n",
      "2022-10-15 15:59:22,712 84=330231.24\n",
      "2022-10-15 15:59:22,714 85=378638.4500000002\n",
      "2022-10-15 15:59:22,715 86=484886.23999999894\n",
      "2022-10-15 15:59:22,716 87=567993.7599999999\n",
      "2022-10-15 15:59:22,718 88=620829.3600000001\n",
      "2022-10-15 15:59:22,719 89=678943.5600000002\n",
      "2022-10-15 15:59:22,720 90=898012.8999999997\n",
      "2022-10-15 15:59:22,721 91=983807.2400000005\n",
      "2022-10-15 15:59:22,723 92=1121668.04\n",
      "2022-10-15 15:59:22,724 93=1226778.630000001\n",
      "2022-10-15 15:59:22,725 94=1327501.1200000008\n",
      "2022-10-15 15:59:22,727 95=1718027.499999999\n",
      "2022-10-15 15:59:22,728 96=2038812.6399999997\n",
      "2022-10-15 15:59:22,729 97=2142099.87\n",
      "2022-10-15 15:59:22,730 98=3249575.280000003\n",
      "2022-10-15 15:59:22,731 99=3793901.31999998\n"
     ]
    }
   ],
   "source": [
    "length = [len(value) for value in diffusion.values()]\n",
    "for i in range(100):\n",
    "    logger.info(f\"{i}={np.percentile(length, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Gen Subgraph by filtering node degrees (<=483, which is by 20% of total graph degree distribution)\n",
    "\n",
    "DEG_THR = 483\n",
    "\n",
    "graph = load_pickle(\"/root/data/HeterGAT/basic/graph/graph-undirected.p\")\n",
    "graph.vs[\"label\"] = graph.vs.indices\n",
    "deg_le_vs = graph.vs.select(_degree_le=DEG_THR)\n",
    "deg_le_subgraph = graph.subgraph(deg_le_vs)\n",
    "save_pickle(deg_le_subgraph, f\"/root/data/HeterGAT/basic/deg_le{DEG_THR}_subgraph.p\")\n",
    "\n",
    "old2new_mp = {}\n",
    "for idx, graphid in enumerate(deg_le_vs.indices):\n",
    "    old2new_mp[graphid] = idx\n",
    "    # logger.info(\"graphid={}, new_graphid={}\".format(graphid,deg_le_483_subgraph.vs[\"label\"][graphid]))\n",
    "\n",
    "diffusion = load_pickle(\"/root/data/HeterGAT/basic/ActionLog.p\")\n",
    "\n",
    "df = {}\n",
    "for hashtag, values in diffusion.items():\n",
    "    logger.info(f\"hashtag={hashtag}\")\n",
    "    vals = []\n",
    "    for user, timestamp in values:\n",
    "        if graph.degree(user)>DEG_THR:\n",
    "            continue\n",
    "        vals.append((old2new_mp[user],timestamp))\n",
    "    df[hashtag] = vals\n",
    "\n",
    "save_pickle(df,f\"/root/data/HeterGAT/basic/deg_le{DEG_THR}_df.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# graph = load_pickle(\"/root/data/HeterGAT/basic/graph/graph-undirected.p\")\n",
    "# logger.info(graph.ecount())\n",
    "\n",
    "# DEG_THR = 483\n",
    "\n",
    "# for user in range(graph.vcount()):\n",
    "#     if graph.degree(user) <= DEG_THR:\n",
    "#         continue\n",
    "#     enum = graph.ecount()\n",
    "#     neighbors = graph.neighbors(user)\n",
    "#     assert len(neighbors) == graph.degree(user)\n",
    "#     eliminated_neighs = random.choices(neighbors, k=len(neighbors)-DEG_THR)\n",
    "#     graph.delete_edges([(user, neigh) for neigh in eliminated_neighs])\n",
    "#     logger.info(f\"old={enum}, new={graph.ecount()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 13:21:50,462 Dump data ...\n",
      "2022-10-19 13:22:01,277 Collected 10000 instances\n",
      "2022-10-19 13:22:11,902 Collected 20000 instances\n",
      "2022-10-19 13:22:21,253 Collected 30000 instances\n",
      "2022-10-19 13:22:32,227 Collected 40000 instances\n",
      "2022-10-19 13:22:42,029 Collected 50000 instances\n",
      "2022-10-19 13:22:54,173 Collected 60000 instances\n",
      "2022-10-19 13:22:56,535 100 (50.00 percent) diffusion processed\n",
      "2022-10-19 13:22:56,639 200 (100.00 percent) diffusion processed\n",
      "2022-10-19 13:22:58,723 Dump 61801 instances in total\n"
     ]
    }
   ],
   "source": [
    "# summarize_diffusion()\n",
    "# summarize_graph()\n",
    "# compute_structural_features()\n",
    "\n",
    "DEG_THR = 483\n",
    "\n",
    "args = {\n",
    "    \"min_active_neighbor\": 3,\n",
    "    \"min_inf\": 40,\n",
    "    \"max_inf\": 1718027,\n",
    "    \"min_deg\": 18,\n",
    "    \"max_deg\": 483,\n",
    "    \"ego_size\": 49,\n",
    "    \"negative\": 1,\n",
    "    \"restart_prob\": 0.2,\n",
    "    \"walk_length\": 1000,\n",
    "    \"output\": f\"/root/data/HeterGAT/stages/stages_subg{DEG_THR}_inf_40_1718027_deg_18_483_ego_50_neg_1_restart_20\",\n",
    "    # \"graph_file\": \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_friends.csv\",\n",
    "    # \"vote_file\":  \"/root/TR-pptusn/DeepInf-master/dataset/data/raw-digg/digg_votes1.csv\",\n",
    "}\n",
    "os.makedirs(args[\"output\"], exist_ok=True)\n",
    "\n",
    "# graph = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/graph/graph-undirected.p\")\n",
    "graph = load_pickle(f\"/root/data/HeterGAT/basic/deg_le{DEG_THR}_subgraph.p\")\n",
    "# diffusion = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/ActionLog.p\")\n",
    "diffusion = load_pickle(f\"/root/data/HeterGAT/basic/deg_le{DEG_THR}_df.p\")\n",
    "degree = graph.degree()\n",
    "\n",
    "def random_walk_with_restart(g, start, restart_prob):\n",
    "    current = random.choice(start)\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        stop = yield current\n",
    "        current = random.choice(start) if random.random() < restart_prob or g.degree(current)==0 \\\n",
    "                else random.choice(g.neighbors(current))\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.adj_matrices = []\n",
    "        self.features = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        self.stages = []\n",
    "        self.hashtags = []\n",
    "\n",
    "    def create(self, u, p, t, label, user_affected_now):\n",
    "        active_neighbor, inactive_neighbor = [], []\n",
    "\n",
    "        for v in graph.neighbors(u):\n",
    "            if v in user_affected_now:\n",
    "                active_neighbor.append(v)\n",
    "            else:\n",
    "                inactive_neighbor.append(v)\n",
    "        if len(active_neighbor) < args[\"min_active_neighbor\"]:\n",
    "            return\n",
    "\n",
    "        n = args[\"ego_size\"] + 1\n",
    "        n_active = 0\n",
    "        ego = []\n",
    "        if len(active_neighbor) < args[\"ego_size\"]:\n",
    "            # we should sample some inactive neighbors\n",
    "            n_active = len(active_neighbor)\n",
    "            ego = set(active_neighbor)\n",
    "            for v in itertools.islice(random_walk_with_restart(graph,\n",
    "                start=active_neighbor + [u,], restart_prob=args[\"restart_prob\"]), args[\"walk_length\"]):\n",
    "                if v!=u and v not in ego:\n",
    "                    ego.add(v)\n",
    "                    if len(ego) == args[\"ego_size\"]:\n",
    "                        break\n",
    "            ego = list(ego)\n",
    "            if len(ego) < args[\"ego_size\"]:\n",
    "                return\n",
    "        else:\n",
    "            n_active = args[\"ego_size\"]\n",
    "            samples = np.random.choice(active_neighbor,\n",
    "                    size=args[\"ego_size\"],\n",
    "                    replace=False)\n",
    "            ego += samples.tolist()\n",
    "        ego.append(u)\n",
    "\n",
    "        order = np.argsort(ego)\n",
    "        ranks = np.argsort(order)\n",
    "\n",
    "        subgraph = graph.subgraph(ego, implementation=\"create_from_scratch\")\n",
    "        adjacency = np.array(subgraph.get_adjacency().data)\n",
    "        adjacency = adjacency[ranks][:, ranks]\n",
    "        self.adj_matrices.append(adjacency)\n",
    "\n",
    "        feature = np.zeros((n,2))\n",
    "        for idx, v in enumerate(ego[:-1]):\n",
    "            if v in user_affected_now:\n",
    "                feature[idx, 0] = 1\n",
    "        feature[n-1, 1] = 1\n",
    "        self.features.append(feature)\n",
    "        self.vertices.append(np.array(ego, dtype=int))\n",
    "        self.labels.append(label)\n",
    "        self.stages.append(t)\n",
    "        self.hashtags.append(p)\n",
    "\n",
    "        circle = subgraph.subgraph(ranks[:n_active], implementation=\"create_from_scratch\")\n",
    "\n",
    "        if len(self.labels) % 10000 == 0:\n",
    "            logger.info(\"Collected %d instances\", len(self.labels))\n",
    "\n",
    "    def dump_data(self):\n",
    "        self.adj_matrices = np.array(self.adj_matrices)\n",
    "        self.features = np.array(self.features)\n",
    "        self.vertices = np.array(self.vertices)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "        output_dir = args[\"output\"]\n",
    "        with open(os.path.join(output_dir, \"adjacency_matrix.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.adj_matrices)\n",
    "        with open(os.path.join(output_dir, \"influence_feature.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.features)\n",
    "        with open(os.path.join(output_dir, \"vertex_id.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.vertices)\n",
    "        with open(os.path.join(output_dir, \"label.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.labels)\n",
    "        with open(os.path.join(output_dir, \"stage.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.stages)\n",
    "        with open(os.path.join(output_dir, \"hashtag.npy\"), \"wb\") as f:\n",
    "            np.save(f, self.hashtags)\n",
    "\n",
    "        logger.info(\"Dump %d instances in total\" % (len(self.labels)))\n",
    "\n",
    "        self.adj_matrices = []\n",
    "        self.features = []\n",
    "        self.vertices = []\n",
    "        self.labels = []\n",
    "        self.stages = []\n",
    "        self.hashtags = []\n",
    "\n",
    "    def dump(self):\n",
    "        logger.info(\"Dump data ...\")\n",
    "\n",
    "        nu = 0\n",
    "        for cascade_idx, cascade in diffusion.items():\n",
    "            nu += 1\n",
    "            if nu % 100 == 0:\n",
    "                logger.info(\"%d (%.2f percent) diffusion processed\" % (nu, 100.*nu/len(diffusion)))\n",
    "\n",
    "            if len(cascade)<args[\"min_inf\"] or len(cascade)>=args[\"max_inf\"]:\n",
    "                continue\n",
    "            user_affected_all = set([item[0] for item in cascade])\n",
    "            user_affected_now = set()\n",
    "            last = 0\n",
    "            #infected = set((cas[0][0],))\n",
    "            for item in cascade[1:]:\n",
    "                u, t = item\n",
    "                while last < len(cascade) and cascade[last][1] < t:\n",
    "                    user_affected_now.add(cascade[last][0])\n",
    "                    last += 1\n",
    "                if len(user_affected_now) == 0:\n",
    "                    continue\n",
    "                if u in user_affected_now:\n",
    "                    continue\n",
    "                \n",
    "                # NOTE: Use discrete stage instead of continuous timestamps\n",
    "                stage = (t-cascade[0][1])*8 // (cascade[-1][1]-cascade[0][1])\n",
    "                stage = max(min(stage, 7), 0) # clamp to between [0,7]\n",
    "\n",
    "                if degree[u]>=args[\"min_deg\"] and degree[u]<args[\"max_deg\"]:\n",
    "                    # create positive case for user u, photo p, time t\n",
    "                    self.create(u, cascade_idx, stage, 1, user_affected_now)\n",
    "\n",
    "                negative = list(set(graph.neighbors(u)) - user_affected_all)\n",
    "                negative = [v for v in negative \\\n",
    "                        if degree[v]>=args[\"min_deg\"] \\\n",
    "                        and degree[v]<args[\"max_deg\"]]\n",
    "                if len(negative) == 0:\n",
    "                    continue\n",
    "                negative_sample = np.random.choice(negative,\n",
    "                        size=min(args[\"negative\"], len(negative)), replace=False)\n",
    "                for v in negative_sample:\n",
    "                    # create negative case for user v photo p, time t\n",
    "                    self.create(v, cascade_idx, stage, 0, user_affected_now)\n",
    "\n",
    "        if len(self.labels) > 0:\n",
    "            self.dump_data()\n",
    "\n",
    "data = Data()\n",
    "data.dump()\n",
    "\n",
    "# summarize_distribution(data.samples_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils.util import *\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# 1. Read Network and ActionLog\n",
    "# graph = read_network()\n",
    "# diffusion_dict = read_actionlog()\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "graph = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/graph/graph-undirected.p\")\n",
    "diffusion_dict = load_pickle(\"/root/Lab_Related/data/Heter-GAT/Classic/ActionLog.p\")\n",
    "\n",
    "# 2. Gen Subnetwork Sample\n",
    "class PreprocessData:\n",
    "    def __init__(self):\n",
    "        self.adj_matrices = []\n",
    "        self.influence_features = []\n",
    "        self.vertex_ids = []\n",
    "        self.labels = []\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def random_walk_with_restart(g, start, restart_prob):\n",
    "    # start is list\n",
    "    current = random.choice(start)\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        stop = yield current\n",
    "        current = random.choice(start) if random.random() < restart_prob or g.degree(current)==0 \\\n",
    "                else random.choice(g.neighbors(current))\n",
    "\n",
    "def create_sample(user, label, user_affected_now, gensample):\n",
    "    active_neighbor, inactive_neighbor = [], []\n",
    "    for v in graph.neighbors(user):\n",
    "        if v in user_affected_now:\n",
    "            active_neighbor.append(v)\n",
    "        else:\n",
    "            inactive_neighbor.append(v)\n",
    "    if len(active_neighbor) < min_active_neighbor:\n",
    "        # logger.info(f\"ERR[len(active_neighbor)<3] len(active_neighbor): {len(active_neighbor)}\")\n",
    "        return\n",
    "    subnetwork_size = ego_size + 1\n",
    "    subnetwork = []\n",
    "    if len(active_neighbor) < ego_size:\n",
    "        # include some inactive_neighbor\n",
    "        subnetwork = set(active_neighbor)\n",
    "        for v in itertools.islice(\n",
    "            random_walk_with_restart(\n",
    "                graph,\n",
    "                start=active_neighbor + [user,],\n",
    "                restart_prob=restart_prob,\n",
    "            ),\n",
    "            walk_length\n",
    "        ):\n",
    "            if v != user and v not in subnetwork:\n",
    "                subnetwork.add(v)\n",
    "                if len(subnetwork) == ego_size:\n",
    "                    break\n",
    "        subnetwork = list(subnetwork)\n",
    "        if len(subnetwork) < ego_size:\n",
    "            # logger.info(f\"ERR[len(subnetwork)<49] len(sub-network): {len(subnetwork)}\")\n",
    "            return\n",
    "    else:\n",
    "        samples = np.random.choice(\n",
    "            active_neighbor,\n",
    "            size=ego_size,\n",
    "            replace=False\n",
    "        )\n",
    "        subnetwork = samples.tolist()\n",
    "    subnetwork.append(user)\n",
    "\n",
    "    ranks = np.array(subnetwork).argsort().argsort()\n",
    "    subgraph = graph.subgraph(subnetwork, implementation=\"create_from_scratch\")\n",
    "    adjacency = np.array(subgraph.get_adjacency().data, dtype=int)\n",
    "    # convert ordered adj-matrix to an original one\n",
    "    # i.e. subnetwork = [12,5,13], and we have the corresponding adj-matrix A\n",
    "    # but A is an ordered one, which A[0][2] means edges between 5 and 13, not 12 and 13\n",
    "    # so we need to convert it with original ranks\n",
    "    # thus we use [][:,] to convert it in both row and column directions\n",
    "    adjacency = adjacency[ranks][:,ranks]\n",
    "    gensample.adj_matrices.append(adjacency)\n",
    "\n",
    "    influence_feature = np.zeros((subnetwork_size,2))\n",
    "    for idx, v in enumerate(subnetwork[:-1]):\n",
    "        if v in user_affected_now:\n",
    "            influence_feature[idx, 0] = 1\n",
    "    influence_feature[subnetwork_size-1,1] = 1\n",
    "    gensample.influence_features.append(influence_feature)\n",
    "\n",
    "    gensample.vertex_ids.append(np.array(subnetwork, dtype=int))\n",
    "\n",
    "    gensample.labels.append(label)\n",
    "\n",
    "def dump_preprocess_data(desc, adj_matrices, influence_features, vertex_ids, labels):\n",
    "    os.makedirs(desc, exist_ok=True)\n",
    "    adj_matrices = np.array(adj_matrices)\n",
    "    influence_features = np.array(influence_features)\n",
    "    vertex_ids = np.array(vertex_ids)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    with open(f\"{desc}/adjacency_matrix.npy\", \"wb\") as f:\n",
    "        np.save(f, adj_matrices)\n",
    "    with open(f\"{desc}/influence_feature.npy\", \"wb\") as f:\n",
    "        np.save(f, influence_features)\n",
    "    with open(f\"{desc}/vertex_id.npy\", \"wb\") as f:\n",
    "        np.save(f, vertex_ids)\n",
    "    with open(f\"{desc}/label.npy\", \"wb\") as f:\n",
    "        np.save(f, labels)\n",
    "\n",
    "    logger.info(\"Dump %d instances in total\" % (len(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 21:43:33,991 204\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "degree = graph.degree()\n",
    "Nslice = Ntimeslice + 1\n",
    "gensample_list = [PreprocessData() for _ in range(Nslice)] # gensample_list[0] is None\n",
    "\n",
    "for idx, (hashtag, cascades) in enumerate(diffusion_dict.items()):\n",
    "    if idx < 0:\n",
    "        continue\n",
    "\n",
    "    cascade_idx = 0\n",
    "    prev_pos, prev_neg = set(), set()\n",
    "    user_affected_now = set()\n",
    "    min_timestamp, max_timestamp = cascades[0][1], cascades[-1][1]\n",
    "    time_span = (max_timestamp-min_timestamp)/Nslice\n",
    "\n",
    "    for tidx in range(Nslice):\n",
    "        lower_b, upper_b = min_timestamp+time_span*tidx, min_timestamp+time_span*(tidx+1)\n",
    "        cur_pos, cur_neg = set(), set()\n",
    "\n",
    "        while cascade_idx < len(cascades) and (cascades[cascade_idx][1] >= lower_b and cascades[cascade_idx][1] <= upper_b):\n",
    "            if cascades[cascade_idx][0] not in user_affected_now:\n",
    "                cur_pos.add(cascades[cascade_idx][0])\n",
    "                cur_neg |= set(graph.neighbors(cascades[cascade_idx][0]))\n",
    "            cascade_idx += 1\n",
    "        logger.info(f\"{len(cur_pos)}\")\n",
    "        break\n",
    "        # if tidx > 0:\n",
    "        #     for active_user in prev_pos:\n",
    "        #         if degree[active_user] >= min_degree and degree[active_user] <= max_degree:\n",
    "        #             create_sample(user=active_user, label=1, user_affected_now=user_affected_now, gensample=gensample_list[tidx])\n",
    "            \n",
    "        #     neg_user = prev_neg - cur_pos\n",
    "        #     sample_neg_user = np.random.choice(list(neg_user), size=min(negative_sample_num * len(prev_pos), len(neg_user)), replace=False)\n",
    "        #     for inactive_user in sample_neg_user:\n",
    "        #         if degree[inactive_user] >= min_degree and degree[inactive_user] <= max_degree:\n",
    "        #             create_sample(user=inactive_user, label=0, user_affected_now=user_affected_now, gensample=gensample_list[tidx])\n",
    "        \n",
    "        # user_affected_now |= cur_pos\n",
    "        # prev_pos, prev_neg = cur_pos, cur_neg\n",
    "        # logger.info(f\"len(user_affected_now): {len(user_affected_now)}, len(prev_pos): {len(prev_pos)}, len(prev_neg): {len(prev_neg)}\")\n",
    "\n",
    "    # logger.info(f\"idx: {idx:>6}, hashtag: {hashtag:>6}, cascades length: {len(cascades):>6}, sample_l length:\" + \\\n",
    "    #     \" \".join([f\"{len(sample):>6}\" for sample in gensample_list[1:]])\n",
    "    # )\n",
    "    break\n",
    "\n",
    "# logger.info(f\"Dump Data to Files [nu={nu}]\")\n",
    "# for idx in range(1, Nslice):\n",
    "#     dump_preprocess_data(\n",
    "#         desc=f\"stages/{idx}\",\n",
    "#         adj_matrices=gensample_list[idx].adj_matrices,\n",
    "#         influence_features=gensample_list[idx].influence_features,\n",
    "#         vertex_ids=gensample_list[idx].vertex_ids,\n",
    "#         labels=gensample_list[idx].labels\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 21:38:03,053 120774\n",
      "2022-09-29 21:38:03,061 217\n"
     ]
    }
   ],
   "source": [
    "Ntimestages = 8\n",
    "sample_ratio = 1\n",
    "subgraph_samples = [PreprocessData() for _ in range(Nslice)] # gensample_list[0] is None\n",
    "\n",
    "def find_rightest_bound(timestamp: float, min_timestamp: float, time_span: float):\n",
    "    idx = int((timestamp-min_timestamp)//time_span)\n",
    "    return Ntimestages-1 if idx>=Ntimestages-1 else idx\n",
    "\n",
    "cascade_item_idx = 0\n",
    "users_affected_now = set()\n",
    "users_affected_all = set([item[0] for item in cascades])\n",
    "time_span = (cascades[-1][1]-cascades[0][1])/Ntimestages\n",
    "\n",
    "logger.info(len(cascades))\n",
    "tidx_l = []\n",
    "for user, timestamp in cascades[1:]:\n",
    "    if timestamp > cascades[0][1]+time_span*1:\n",
    "        break\n",
    "    while cascade_item_idx < len(cascades) and cascades[cascade_item_idx][1] < timestamp:\n",
    "        users_affected_now.add(cascades[cascade_item_idx][0])\n",
    "        cascade_item_idx += 1\n",
    "    if len(users_affected_now) == 0 or user in users_affected_now:\n",
    "        continue\n",
    "    tidx = find_rightest_bound(timestamp, min_timestamp=cascades[0][1], time_span=time_span)\n",
    "    tidx_l.append(tidx)\n",
    "    # Pos\n",
    "    # if degree[user] >= min_degree and degree[user] <= max_degree:\n",
    "    #     create_sample(user=user, label=1, user_affected_now=users_affected_now, gensample=subgraph_samples[tidx])\n",
    "    \n",
    "    # # Neg\n",
    "    # neg_samples = list(set(graph.neighbors(user)) - users_affected_all)\n",
    "    # neg_samples = np.random.choice(neg_samples, size=min(len(neg_samples), sample_ratio), replace=False)\n",
    "    # for neg_sample in neg_samples:\n",
    "    #     if degree[neg_sample] >= min_degree and degree[neg_sample] <= max_degree:\n",
    "    #         create_sample(user=neg_sample, label=0, user_affected_now=users_affected_now, gensample=subgraph_samples[tidx])\n",
    "    \n",
    "    # logger.info(f\"idx: {idx:>6}, hashtag: {hashtag:>6}, cascades length: {len(cascades):>6}, sample_l length:\" + \\\n",
    "    #     \" \".join([f\"{len(sample):>6}\" for sample in subgraph_samples])\n",
    "    # )\n",
    "logger.info(len(tidx_l))\n",
    "\n",
    "for idx in range(1, Nslice):\n",
    "    dump_preprocess_data(\n",
    "        desc=f\"stages/{idx}\",\n",
    "        adj_matrices=gensample_list[idx].adj_matrices,\n",
    "        influence_features=gensample_list[idx].influence_features,\n",
    "        vertex_ids=gensample_list[idx].vertex_ids,\n",
    "        labels=gensample_list[idx].labels\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
