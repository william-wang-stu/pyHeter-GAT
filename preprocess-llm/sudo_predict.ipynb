{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "# read from down sampling results\n",
    "# apply sudo label assignment\n",
    "\n",
    "import regex\n",
    "import json\n",
    "\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/input_sample.jsonl\"\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1/generation_1.jsonl\"\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "\n",
    "# regex_pattern = r\"\\[\\d+\\] ([\\w\\s]+)\"\n",
    "regex_pattern = r\"\\[\\d+\\] ([\\u4e00-\\u9fff]+)\"\n",
    "\n",
    "def read_from_topicgpt(filepath):\n",
    "    user_tweets = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            text = info[\"text\"]\n",
    "            mt_info = info[\"meta_info\"]\n",
    "            topic = info[\"responses\"]\n",
    "            topic = regex.compile(regex_pattern).findall(topic)\n",
    "            if len(topic) > 0: topic = topic[0]\n",
    "            else: topic = \"None\"\n",
    "            if mt_info[\"user\"] not in user_tweets: user_tweets[mt_info[\"user\"]] = []\n",
    "            user_tweets[mt_info[\"user\"]].append({\n",
    "                \"user\": mt_info[\"user\"],\n",
    "                \"ts\": mt_info[\"ts\"],\n",
    "                \"text\": text,\n",
    "                \"tag\": mt_info[\"tag\"],\n",
    "                \"bertopic_label\": mt_info[\"bertopic_label\"],\n",
    "                \"topicgpt_label\": topic,\n",
    "            })\n",
    "    return user_tweets\n",
    "\n",
    "user_infos = read_from_topicgpt(down_sampling_filepath)\n",
    "print(len(user_infos))\n",
    "print(sum([len(tweets) for user, tweets in user_infos.items()]))\n",
    "\n",
    "# for user, infos in user_infos.items():\n",
    "#     print(user)\n",
    "#     for info in infos:\n",
    "#         print(info)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256\n",
      "106988\n"
     ]
    }
   ],
   "source": [
    "# # Get other unlabeled data\n",
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "# user_tweets_dict = {}\n",
    "# for tag, cascades in cascade_dict.items():\n",
    "#     for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "#         if user not in user_tweets_dict:\n",
    "#             user_tweets_dict[user] = []\n",
    "#         user_tweets_dict[user].append({\n",
    "#             \"tag\": tag,\n",
    "#             \"bertopic_label\": str(bertopic_label),\n",
    "#             \"user\": user,\n",
    "#             \"ts\": ts,\n",
    "#             \"text\": tweet,\n",
    "#         })\n",
    "# print(sum([len(tweets) for user, tweets in user_tweets_dict.items()]))\n",
    "\n",
    "# user_remain_infos = {}\n",
    "# user_seeds = user_infos.keys()\n",
    "# for user, full_infos in user_tweets_dict.items():\n",
    "#     if user not in user_seeds:\n",
    "#         # user_remain_infos[user] = full_infos\n",
    "#         continue\n",
    "\n",
    "#     infos = user_infos[user]\n",
    "#     remain_infos = []\n",
    "#     for info in full_infos:\n",
    "#         flag = False\n",
    "#         for s_info in infos:\n",
    "#             if info[\"text\"] == s_info[\"text\"] \\\n",
    "#                 and info[\"tag\"] == s_info[\"tag\"] \\\n",
    "#                 and info[\"user\"] == s_info[\"user\"] \\\n",
    "#                 and info[\"ts\"] == s_info[\"ts\"]:\n",
    "#                 flag = True\n",
    "#                 break\n",
    "#         if not flag:\n",
    "#             remain_infos.append(info)\n",
    "#     # print(len(infos), len(remain_infos), len(full_infos), \"{:.2f}\".format(len(remain_infos) / len(full_infos)))\n",
    "#     # try:\n",
    "#     #     assert len(infos) + len(remain_infos) == len(full_infos)\n",
    "#     # except Exception as e:\n",
    "#     #     print(user, len(infos), len(remain_infos), len(full_infos))\n",
    "#     user_remain_infos[user] = remain_infos\n",
    "\n",
    "# print(sum([len(infos) for infos in user_remain_infos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_to_en = {\n",
    "    \"ç§‘æŠ€\": \"Technology\",\n",
    "    \"ä½“è‚²\": \"Sports\",\n",
    "    \"ç¤¾ä¼šé—®é¢˜\": \"Social Issues\",\n",
    "    \"åŽ†å²\": \"History\",\n",
    "    \"å¨±ä¹\": \"Entertainment\",\n",
    "    \"å†œä¸š\": \"Agriculture\",\n",
    "    \"æ”¿æ²»\": \"Politics\",\n",
    "    \"å†›äº‹\": \"Military\",\n",
    "    \"æ•™è‚²\": \"Education\",\n",
    "    \"ç»æµŽ\": \"Economy\",\n",
    "    \"çŽ¯å¢ƒ\": \"Environment\",\n",
    "    \"è´¸æ˜“\": \"Trade\",\n",
    "    \"æ–‡åŒ–\": \"Culture\",\n",
    "    \"åœ°ç¼˜æ”¿æ²»\": \"Geopolitics\",\n",
    "    \"æ–‡å­¦\": \"Literature\",\n",
    "    \"é£Ÿå“\": \"Food\",\n",
    "    \"å¤©æ°”\": \"Weather\",\n",
    "    \"åª’ä½“\": \"Media\",\n",
    "    \"ç¤¾äº¤åª’ä½“\": \"Social Media\",\n",
    "    \"å¤–äº¤\": \"Diplomacy\",\n",
    "    \"çŠ¯ç½ª\": \"Crime\",\n",
    "    \"å®‰å…¨\": \"Security\",\n",
    "    \"å¥åº·\": \"Health\",\n",
    "    \"äº¤é€š\": \"Transportation\",\n",
    "    \"æ”¿åºœ\": \"Government\",\n",
    "    \"å›½é™…å…³ç³»\": \"International Relations\",\n",
    "    \"æ³•å¾‹\": \"Law\",\n",
    "    \"å®—æ•™\": \"Religion\",\n",
    "    \"æ—…æ¸¸\": \"Tourism\",\n",
    "    \"é£Ÿå“å®‰å…¨\": \"Food Safety\",\n",
    "    \"å•†ä¸š\": \"Business\",\n",
    "    \"ç¤¾äº¤\": \"Social Interaction\"\n",
    "}\n",
    "\n",
    "en_to_ch = {v: k for k, v in ch_to_en.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/topicGPT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "# 2. Use trained bertopic model to assign sudo labels\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# TODO: use broader interests\n",
    "# gt_ft_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/gt_ft_mapping.data\"\n",
    "gt_ft_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/gt_ft_mapping.data\"\n",
    "gt_ft_mapping = load_pickle(gt_ft_filepath)\n",
    "\n",
    "# Get labeled data\n",
    "data = []\n",
    "target = []\n",
    "for user, infos in user_infos.items():\n",
    "    for info in infos:\n",
    "        if info[\"topicgpt_label\"] == \"None\": continue\n",
    "        if info[\"topicgpt_label\"] not in ch_to_en: continue\n",
    "        if ch_to_en[info[\"topicgpt_label\"]] not in gt_ft_mapping: continue\n",
    "        data.append(info[\"text\"])\n",
    "        # target.append(info[\"topicgpt_label\"])\n",
    "        target.append(gt_ft_mapping[ch_to_en[info[\"topicgpt_label\"]]])\n",
    "\n",
    "# Discretize targets\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# target = label_encoder.fit_transform(target)\n",
    "\n",
    "print(len(data))\n",
    "print(sum([len(tweets) for user, tweets in user_infos.items()]))\n",
    "\n",
    "# split into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Bertopic Model\n",
    "\n",
    "# # Skip over dimensionality reduction, replace cluster model with classifier,\n",
    "# # and reduce frequent words while we are at it.\n",
    "# empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "# clf = LogisticRegression()\n",
    "# ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Create a fully supervised BERTopic instance\n",
    "# topic_model= BERTopic(\n",
    "#     umap_model=empty_dimensionality_model,\n",
    "#     hdbscan_model=clf,\n",
    "#     ctfidf_model=ctfidf_model,\n",
    "#     embedding_model=sbert,\n",
    "# )\n",
    "# topic_model = topic_model.fit(X_train, y=y_train)\n",
    "\n",
    "# # Map input `y` to topics\n",
    "# mappings = topic_model.topic_mapper_.get_mappings()\n",
    "# # reverse_mapping = {v: k for k, v in mappings.items()}\n",
    "# # print(mappings)\n",
    "\n",
    "# y_preds = []\n",
    "# topics, _ = topic_model.transform(X_test[:100])\n",
    "# # interest_labels = label_encoder.inverse_transform([reverse_mapping[topic] for topic in topics])\n",
    "# interest_labels = [mappings[topic] for topic in topics]\n",
    "# print(interest_labels)\n",
    "# # y_preds.append(mappings[topic[0]])\n",
    "\n",
    "# # test accuracy\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(y_test[:100], interest_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n"
     ]
    }
   ],
   "source": [
    "# 2. Direct SBert Cosine Similarity\n",
    "\n",
    "# Get embeddings\n",
    "sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "embeddings = sbert.encode(X_train)\n",
    "\n",
    "topic_embeddings = {}\n",
    "for emb, label in zip(embeddings, y_train):\n",
    "    if label not in topic_embeddings:\n",
    "        topic_embeddings[label] = []\n",
    "    topic_embeddings[label].append(emb)\n",
    "\n",
    "for interest, emb in topic_embeddings.items():\n",
    "    topic_embeddings[interest] = np.mean(emb, axis=0)\n",
    "    # print(topic_embeddings[interest].shape)\n",
    "key2idx = {i: k for i, k in enumerate(topic_embeddings.keys())}\n",
    "topic_t_embeddings = np.array([emb for emb in topic_embeddings.values()])\n",
    "\n",
    "# Get test embeddings\n",
    "test_embeddings = sbert.encode(X_test[:100])\n",
    "# print(topic_t_embeddings.shape)\n",
    "# print(test_embeddings.shape)\n",
    "\n",
    "# Get cosine similarity and Give Labels\n",
    "cosine_scores = util.pytorch_cos_sim(test_embeddings, topic_t_embeddings)\n",
    "# print(cosine_scores.shape)\n",
    "predictions = np.argmax(cosine_scores, axis=1).tolist()\n",
    "predictions = [key2idx[p] for p in predictions]\n",
    "# print(predictions)\n",
    "# print(y_test[:100])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test[:100], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63932\n"
     ]
    }
   ],
   "source": [
    "# remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/input_sample_remain.jsonl\"\n",
    "# user_remains_info = {}\n",
    "# with open(remain_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         info = json.loads(line)\n",
    "#         user = info[\"meta_info\"][\"user\"]\n",
    "#         if user not in user_remains_info:\n",
    "#             user_remains_info[user] = []\n",
    "#         user_remains_info[user].append(info)\n",
    "\n",
    "# print(sum([len(infos) for infos in user_remains_info.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256\n",
      "63932\n",
      "174757\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get other unlabeled data\n",
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "user_tweets_dict = {}\n",
    "for tag, cascades in cascade_dict.items():\n",
    "    for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "        if user not in user_tweets_dict:\n",
    "            user_tweets_dict[user] = []\n",
    "        user_tweets_dict[user].append({\n",
    "            \"tag\": tag,\n",
    "            \"bertopic_label\": str(bertopic_label),\n",
    "            \"user\": user,\n",
    "            \"ts\": ts,\n",
    "            \"text\": tweet,\n",
    "        })\n",
    "print(sum([len(tweets) for user, tweets in user_tweets_dict.items()]))\n",
    "\n",
    "candidate_users = list(\n",
    "    set(list(user_tweets_dict.keys())) - \\\n",
    "        set(list(user_infos.keys())))\n",
    "expanded_users = random.sample(candidate_users, k=2000)\n",
    "\n",
    "\n",
    "# Get Remaining Info\n",
    "remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/input_sample_remain.jsonl\"\n",
    "user_remains_info = {}\n",
    "with open(remain_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        info = json.loads(line)\n",
    "        user = info[\"meta_info\"][\"user\"]\n",
    "        if user not in user_remains_info:\n",
    "            user_remains_info[user] = []\n",
    "        user_remains_info[user].append(info)\n",
    "\n",
    "print(sum([len(infos) for infos in user_remains_info.values()]))\n",
    "\n",
    "# 1. Sample only 2000 / full users to expand\n",
    "for user in expanded_users:\n",
    "    infos = user_tweets_dict[user]\n",
    "    user_remains_info[user] = [\n",
    "        {\n",
    "            \"meta_info\": {\n",
    "                \"tag\": info[\"tag\"],\n",
    "                \"user\": info[\"user\"],\n",
    "                \"ts\": info[\"ts\"],\n",
    "                \"bertopic_label\": info[\"bertopic_label\"],\n",
    "            },\n",
    "            \"text\": info[\"text\"],\n",
    "            # \"topicgpt_label\": \"None\",\n",
    "        }\n",
    "        for info in infos\n",
    "    ]\n",
    "\n",
    "print(sum([len(infos) for infos in user_remains_info.values()]))\n",
    "\n",
    "# 2. Sample At most 20 tweets for each user\n",
    "# for user in candidate_users:\n",
    "#     user_remains_info[user] = random.sample(list(user_tweets_dict[user]), k=min(len(user_tweets_dict[user]), 20))\n",
    "\n",
    "# t_len = []\n",
    "# for user, infos in user_remains_info.items():\n",
    "#     t_len.append(len(infos))\n",
    "\n",
    "# def analyse_distribution(data):\n",
    "#     for i in range(10):\n",
    "#         t = np.percentile(data, i*10)\n",
    "#         print(t)\n",
    "# analyse_distribution(t_len)\n",
    "\n",
    "# for user in expanded_users:\n",
    "#     user_remains_info[user] = user_tweets_dict[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127169\n"
     ]
    }
   ],
   "source": [
    "# # Get other unlabeled data\n",
    "\n",
    "# user_remain_infos = {}\n",
    "# user_seeds = user_infos.keys()\n",
    "# for user, full_infos in user_tweets_dict.items():\n",
    "#     if user not in user_seeds:\n",
    "#         if user in expanded_users:\n",
    "#             user_remain_infos[user] = full_infos\n",
    "#         continue\n",
    "\n",
    "#     infos = user_infos[user]\n",
    "#     remain_infos = []\n",
    "#     for info in full_infos:\n",
    "#         flag = False\n",
    "#         for s_info in infos:\n",
    "#             if info[\"text\"] == s_info[\"text\"] \\\n",
    "#                 and info[\"tag\"] == s_info[\"tag\"] \\\n",
    "#                 and info[\"user\"] == s_info[\"user\"] \\\n",
    "#                 and info[\"ts\"] == s_info[\"ts\"]:\n",
    "#                 flag = True\n",
    "#                 break\n",
    "#         if not flag:\n",
    "#             remain_infos.append(info)\n",
    "#     # print(len(infos), len(remain_infos), len(full_infos), \"{:.2f}\".format(len(remain_infos) / len(full_infos)))\n",
    "#     # try:\n",
    "#     #     assert len(infos) + len(remain_infos) == len(full_infos)\n",
    "#     # except Exception as e:\n",
    "#     #     print(user, len(infos), len(remain_infos), len(full_infos))\n",
    "#     user_remain_infos[user] = remain_infos\n",
    "\n",
    "# print(sum([len(infos) for infos in user_remain_infos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_remain_infos_cp = {}\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1.jsonl\"\n",
    "filename = \"ex_2000user\"\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/generation_1/generation_1_{}.jsonl\".format(filename)\n",
    "\n",
    "with open(save_filepath, 'w') as f:\n",
    "    # for user, infos in user_remain_infos.items():\n",
    "    for user, infos in user_remains_info.items():\n",
    "        testing_texts = [info[\"text\"] for info in infos]\n",
    "        test_embeddings = sbert.encode(X_test[:100])\n",
    "        cosine_scores = util.pytorch_cos_sim(test_embeddings, topic_t_embeddings)\n",
    "        # print(cosine_scores.shape)\n",
    "        predictions = np.argmax(cosine_scores, axis=1).tolist()\n",
    "        predictions = [key2idx[p] for p in predictions]\n",
    "\n",
    "        # user_remain_infos_cp[user] = []\n",
    "        for info, pred in zip(infos, predictions):\n",
    "            info[\"topicgpt_label\"] = pred\n",
    "            f.write(json.dumps(info) + \"\\n\")\n",
    "            # user_remain_infos_cp[user].append(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @HananyaNaftali: The Church of the Holy Sepulchre in #Jerusalem is closed due to Coronavirus. #Israel\n",
      "\n",
      "The last time it was closed was iâ€¦\n",
      "---Health---Religion---\n",
      "RT @krislc: #æ”¿æ²»æ”¬ç‚’ political mutual destruction has begun. Question is: Beijing knows this will likely happen when deciding to break from trâ€¦\n",
      "---Military---Politics---\n",
      "Pakistan condemned the inhuman Syrian Regime attack on Turkish troops. Pakistanis are with the Turks at this critical time; #sehitlerimizvar\n",
      "---Academic_disciplines---Military---\n",
      "RT @www_HKer: For ðŸ‡¹ðŸ‡­Thai friends: \n",
      "\n",
      "#Rule1 to fight #CCPViurs is : you do not trust CCP\n",
      "#Rule2 : you DO NOT trust CCP\n",
      "#Rule3 : remember theâ€¦\n",
      "---Politics---Military---\n",
      "RT @Yfreeeeeeehk: Lovely #HongKongersâ¤ï¸\n",
      "Forwadly to clean up the Mosque after police defaced it by a water cannon.\n",
      "#PoliceBrutalitiy #HongKâ€¦\n",
      "---Society---Food_and_drink---\n",
      "#Gambian newspapers #DailyNews and @PointGambia highlight today #4Mar, in its printed editions, the solidarity vocaâ€¦ https://t.co/u1R13oWLB4\n",
      "---Academic_disciplines---Information---\n",
      "RT @UNDP: By 2030, ðŸŒ¡ï¸ across the Arab Region could rise by 2C. How do you transition to #greenenergy &amp; build #ClimateAction in one of the hâ€¦\n",
      "---Nature---Technology---\n",
      "RT @C_Barraud: ðŸ‡ºðŸ‡¸ ðŸ‡¨ðŸ‡³ The US and #China are on the brink of a new #ColdWar that could devastate the global economy - Business Insider\n",
      "https:â€¦\n",
      "---Economy---Politics---\n",
      "RT @marcorubio: Nervous days for drug lords as @Southcom conducts massive counter-drug operation off the coasts of #Venezuela https://t.co/â€¦\n",
      "---Science---Military---\n",
      "RT @TexasDSHS: #StayHomeTexas and flatten the curve. \n",
      "\n",
      "Texas doctors, first responders, and all health care and emergency workers need youâ€¦\n",
      "---Academic_disciplines---Health---\n",
      "RT @WellingMichael: Regime #StormTroopers of socialist  Regime in Venezuela ðŸ‡»ðŸ‡ª... will #BernieSanders apply the same use of force to Americâ€¦\n",
      "---Military---Politics---\n",
      "RT @fahadnabeelfn: #CoronaVirusUpdates - #Kuwait amends adhan/azaan, calls to â€˜pray at homeâ€™\n",
      "\n",
      "\"It is said in Muslim book Sahih Al- Bukhariâ€¦\n",
      "---Health---Religion---\n",
      "RT @MaryJoe38642126: .@democracynow \n",
      "\n",
      "PLS REPORT: Itâ€™s #Crucial That #China #BanYulin The #DogCatMeatTrade &amp; #WetMarkets \n",
      "\n",
      "#Sick &amp; #Healthyâ€¦\n",
      "---Economy---Politics---\n",
      "RT @DrTaraO: Kim Dae-Jung Center sued Lee Ju-seong, who escaped from #NorthKorea, for libel for writing a book Purple Lake--re NKorean invoâ€¦\n",
      "---Mass_media---Society---\n",
      "RT @CynthiaDRitchie: In #ZardarisFilthyPPP an FIR has been lodged against me in Jacobabad, accusing me of character assassination of BB.\n",
      "\n",
      "Lâ€¦\n",
      "---Information---Politics---\n",
      "RT @JackLawsome: Kind words can be short\n",
      "and easy to speak,\n",
      "but their echoes are truly\n",
      "endless. ~ Mother Teresa\n",
      "\n",
      "#JackLawsome #Kindness #Imâ€¦\n",
      "---Philosophy---Human_behavior---\n",
      "#October1st https://t.co/87wGJQPp9V\n",
      "---Entertainment---Internet---\n",
      "RT @lattevani: #FreedomHK #GlobeConnect\n",
      "Crowdfunding miracle\n",
      "Hong Kong protest https://t.co/sBvMV90OhG\n",
      "---Politics---Society---\n",
      "RT @CarolynBMaloney: Yesterday, Congress passed the #HongKongHumanRightsDemocracyAct - a strong signal to President Trump &amp; the Chinese Govâ€¦\n",
      "---Law---Politics---\n",
      "RT @seedling_tv: What would you do if you knew you could grow more food on your #farm while #conserving #resources? Well, itâ€™s possible andâ€¦\n",
      "---Food_and_drink---Society---\n",
      "RT @Alam_Chaudry: #LiKashing is one of the mega billionaire had the courage to stand by #HongKongProstesters and speak out against #Beijingâ€¦\n",
      "---Law---Society---\n",
      "RT @AapkaCharul: Some Hate mongers responsible for #DelhiRiots are Swara, Rana Ayyub &amp; Saba Naqvi. They spreaded Hate, Rumours, False videoâ€¦\n",
      "---Mass_media---Internet---\n",
      "RT @ClaudiaMCMo: Raw evidence of Hongkong #police behaving like #thugs.\n",
      "See footage on right from yesterday aftn.\n",
      "#HKProtests\n",
      "Credit, NowTVâ€¦\n",
      "---Society---Government---\n",
      "RT @Haidar_Ahmad_: A fire erupted yesterday at the #Palmyra Orchid oasis south of the city.\n",
      "\n",
      "The fire spread across an area of around 150 hâ€¦\n",
      "---Nature---Health---\n",
      "RT @Stand_with_HK: ðŸ”´#LIVE: The #HongKongEOM Press Conference is happening now! 19 delegates from 10 countries are sharing their observationâ€¦\n",
      "---Government---Military---\n",
      "RT @andrewspha: So kahle kahle the government want children to go to school kodwa dololo with all necessary #Covid19SA regulations includinâ€¦\n",
      "---Education---Health---\n",
      "RT @ipsvipul_: A very happy #CivilServicesDay to all seniors and colleagues in various services. May God give us strength to keep performinâ€¦\n",
      "---Academic_disciplines---Politics---\n",
      "RT @JimmyLaiApple: Many â€œcelebrities\" and their family members have foreign passports. #JackieChan, #LisaWang, #EricTsang #Alan Tam etc areâ€¦\n",
      "---Business---Society---\n",
      "RT @KongTsungGan: Today is Day 1 of #FiveDemandsWeek/#HKGoldenWeek &amp; all over #HK people have been queuing up at #YellowEconomicCircle shopâ€¦\n",
      "---Government---Society---\n",
      "RT @HKers721: @IngrahamAngle @SolomonYue upon #US-CCPWar, #ObamaGate uncovered, #LiFeiFei the #CCP spy, controlling twitter, #CCP organisedâ€¦\n",
      "---Politics---Military---\n",
      "RT @MHiesboeck: As I said a week ago.  HK is gone. #ballots for election of House Committee chairman were distributed to pro Beijing lawmakâ€¦\n",
      "---Government---Politics---\n",
      "#HongKongPoliceState\n",
      "#HongKongPoliceTerrorists\n",
      "#HongKongPoliceBurtality \n",
      "#SOSHK #PolyU\n",
      "#HongKongProtesters https://t.co/5qRnbO68XQ\n",
      "---Government---Society---\n",
      "RT @sumlokkei: #dc2019 just became the biggest election in #hk history - with 2.33 million turned out to vote, beating the 2016 #legco elecâ€¦\n",
      "---Government---Politics---\n",
      "RT @yannnnnn111: A #HongKonger joined the Cake International 2019 Competition in #UK w/ a HK theme cake\n",
      "\n",
      "However, she is being disqualifiedâ€¦\n",
      "---Law---Time---\n",
      "RT @OfficalPshk: #HKers paid tribute outside the #PrinceEdward train station as today marks the 9-month anniversary since the #831Incident.â€¦\n",
      "---Time---Law---\n",
      "RT @Dr_fizakhan: #Kashmir tujhe salaam.ðŸ˜¥ðŸ˜¢ #LockdownExtended #kashmir #IndiaLockdown https://t.co/RzoP6HhQ0A\n",
      "---Religion---Government---\n",
      "RT @DiazCanelB: Doctors and not bombs. Fidel's legacy. \n",
      "#SomosCuba #SomosContinuidad\n",
      "https://t.co/OM8dhikaPv\n",
      "---Science---Society---\n",
      "RT @BasedPoland: This video will make you understand what's at stake in ðŸ‡§ðŸ‡· \n",
      "\n",
      "The Supreme Court (STF) wants to impeach Pres. #Bolsonaro  \n",
      "\n",
      "Iâ€¦\n",
      "---Mass_media---Politics---\n",
      "RT @zora69450573: #VenceremosAlCOVID19\n",
      "---Internet---Health---\n",
      "RT @CECCgov: The #ChinaCables reveal for the world to see the brutality of the mass internment camps &amp; high-tech policing system in #Xinjiaâ€¦\n",
      "---Law---Politics---\n",
      "RT @nikki_miumiu: Donâ€™t get it wrong, world, the success of #HongKong Election and #HumanRightsDay march didnâ€™t change the govtâ€™s idea to sâ€¦\n",
      "---Government---Politics---\n",
      "RT @NASAKennedy: The countdown to #LaunchAmerica is on! ðŸš€\n",
      "\n",
      "Just one day remains until @AstroBehnken and @Astro_Doug will launch to the @Spaâ€¦\n",
      "---Universe---Technology---\n",
      "RT @WesJWHK: @RepJoeKennedy Shameless #CCP has lost its patience &amp; is launching the most direct blatant assault on Hong Kongâ€™s #RuleOfLaw/jâ€¦\n",
      "---Government---Society---\n",
      "RT @YourMarkLubbers: Make no mistake: Hong Kong will be free.\n",
      "\n",
      "#Anonymous #OpHongKong #SelfDetermination #HongKongProtests https://t.co/Xzfâ€¦\n",
      "---Law---Society---\n",
      "RT @WellingMichael: #Venezuela #3Sep COHERENCEY AND CLARITY! @MariaCorinaYA : \"First things first, get #Maduro and the mafias out of power,â€¦\n",
      "---Internet---Politics---\n",
      "RT @joshuawongcf: 1/ On the eve of HK's Tiananmen massacre vigil, Beijing is now scrapping its promise of #1country2systems by circumventinâ€¦\n",
      "---Government---Politics---\n",
      "RT @ecapobianco: Striking to see the trajectory of the #USðŸ‡ºðŸ‡¸ #SpainðŸ‡ªðŸ‡¸ &amp; #UKðŸ‡¬ðŸ‡§. ItalyðŸ‡®ðŸ‡¹seems to be finally bending the curve but the blue coâ€¦\n",
      "---Time---Military---\n",
      "RT @RepCharlieCrist: Sent letter calling on the #PeoplesHouse to investigate the Chinese Communist Partyâ€™s role in covering up the pandemicâ€¦\n",
      "---Military---Health---\n",
      "RT @SungYoonLee1: Yoon Mee-hyang, disgraced ex-#ComfortWomen \"advocate,\" her husband &amp; #Minbyun lawyers in 2018 pressured NK restaurant manâ€¦\n",
      "---Human_behavior---Law---\n",
      "RT @Adomenas: The cross for Hong Kong is already in place - on the Hill of Crosses in #Lithuania. There will be others - signs, demonstratiâ€¦\n",
      "---Military---Politics---\n",
      "RT @DiazCanelB: Cuba and China work on the first joint biotech park.\n",
      "#SomosCuba \n",
      "https://t.co/B2IF4S3Fm9\n",
      "---Science---Military---\n",
      "RT @ferozaazizz: What the media fails to show you...\n",
      "#afghanistan #kabul #Nangarhar #KabulHospitalAttack #NangarharAttack #Syria #yemen #Etâ€¦\n",
      "---People---Military---\n",
      "RT @tlietome: Can you support me ?\n",
      "#SOSHK #HongKongPoliceTerrorism #ProtectHKStudents #StandWithHongKong https://t.co/6LguW3Bp7r\n",
      "---Government---Law---\n",
      "RT @benedictrogers: #POLL: Should Britain place #Magnitsky sanctions on key #CCP and #HongKong political leaders and Police for perpetratinâ€¦\n",
      "---Law---Politics---\n",
      "RT @AFP: #IWD2020 Women were on the frontline of a wave of popular protests that rocked the world over the past 12 months. For #Internationâ€¦\n",
      "---Mass_media---Law---\n",
      "RT @HouseForeignGOP: LR @RepMcCaul calls for bipartisan investigation into the #ChineseCommunistyParty &amp; @WHOâ¬‡ï¸\n",
      "\n",
      "\"In my judgment, it is appâ€¦\n",
      "---Politics---Military---\n",
      "RT @MischaEDM: Exposing #Facebook, you got Horizons Ventures (Li Ka-Shing Family Foundation), and BINGO!\n",
      "\n",
      "CCPâ€™s VOA Amanda Bennettâ€™s hubbyâ€¦\n",
      "---Human_behavior---Internet---\n",
      "RT @Ali_F_Alizada: 23rd of March has a double significance for our family. Today along with #PakistanDay is also my daughter #Nazrinâ€™s #birâ€¦\n",
      "---Culture---Human_behavior---\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for ypred, ytrue in zip(predictions, y_test):\n",
    "    if idx > 100: break\n",
    "    if ypred != ytrue:\n",
    "        print(X_test[idx])\n",
    "        print(\"---\" + ypred + \"---\" + ytrue + \"---\")\n",
    "        # print(label_encoder.inverse_transform([ypred]), label_encoder.inverse_transform([ytrue]))\n",
    "    idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
