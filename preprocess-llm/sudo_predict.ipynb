{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "18289\n"
     ]
    }
   ],
   "source": [
    "# read from down sampling results\n",
    "# apply sudo label assignment\n",
    "\n",
    "import regex\n",
    "import json\n",
    "\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/input_sample.jsonl\"\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1/generation_1.jsonl\"\n",
    "down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "\n",
    "regex_pattern = r\"\\[\\d+\\] ([\\w\\s]+)\"\n",
    "\n",
    "def read_from_topicgpt(filepath):\n",
    "    user_tweets = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            text = info[\"text\"]\n",
    "            mt_info = info[\"meta_info\"]\n",
    "            topic = info[\"responses\"]\n",
    "            topic = regex.compile(regex_pattern).findall(topic)\n",
    "            if len(topic) > 0: topic = topic[0]\n",
    "            else: topic = \"None\"\n",
    "            if mt_info[\"user\"] not in user_tweets: user_tweets[mt_info[\"user\"]] = []\n",
    "            user_tweets[mt_info[\"user\"]].append({\n",
    "                \"user\": mt_info[\"user\"],\n",
    "                \"ts\": mt_info[\"ts\"],\n",
    "                \"text\": text,\n",
    "                \"tag\": mt_info[\"tag\"],\n",
    "                \"bertopic_label\": mt_info[\"bertopic_label\"],\n",
    "                \"topicgpt_label\": topic,\n",
    "            })\n",
    "    return user_tweets\n",
    "\n",
    "user_infos = read_from_topicgpt(down_sampling_filepath)\n",
    "print(len(user_infos))\n",
    "print(sum([len(tweets) for user, tweets in user_infos.items()]))\n",
    "\n",
    "# for user, infos in user_infos.items():\n",
    "#     print(user)\n",
    "#     for info in infos:\n",
    "#         print(info)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256\n",
      "106988\n"
     ]
    }
   ],
   "source": [
    "# # Get other unlabeled data\n",
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "# user_tweets_dict = {}\n",
    "# for tag, cascades in cascade_dict.items():\n",
    "#     for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "#         if user not in user_tweets_dict:\n",
    "#             user_tweets_dict[user] = []\n",
    "#         user_tweets_dict[user].append({\n",
    "#             \"tag\": tag,\n",
    "#             \"bertopic_label\": str(bertopic_label),\n",
    "#             \"user\": user,\n",
    "#             \"ts\": ts,\n",
    "#             \"text\": tweet,\n",
    "#         })\n",
    "# print(sum([len(tweets) for user, tweets in user_tweets_dict.items()]))\n",
    "\n",
    "# user_remain_infos = {}\n",
    "# user_seeds = user_infos.keys()\n",
    "# for user, full_infos in user_tweets_dict.items():\n",
    "#     if user not in user_seeds:\n",
    "#         # user_remain_infos[user] = full_infos\n",
    "#         continue\n",
    "\n",
    "#     infos = user_infos[user]\n",
    "#     remain_infos = []\n",
    "#     for info in full_infos:\n",
    "#         flag = False\n",
    "#         for s_info in infos:\n",
    "#             if info[\"text\"] == s_info[\"text\"] \\\n",
    "#                 and info[\"tag\"] == s_info[\"tag\"] \\\n",
    "#                 and info[\"user\"] == s_info[\"user\"] \\\n",
    "#                 and info[\"ts\"] == s_info[\"ts\"]:\n",
    "#                 flag = True\n",
    "#                 break\n",
    "#         if not flag:\n",
    "#             remain_infos.append(info)\n",
    "#     # print(len(infos), len(remain_infos), len(full_infos), \"{:.2f}\".format(len(remain_infos) / len(full_infos)))\n",
    "#     # try:\n",
    "#     #     assert len(infos) + len(remain_infos) == len(full_infos)\n",
    "#     # except Exception as e:\n",
    "#     #     print(user, len(infos), len(remain_infos), len(full_infos))\n",
    "#     user_remain_infos[user] = remain_infos\n",
    "\n",
    "# print(sum([len(infos) for infos in user_remain_infos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17471\n",
      "18289\n"
     ]
    }
   ],
   "source": [
    "# 2. Use trained bertopic model to assign sudo labels\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# TODO: use broader interests\n",
    "gt_ft_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/gt_ft_mapping.data\"\n",
    "gt_ft_mapping = load_pickle(gt_ft_filepath)\n",
    "\n",
    "# Get labeled data\n",
    "data = []\n",
    "target = []\n",
    "for user, infos in user_infos.items():\n",
    "    for info in infos:\n",
    "        if info[\"topicgpt_label\"] == \"None\": continue\n",
    "        if info[\"topicgpt_label\"] not in gt_ft_mapping: continue\n",
    "        data.append(info[\"text\"])\n",
    "        # target.append(info[\"topicgpt_label\"])\n",
    "        target.append(gt_ft_mapping[info[\"topicgpt_label\"]])\n",
    "\n",
    "# Discretize targets\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# target = label_encoder.fit_transform(target)\n",
    "\n",
    "print(len(data))\n",
    "print(sum([len(tweets) for user, tweets in user_infos.items()]))\n",
    "\n",
    "# split into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bertopic Model\n",
    "\n",
    "# Skip over dimensionality reduction, replace cluster model with classifier,\n",
    "# and reduce frequent words while we are at it.\n",
    "empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "clf = LogisticRegression()\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a fully supervised BERTopic instance\n",
    "topic_model= BERTopic(\n",
    "    umap_model=empty_dimensionality_model,\n",
    "    hdbscan_model=clf,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    embedding_model=sbert,\n",
    ")\n",
    "topic_model = topic_model.fit(X_train, y=y_train)\n",
    "\n",
    "# Map input `y` to topics\n",
    "mappings = topic_model.topic_mapper_.get_mappings()\n",
    "# reverse_mapping = {v: k for k, v in mappings.items()}\n",
    "# print(mappings)\n",
    "\n",
    "y_preds = []\n",
    "topics, _ = topic_model.transform(X_test[:100])\n",
    "# interest_labels = label_encoder.inverse_transform([reverse_mapping[topic] for topic in topics])\n",
    "interest_labels = [mappings[topic] for topic in topics]\n",
    "print(interest_labels)\n",
    "# y_preds.append(mappings[topic[0]])\n",
    "\n",
    "# test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test[:100], interest_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42\n"
     ]
    }
   ],
   "source": [
    "# 2. Direct SBert Cosine Similarity\n",
    "\n",
    "# Get embeddings\n",
    "sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "embeddings = sbert.encode(X_train)\n",
    "\n",
    "topic_embeddings = {}\n",
    "for emb, label in zip(embeddings, y_train):\n",
    "    if label not in topic_embeddings:\n",
    "        topic_embeddings[label] = []\n",
    "    topic_embeddings[label].append(emb)\n",
    "\n",
    "for interest, emb in topic_embeddings.items():\n",
    "    topic_embeddings[interest] = np.mean(emb, axis=0)\n",
    "    # print(topic_embeddings[interest].shape)\n",
    "key2idx = {i: k for i, k in enumerate(topic_embeddings.keys())}\n",
    "topic_t_embeddings = np.array([emb for emb in topic_embeddings.values()])\n",
    "\n",
    "# Get test embeddings\n",
    "test_embeddings = sbert.encode(X_test[:100])\n",
    "# print(topic_t_embeddings.shape)\n",
    "# print(test_embeddings.shape)\n",
    "\n",
    "# Get cosine similarity and Give Labels\n",
    "cosine_scores = util.pytorch_cos_sim(test_embeddings, topic_t_embeddings)\n",
    "# print(cosine_scores.shape)\n",
    "predictions = np.argmax(cosine_scores, axis=1).tolist()\n",
    "predictions = [key2idx[p] for p in predictions]\n",
    "# print(predictions)\n",
    "# print(y_test[:100])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test[:100], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256\n",
      "106988\n"
     ]
    }
   ],
   "source": [
    "# Get other unlabeled data\n",
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "user_tweets_dict = {}\n",
    "for tag, cascades in cascade_dict.items():\n",
    "    for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "        if user not in user_tweets_dict:\n",
    "            user_tweets_dict[user] = []\n",
    "        user_tweets_dict[user].append({\n",
    "            \"tag\": tag,\n",
    "            \"bertopic_label\": str(bertopic_label),\n",
    "            \"user\": user,\n",
    "            \"ts\": ts,\n",
    "            \"text\": tweet,\n",
    "        })\n",
    "print(sum([len(tweets) for user, tweets in user_tweets_dict.items()]))\n",
    "\n",
    "user_remain_infos = {}\n",
    "user_seeds = user_infos.keys()\n",
    "for user, full_infos in user_tweets_dict.items():\n",
    "    if user not in user_seeds:\n",
    "        # user_remain_infos[user] = full_infos\n",
    "        continue\n",
    "\n",
    "    infos = user_infos[user]\n",
    "    remain_infos = []\n",
    "    for info in full_infos:\n",
    "        flag = False\n",
    "        for s_info in infos:\n",
    "            if info[\"text\"] == s_info[\"text\"] \\\n",
    "                and info[\"tag\"] == s_info[\"tag\"] \\\n",
    "                and info[\"user\"] == s_info[\"user\"] \\\n",
    "                and info[\"ts\"] == s_info[\"ts\"]:\n",
    "                flag = True\n",
    "                break\n",
    "        if not flag:\n",
    "            remain_infos.append(info)\n",
    "    # print(len(infos), len(remain_infos), len(full_infos), \"{:.2f}\".format(len(remain_infos) / len(full_infos)))\n",
    "    # try:\n",
    "    #     assert len(infos) + len(remain_infos) == len(full_infos)\n",
    "    # except Exception as e:\n",
    "    #     print(user, len(infos), len(remain_infos), len(full_infos))\n",
    "    user_remain_infos[user] = remain_infos\n",
    "\n",
    "print(sum([len(infos) for infos in user_remain_infos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user, infos \u001b[38;5;129;01min\u001b[39;00m user_remain_infos\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     testing_texts \u001b[38;5;241m=\u001b[39m [info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m infos]\n\u001b[0;32m----> 5\u001b[0m     test_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mpytorch_cos_sim(test_embeddings, topic_t_embeddings)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(cosine_scores.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    982\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    983\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    987\u001b[0m )\n\u001b[0;32m--> 988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    572\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    573\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         output_attentions,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/topicGPT/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:342\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[1;32m    340\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[0;32m--> 342\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n\u001b[1;32m    344\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# user_remain_infos_cp = {}\n",
    "\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1.jsonl\"\n",
    "\n",
    "with open(save_filepath, 'w') as f:\n",
    "    for user, infos in user_remain_infos.items():\n",
    "        testing_texts = [info[\"text\"] for info in infos]\n",
    "        test_embeddings = sbert.encode(X_test[:100])\n",
    "        cosine_scores = util.pytorch_cos_sim(test_embeddings, topic_t_embeddings)\n",
    "        # print(cosine_scores.shape)\n",
    "        predictions = np.argmax(cosine_scores, axis=1).tolist()\n",
    "        predictions = [key2idx[p] for p in predictions]\n",
    "\n",
    "        # user_remain_infos_cp[user] = []\n",
    "        for info, pred in zip(infos, predictions):\n",
    "            info[\"topicgpt_label\"] = pred\n",
    "            f.write(json.dumps(info) + \"\\n\")\n",
    "            # user_remain_infos_cp[user].append(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @HananyaNaftali: The Church of the Holy Sepulchre in #Jerusalem is closed due to Coronavirus. #Israel\n",
      "\n",
      "The last time it was closed was i…\n",
      "---Health---Religion---\n",
      "RT @krislc: #政治攬炒 political mutual destruction has begun. Question is: Beijing knows this will likely happen when deciding to break from tr…\n",
      "---Military---Politics---\n",
      "Pakistan condemned the inhuman Syrian Regime attack on Turkish troops. Pakistanis are with the Turks at this critical time; #sehitlerimizvar\n",
      "---Academic_disciplines---Military---\n",
      "RT @www_HKer: For 🇹🇭Thai friends: \n",
      "\n",
      "#Rule1 to fight #CCPViurs is : you do not trust CCP\n",
      "#Rule2 : you DO NOT trust CCP\n",
      "#Rule3 : remember the…\n",
      "---Politics---Military---\n",
      "RT @Yfreeeeeeehk: Lovely #HongKongers❤️\n",
      "Forwadly to clean up the Mosque after police defaced it by a water cannon.\n",
      "#PoliceBrutalitiy #HongK…\n",
      "---Society---Food_and_drink---\n",
      "#Gambian newspapers #DailyNews and @PointGambia highlight today #4Mar, in its printed editions, the solidarity voca… https://t.co/u1R13oWLB4\n",
      "---Academic_disciplines---Information---\n",
      "RT @UNDP: By 2030, 🌡️ across the Arab Region could rise by 2C. How do you transition to #greenenergy &amp; build #ClimateAction in one of the h…\n",
      "---Nature---Technology---\n",
      "RT @C_Barraud: 🇺🇸 🇨🇳 The US and #China are on the brink of a new #ColdWar that could devastate the global economy - Business Insider\n",
      "https:…\n",
      "---Economy---Politics---\n",
      "RT @marcorubio: Nervous days for drug lords as @Southcom conducts massive counter-drug operation off the coasts of #Venezuela https://t.co/…\n",
      "---Science---Military---\n",
      "RT @TexasDSHS: #StayHomeTexas and flatten the curve. \n",
      "\n",
      "Texas doctors, first responders, and all health care and emergency workers need you…\n",
      "---Academic_disciplines---Health---\n",
      "RT @WellingMichael: Regime #StormTroopers of socialist  Regime in Venezuela 🇻🇪... will #BernieSanders apply the same use of force to Americ…\n",
      "---Military---Politics---\n",
      "RT @fahadnabeelfn: #CoronaVirusUpdates - #Kuwait amends adhan/azaan, calls to ‘pray at home’\n",
      "\n",
      "\"It is said in Muslim book Sahih Al- Bukhari…\n",
      "---Health---Religion---\n",
      "RT @MaryJoe38642126: .@democracynow \n",
      "\n",
      "PLS REPORT: It’s #Crucial That #China #BanYulin The #DogCatMeatTrade &amp; #WetMarkets \n",
      "\n",
      "#Sick &amp; #Healthy…\n",
      "---Economy---Politics---\n",
      "RT @DrTaraO: Kim Dae-Jung Center sued Lee Ju-seong, who escaped from #NorthKorea, for libel for writing a book Purple Lake--re NKorean invo…\n",
      "---Mass_media---Society---\n",
      "RT @CynthiaDRitchie: In #ZardarisFilthyPPP an FIR has been lodged against me in Jacobabad, accusing me of character assassination of BB.\n",
      "\n",
      "L…\n",
      "---Information---Politics---\n",
      "RT @JackLawsome: Kind words can be short\n",
      "and easy to speak,\n",
      "but their echoes are truly\n",
      "endless. ~ Mother Teresa\n",
      "\n",
      "#JackLawsome #Kindness #Im…\n",
      "---Philosophy---Human_behavior---\n",
      "#October1st https://t.co/87wGJQPp9V\n",
      "---Entertainment---Internet---\n",
      "RT @lattevani: #FreedomHK #GlobeConnect\n",
      "Crowdfunding miracle\n",
      "Hong Kong protest https://t.co/sBvMV90OhG\n",
      "---Politics---Society---\n",
      "RT @CarolynBMaloney: Yesterday, Congress passed the #HongKongHumanRightsDemocracyAct - a strong signal to President Trump &amp; the Chinese Gov…\n",
      "---Law---Politics---\n",
      "RT @seedling_tv: What would you do if you knew you could grow more food on your #farm while #conserving #resources? Well, it’s possible and…\n",
      "---Food_and_drink---Society---\n",
      "RT @Alam_Chaudry: #LiKashing is one of the mega billionaire had the courage to stand by #HongKongProstesters and speak out against #Beijing…\n",
      "---Law---Society---\n",
      "RT @AapkaCharul: Some Hate mongers responsible for #DelhiRiots are Swara, Rana Ayyub &amp; Saba Naqvi. They spreaded Hate, Rumours, False video…\n",
      "---Mass_media---Internet---\n",
      "RT @ClaudiaMCMo: Raw evidence of Hongkong #police behaving like #thugs.\n",
      "See footage on right from yesterday aftn.\n",
      "#HKProtests\n",
      "Credit, NowTV…\n",
      "---Society---Government---\n",
      "RT @Haidar_Ahmad_: A fire erupted yesterday at the #Palmyra Orchid oasis south of the city.\n",
      "\n",
      "The fire spread across an area of around 150 h…\n",
      "---Nature---Health---\n",
      "RT @Stand_with_HK: 🔴#LIVE: The #HongKongEOM Press Conference is happening now! 19 delegates from 10 countries are sharing their observation…\n",
      "---Government---Military---\n",
      "RT @andrewspha: So kahle kahle the government want children to go to school kodwa dololo with all necessary #Covid19SA regulations includin…\n",
      "---Education---Health---\n",
      "RT @ipsvipul_: A very happy #CivilServicesDay to all seniors and colleagues in various services. May God give us strength to keep performin…\n",
      "---Academic_disciplines---Politics---\n",
      "RT @JimmyLaiApple: Many “celebrities\" and their family members have foreign passports. #JackieChan, #LisaWang, #EricTsang #Alan Tam etc are…\n",
      "---Business---Society---\n",
      "RT @KongTsungGan: Today is Day 1 of #FiveDemandsWeek/#HKGoldenWeek &amp; all over #HK people have been queuing up at #YellowEconomicCircle shop…\n",
      "---Government---Society---\n",
      "RT @HKers721: @IngrahamAngle @SolomonYue upon #US-CCPWar, #ObamaGate uncovered, #LiFeiFei the #CCP spy, controlling twitter, #CCP organised…\n",
      "---Politics---Military---\n",
      "RT @MHiesboeck: As I said a week ago.  HK is gone. #ballots for election of House Committee chairman were distributed to pro Beijing lawmak…\n",
      "---Government---Politics---\n",
      "#HongKongPoliceState\n",
      "#HongKongPoliceTerrorists\n",
      "#HongKongPoliceBurtality \n",
      "#SOSHK #PolyU\n",
      "#HongKongProtesters https://t.co/5qRnbO68XQ\n",
      "---Government---Society---\n",
      "RT @sumlokkei: #dc2019 just became the biggest election in #hk history - with 2.33 million turned out to vote, beating the 2016 #legco elec…\n",
      "---Government---Politics---\n",
      "RT @yannnnnn111: A #HongKonger joined the Cake International 2019 Competition in #UK w/ a HK theme cake\n",
      "\n",
      "However, she is being disqualified…\n",
      "---Law---Time---\n",
      "RT @OfficalPshk: #HKers paid tribute outside the #PrinceEdward train station as today marks the 9-month anniversary since the #831Incident.…\n",
      "---Time---Law---\n",
      "RT @Dr_fizakhan: #Kashmir tujhe salaam.😥😢 #LockdownExtended #kashmir #IndiaLockdown https://t.co/RzoP6HhQ0A\n",
      "---Religion---Government---\n",
      "RT @DiazCanelB: Doctors and not bombs. Fidel's legacy. \n",
      "#SomosCuba #SomosContinuidad\n",
      "https://t.co/OM8dhikaPv\n",
      "---Science---Society---\n",
      "RT @BasedPoland: This video will make you understand what's at stake in 🇧🇷 \n",
      "\n",
      "The Supreme Court (STF) wants to impeach Pres. #Bolsonaro  \n",
      "\n",
      "I…\n",
      "---Mass_media---Politics---\n",
      "RT @zora69450573: #VenceremosAlCOVID19\n",
      "---Internet---Health---\n",
      "RT @CECCgov: The #ChinaCables reveal for the world to see the brutality of the mass internment camps &amp; high-tech policing system in #Xinjia…\n",
      "---Law---Politics---\n",
      "RT @nikki_miumiu: Don’t get it wrong, world, the success of #HongKong Election and #HumanRightsDay march didn’t change the govt’s idea to s…\n",
      "---Government---Politics---\n",
      "RT @NASAKennedy: The countdown to #LaunchAmerica is on! 🚀\n",
      "\n",
      "Just one day remains until @AstroBehnken and @Astro_Doug will launch to the @Spa…\n",
      "---Universe---Technology---\n",
      "RT @WesJWHK: @RepJoeKennedy Shameless #CCP has lost its patience &amp; is launching the most direct blatant assault on Hong Kong’s #RuleOfLaw/j…\n",
      "---Government---Society---\n",
      "RT @YourMarkLubbers: Make no mistake: Hong Kong will be free.\n",
      "\n",
      "#Anonymous #OpHongKong #SelfDetermination #HongKongProtests https://t.co/Xzf…\n",
      "---Law---Society---\n",
      "RT @WellingMichael: #Venezuela #3Sep COHERENCEY AND CLARITY! @MariaCorinaYA : \"First things first, get #Maduro and the mafias out of power,…\n",
      "---Internet---Politics---\n",
      "RT @joshuawongcf: 1/ On the eve of HK's Tiananmen massacre vigil, Beijing is now scrapping its promise of #1country2systems by circumventin…\n",
      "---Government---Politics---\n",
      "RT @ecapobianco: Striking to see the trajectory of the #US🇺🇸 #Spain🇪🇸 &amp; #UK🇬🇧. Italy🇮🇹seems to be finally bending the curve but the blue co…\n",
      "---Time---Military---\n",
      "RT @RepCharlieCrist: Sent letter calling on the #PeoplesHouse to investigate the Chinese Communist Party’s role in covering up the pandemic…\n",
      "---Military---Health---\n",
      "RT @SungYoonLee1: Yoon Mee-hyang, disgraced ex-#ComfortWomen \"advocate,\" her husband &amp; #Minbyun lawyers in 2018 pressured NK restaurant man…\n",
      "---Human_behavior---Law---\n",
      "RT @Adomenas: The cross for Hong Kong is already in place - on the Hill of Crosses in #Lithuania. There will be others - signs, demonstrati…\n",
      "---Military---Politics---\n",
      "RT @DiazCanelB: Cuba and China work on the first joint biotech park.\n",
      "#SomosCuba \n",
      "https://t.co/B2IF4S3Fm9\n",
      "---Science---Military---\n",
      "RT @ferozaazizz: What the media fails to show you...\n",
      "#afghanistan #kabul #Nangarhar #KabulHospitalAttack #NangarharAttack #Syria #yemen #Et…\n",
      "---People---Military---\n",
      "RT @tlietome: Can you support me ?\n",
      "#SOSHK #HongKongPoliceTerrorism #ProtectHKStudents #StandWithHongKong https://t.co/6LguW3Bp7r\n",
      "---Government---Law---\n",
      "RT @benedictrogers: #POLL: Should Britain place #Magnitsky sanctions on key #CCP and #HongKong political leaders and Police for perpetratin…\n",
      "---Law---Politics---\n",
      "RT @AFP: #IWD2020 Women were on the frontline of a wave of popular protests that rocked the world over the past 12 months. For #Internation…\n",
      "---Mass_media---Law---\n",
      "RT @HouseForeignGOP: LR @RepMcCaul calls for bipartisan investigation into the #ChineseCommunistyParty &amp; @WHO⬇️\n",
      "\n",
      "\"In my judgment, it is app…\n",
      "---Politics---Military---\n",
      "RT @MischaEDM: Exposing #Facebook, you got Horizons Ventures (Li Ka-Shing Family Foundation), and BINGO!\n",
      "\n",
      "CCP’s VOA Amanda Bennett’s hubby…\n",
      "---Human_behavior---Internet---\n",
      "RT @Ali_F_Alizada: 23rd of March has a double significance for our family. Today along with #PakistanDay is also my daughter #Nazrin’s #bir…\n",
      "---Culture---Human_behavior---\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for ypred, ytrue in zip(predictions, y_test):\n",
    "    if idx > 100: break\n",
    "    if ypred != ytrue:\n",
    "        print(X_test[idx])\n",
    "        print(\"---\" + ypred + \"---\" + ytrue + \"---\")\n",
    "        # print(label_encoder.inverse_transform([ypred]), label_encoder.inverse_transform([ytrue]))\n",
    "    idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
