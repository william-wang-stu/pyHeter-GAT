{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n",
    "    \n",
    "def sort_dict(m: dict) -> dict:\n",
    "    return dict(sorted(m.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Generation_1\n",
    "# import json\n",
    "\n",
    "# sample_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/input_sample.jsonl\"\n",
    "# samples = []\n",
    "# with open(sample_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         samples.append(json.loads(line))\n",
    "\n",
    "# result_filepath = \"/root/pyHeter-GAT/preprocess-llm/topicGPT/script/generation_1_tmp copy.jsonl\"\n",
    "# results = []\n",
    "# with open(result_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "# with open(save_filepath, 'w') as f:\n",
    "#     for idx, sample in enumerate(samples):\n",
    "#         result = {\n",
    "#             **sample,\n",
    "#             \"responses\": results[idx][\"response\"],\n",
    "#         }\n",
    "#         f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9388\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# sample_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample.jsonl\"\n",
    "# samples = []\n",
    "# with open(sample_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         samples.append(json.loads(line))\n",
    "\n",
    "# result_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/generation_1/generation_1.jsonl\"\n",
    "# results = []\n",
    "# with open(result_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/generation_1/generation_1.jsonl\"\n",
    "# with open(save_filepath, 'w') as f:\n",
    "#     for idx, sample in enumerate(samples):\n",
    "#         if idx == 1000: continue\n",
    "#         i = idx\n",
    "#         if i > 1000: i -= 1\n",
    "#         try:\n",
    "#             result = {\n",
    "#                 **sample,\n",
    "#                 \"responses\": results[i][\"responses\"],\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(i)\n",
    "#             print(e)\n",
    "#         f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_&_social_concern 43511.8 25\n",
      "arts_&_culture 24074 14\n",
      "diaries_&_daily_life 16859 10\n",
      "science_&_technology 16049 9\n",
      "business_&_entrepreneurs 12969 7\n",
      "film_tv_&_video 11124 6\n",
      "food_&_dining 10025 6\n",
      "sports 7190 4\n",
      "other_hobbies 6597 4\n",
      "celebrity_&_pop_culture 6398 4\n",
      "travel_&_adventure 5748 3\n",
      "learning_&_educational 4090 2\n",
      "music 3170 2\n",
      "fitness_&_health 3134 2\n",
      "family 2253 1\n",
      "gaming 1557 1\n",
      "relationships 1033 1\n",
      "fashion_&_style 768 0\n",
      "youth_&_student_life 100 0\n"
     ]
    }
   ],
   "source": [
    "# # 1. 观察Bertopic生成的话题标签数量级\n",
    "\n",
    "# filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\"\n",
    "# cascade_dict = load_pickle(filepath)\n",
    "\n",
    "# cnt = {}\n",
    "\n",
    "# label_mp = {\n",
    "#     0: \"arts_&_culture\",\n",
    "#     1: \"business_&_entrepreneurs\",\n",
    "#     2: \"celebrity_&_pop_culture\",\n",
    "#     3: \"diaries_&_daily_life\",\n",
    "#     4: \"family\",\n",
    "#     5: \"fashion_&_style\",\n",
    "#     6: \"film_tv_&_video\",\n",
    "#     7: \"fitness_&_health\",\n",
    "#     8: \"food_&_dining\",\n",
    "#     9: \"gaming\",\n",
    "#     10: \"learning_&_educational\",\n",
    "#     11: \"music\",\n",
    "#     12: \"news_&_social_concern\",\n",
    "#     13: \"other_hobbies\",\n",
    "#     14: \"relationships\",\n",
    "#     15: \"science_&_technology\",\n",
    "#     16: \"sports\",\n",
    "#     17: \"travel_&_adventure\",\n",
    "#     18: \"youth_&_student_life\"\n",
    "# }\n",
    "\n",
    "# for key, cascades in cascade_dict.items():\n",
    "#     for b_label in cascades[\"label\"]:\n",
    "#         if b_label not in cnt: cnt[b_label] = 0\n",
    "#         cnt[b_label] += 1\n",
    "\n",
    "# # print(cnt)\n",
    "# cnt[12] /= 10\n",
    "# tot = sum([v for k, v in cnt.items()])\n",
    "# for k, v in sort_dict(cnt).items():\n",
    "#     print(label_mp[k], v, round(100*v/tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_&_social_concern 399217.6 31\n",
      "food_&_dining 304791 24\n",
      "arts_&_culture 223379 18\n",
      "science_&_technology 71271 6\n",
      "diaries_&_daily_life 31752 3\n",
      "other_hobbies 29770 2\n",
      "celebrity_&_pop_culture 27953 2\n",
      "business_&_entrepreneurs 27397 2\n",
      "film_tv_&_video 20163 2\n",
      "fitness_&_health 18130 1\n",
      "sports 15728 1\n",
      "travel_&_adventure 15472 1\n",
      "family 13766 1\n",
      "gaming 13201 1\n",
      "learning_&_educational 12768 1\n",
      "music 12510 1\n",
      "relationships 10685 1\n",
      "fashion_&_style 10504 1\n",
      "youth_&_student_life 10286 1\n"
     ]
    }
   ],
   "source": [
    "# # 2. 统计最后生成的话题通道增广图的边数量级\n",
    "# # 结论: 1)topic=12的图的边数量级在400w, topic=8/0的数量级在30w, 其余话题基本在几w的量级(最少的1w+)\n",
    "# # 2) 从用户话题推文到话题通道的扩展量级维持在10左右, 即推文数量级*10～话题通道的数量级\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "# hedge_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_graph/topic_diffusion_graph_full_windowsize300.data\"\n",
    "# hedge_graph_dict = load_pickle(hedge_filepath)\n",
    "\n",
    "# # 去掉原始的传播边\n",
    "# graph_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/edges.data\"\n",
    "# graph = load_pickle(graph_filepath)\n",
    "# n_org_edges = len(graph)\n",
    "\n",
    "# cnt_after = {}\n",
    "# for key, hgraph in hedge_graph_dict.items():\n",
    "#     # print(hgraph.edge_index)\n",
    "#     _, n_edges = hgraph.edge_index.shape\n",
    "#     # print(n_edges)\n",
    "#     cnt_after[key] = n_edges - n_org_edges\n",
    "#     # print(type(hgraph))\n",
    "\n",
    "# cnt_after[12] /= 10\n",
    "# tot2 = sum([v for k, v in cnt_after.items()])\n",
    "# for k,v in sort_dict(cnt_after).items():\n",
    "#     # print(k, v, cnt[k], v / cnt[k])\n",
    "#     print(label_mp[k], v, round(100*v/tot2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_graph_windowsize30.0.data\n",
      "37\n",
      "29\n",
      "Politics 1098\n",
      "Society 994\n",
      "Law 726\n",
      "Military 699\n",
      "Health 690\n",
      "Internet 422\n",
      "Government 277\n",
      "Religion 268\n",
      "Mass_media 193\n",
      "Economy 174\n",
      "Entertainment 174\n",
      "Technology 152\n",
      "Nature 87\n",
      "Business 84\n",
      "Sports 69\n",
      "Culture 69\n",
      "Education 61\n",
      "Food_and_drink 59\n",
      "Human_behavior 47\n",
      "Academic_disciplines 40\n"
     ]
    }
   ],
   "source": [
    "# 回想一下之前怎么构造的图\n",
    "\n",
    "# 参考utils/graph.py中的build_heteredge_mats函数\n",
    "# 1. 按照级联粒度，使用滑动窗口来构造话题通道\n",
    "# P.S. 降采样了12: news_&_social_concern话题，因为它在总推文中的占比约为43w/52w\n",
    "# 2. 按照话题粒度合并不同级联内的话题通道\n",
    "# 3. 将每个话题的图和原始拓扑图合并\n",
    "# Option: 4. 采用Motif方式做数据增强\n",
    "\n",
    "import json\n",
    "import regex\n",
    "import random\n",
    "from utils.log import logger\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1. Prepare input from TopicGPT\n",
    "# P.S. start from 2000 seed users (out of 10000 total users)\n",
    "\n",
    "result_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "results = []\n",
    "with open(result_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "# 1-2. Prepare topic to interest mappings\n",
    "gt_ft_mapping = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/gt_ft_mapping.data\")\n",
    "\n",
    "# 2. generate simedges from interest-aware cascades\n",
    "\n",
    "def generate_interest_cascades(data_dict: list, remove: bool = False):\n",
    "    regex_pattern = r\"\\[(\\d+)\\] ([\\w\\s]+): ([\\w\\s]+)\\.\"\n",
    "\n",
    "    interest_cascades = {}\n",
    "    for elem in data_dict:\n",
    "        response = elem[\"responses\"]\n",
    "        parts = regex.compile(regex_pattern).findall(response)\n",
    "        if len(parts) > 0: parts = parts[0]\n",
    "        else: continue\n",
    "        _, t, _ = parts[:3]\n",
    "        interest = gt_ft_mapping[t]\n",
    "        if interest not in interest_cascades: interest_cascades[interest] = []\n",
    "        mt_info = elem[\"meta_info\"]\n",
    "        interest_cascades[interest].append((mt_info[\"user\"], int(mt_info[\"ts\"])))\n",
    "    print(len(interest_cascades))\n",
    "    \n",
    "    # remove minor interests\n",
    "    interest_cascades_cp = interest_cascades.copy()\n",
    "    for t, cascades in interest_cascades_cp.items():\n",
    "        if len(cascades) < 10:\n",
    "            interest_cascades.pop(t)\n",
    "    print(len(interest_cascades))\n",
    "\n",
    "    # sort by timestamp\n",
    "    for t in interest_cascades:\n",
    "        interest_cascades[t] = sorted(interest_cascades[t], key=lambda x: x[1])\n",
    "    return interest_cascades\n",
    "\n",
    "def generate_simedges(interest_cascades: dict, time_distance: int = 3600 * 24 * 30):\n",
    "    simedges = {}\n",
    "    for interest, cascades in interest_cascades.items():\n",
    "        simedges[interest] = []\n",
    "        for i in range(len(cascades)-1):\n",
    "            for j in range(i, len(cascades)-1):\n",
    "                if cascades[j][1] - cascades[i][1] < time_distance and \\\n",
    "                    cascades[j][0] == cascades[i][0]:\n",
    "                    simedges[interest].append((cascades[i][0], cascades[j][0]))\n",
    "        # for i in range(len(cascades)-1):\n",
    "        #     for j in range(max(0,i-1-window_size),min(i+1+window_size,len(cascades))): # (i-ws-1<-i->i+ws+1)\n",
    "        #         if cascades[i][0] != cascades[j][0]:\n",
    "        #             simedges[interest].append((cascades[i][0],cascades[j][0]))\n",
    "    \n",
    "    # remove abundant edges\n",
    "    for interest, edges in simedges.items():\n",
    "        simedges[interest] = list(set(edges))\n",
    "    return simedges\n",
    "\n",
    "# 3. convert simedges to graph\n",
    "def convert_to_graph(simedges: dict, originial_edges: Optional[list] = None):\n",
    "    if originial_edges:\n",
    "        for interest, edges in simedges.items():\n",
    "            simedges[interest] = list(set(edges) + set(originial_edges))\n",
    "    \n",
    "    # add self-loop edges\n",
    "    for interest, edges in simedges.items():\n",
    "        edges += [(u,u) for u in set([u for e in edges for u in e])]\n",
    "        simedges[interest] = list(set(edges))\n",
    "    \n",
    "    # convert to graph\n",
    "    graph_d = {}\n",
    "    for interest, edges in simedges.items():\n",
    "        edges = list(zip(*edges))\n",
    "        edges_t = torch.LongTensor(edges)\n",
    "        weight_t = torch.FloatTensor([1]*edges_t.size(1))\n",
    "        graph_d[interest] = Data(edge_index=edges_t, edge_weight=weight_t)\n",
    "    \n",
    "    return graph_d\n",
    "\n",
    "time_distance = 3600 * 24 * 30\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_graph_windowsize{td}.data\"\n",
    "save_filepath = save_filepath.format(td=time_distance / (3600 * 24))\n",
    "print(save_filepath)\n",
    "# graph_d = convert_to_graph(\n",
    "#     generate_simedges(generate_interest_cascades(results, remove=True), window_size))\n",
    "# save_pickle(graph_d, save_filepath)\n",
    "\n",
    "interest_cascades = generate_interest_cascades(results, remove=True)\n",
    "\n",
    "simedges = generate_simedges(interest_cascades, time_distance)\n",
    "# select top20 interests\n",
    "len_mp = {k:len(v) for k,v in simedges.items()}\n",
    "for k in list(sort_dict(len_mp).keys())[:20]:\n",
    "    print(k, len(simedges[k]))\n",
    "    # simedges.pop(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-21 13:01:35\n",
      "2020-01-18 02:32:05.200000\n",
      "2020-03-06 05:04:10.400000\n",
      "2020-04-02 01:36:44.200000\n",
      "2020-04-18 03:24:35.600000\n",
      "2020-05-01 17:11:26\n",
      "2020-05-13 23:57:51.400000\n",
      "2020-05-17 13:52:18.800000\n",
      "2020-05-25 09:52:55\n",
      "2020-06-01 03:48:11.800000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# inspect time distance distribution\n",
    "time_diff = 3600\n",
    "time_distance = []\n",
    "for interest, cascades in interest_cascades.items():\n",
    "    for elem in cascades:\n",
    "        time_distance.append(elem[1] / time_diff)\n",
    "        # print(elem[1])\n",
    "        # print(datetime.datetime.utcfromtimestamp(elem[1]))\n",
    "        # print(datetime.datetime.utcfromtimestamp(elem[1] - time_diff))\n",
    "    break\n",
    "\n",
    "def analyse_distribution(data):\n",
    "    for i in range(10):\n",
    "        t = np.percentile(data, i*10)\n",
    "        print(datetime.datetime.utcfromtimestamp(t * time_diff))\n",
    "\n",
    "analyse_distribution(time_distance)\n",
    "# simedges = generate_simedges(generate_interest_cascades(results, remove=True), window_size)\n",
    "# select top20 interests\n",
    "# len_mp = {k:len(v) for k,v in simedges.items()}\n",
    "# for k in list(sort_dict(len_mp).keys())[20:]:\n",
    "#     print(k, len(simedges[k]))\n",
    "#     simedges.pop(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
