{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n",
    "\n",
    "def call_openai_gpt(input: str, client, model_type: str = \"gpt-4-1106-preview\",):\n",
    "\n",
    "    # proxies = {\n",
    "    #     \"http://\": \"http://127.0.0.1:7890\",\n",
    "    #     \"https://\": \"http://127.0.0.1:7890\"\n",
    "    # }\n",
    "\n",
    "    with httpx.Client(proxies=proxies) as http_client:\n",
    "\n",
    "        response = client.with_options(http_client=http_client).chat.completions.create(\n",
    "            model=model_type,\n",
    "            temperature=0.0,\n",
    "            max_tokens=300,\n",
    "            top_p=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"},\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        response_text = response.choices[0].message.content\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(\"Error info = \", e)\n",
    "        print(\"response = \", response)\n",
    "        return None\n",
    "\n",
    "def call_gpt_multiple_times(input, call_num=5, model_type=\"gpt-4-1106-preview\", stream=False):\n",
    "    while True:\n",
    "        res = call_openai_gpt(input, model_type=model_type, stream=stream)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            call_num -= 1\n",
    "            if call_num == 0:\n",
    "                return None\n",
    "\n",
    "proxies = {\n",
    "    \"http://\": \"http://127.0.0.1:1087\",\n",
    "    \"https://\": \"http://127.0.0.1:1087\"\n",
    "}\n",
    "# sk-proj-G8o8mBeMaQ7MDip722nbT3BlbkFJx9sbLeKa44z1ZvTgZDas\n",
    "client = OpenAI(api_key='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',)\n",
    "\n",
    "input = \"hello\"\n",
    "res = call_openai_gpt(input, client, model_type=\"gpt-4\",)\n",
    "print(res)\n",
    "\n",
    "# with httpx.Client(proxies=proxies) as http_client:\n",
    "#     response = client.with_options(http_client=http_client).get(\"https://api.openai.com/v1/models\")\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 政治：文本讨论的是反腐败问题，这属于政治范畴。\n"
     ]
    }
   ],
   "source": [
    "# 通义千问\n",
    "\n",
    "import random\n",
    "from http import HTTPStatus\n",
    "from dashscope import Generation  # 建议dashscope SDK 的版本 >= 1.14.0\n",
    "import dashscope\n",
    "\n",
    "def call_with_messages(input, model_type=\"qwen-turbo\",):\n",
    "    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                {'role': 'user', 'content': input}]\n",
    "    response = Generation.call(model=model_type,\n",
    "                               messages=messages,\n",
    "                               # 设置随机数种子seed，如果没有设置，则随机数种子默认为1234\n",
    "                               seed=random.randint(1, 10000),\n",
    "                               # 将输出设置为\"message\"格式\n",
    "                               result_format='message')\n",
    "    if response.status_code == HTTPStatus.OK:\n",
    "        print(response[\"output\"].choices[0].message.content)\n",
    "    else:\n",
    "        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (\n",
    "            response.request_id, response.status_code,\n",
    "            response.code, response.message\n",
    "        ))\n",
    "\n",
    "dashscope.api_key = \"sk-4c21db15f70f40ef8265aace84eedb4c\"\n",
    "# dashscope.api_key = \"sk-d20def1d36834c2bb7c519d3fdeede07\"\n",
    "\n",
    "prompt3 = open(\"/root/pyHeter-GAT/preprocess-llm/topicGPT/prompt/generation_1_ch.txt\", \"r\").read()\n",
    "text = \"这任何不以监督体制为核心的反腐败都是耍流氓\"\n",
    "input = prompt3.format(Topics=\"[1] 政治\", Document=text)\n",
    "\n",
    "call_with_messages(input, model_type=\"qwen-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 政治: 提到反腐败的政治议题。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "prompt1_with_special_token = \"\"\"\\\n",
    "[INST]<<SYS>>Read the text below and list up to 3 topics. \\\n",
    "Each topic should contain fewer than 3 words. Ensure you only return the topic and nothing more. \\n\\\n",
    "The desired output format:Topic 1: xxx\\nTopic 2: xxx\\nTopic 3: xxx</SYS>>\n",
    "<text> [/INST]\"\"\"\n",
    "\n",
    "prompt3 = open(\"/root/pyHeter-GAT/preprocess-llm/topicGPT/prompt/generation_1_ch.txt\", \"r\").read()\n",
    "\n",
    "text = \"这任何不以监督体制为核心的反腐败都是耍流氓\"\n",
    "input = prompt3.format(Topics=\"[1] 政治\", Document=text)\n",
    "\n",
    "res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# prompt2 = \"\"\"\\\n",
    "# Read the text below and list up to 3 topics. \\\n",
    "# Each topic should contain fewer than 3 words. Ensure you only return the topic and nothing more. \\n\\\n",
    "# The desired output format:Topic 1: xxx\\nTopic 2: xxx\\nTopic 3: xxx</SYS>>\n",
    "# <text>\"\"\"\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    # 'comp.graphics',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    categories=categories, \n",
    "    shuffle=True, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# print(data_train.keys())\n",
    "\n",
    "for idx, text in enumerate(data_train['data']):\n",
    "    # input = prompt1_with_special_token.replace(\"<text>\", text)\n",
    "    # print(text)\n",
    "    input = prompt3.format(Topics=\"[1] Trade\", Document=text)\n",
    "    print(len(input))\n",
    "    res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "    print(text, res)\n",
    "    \n",
    "    # result = {\n",
    "    #     \"meta_info\": {\n",
    "    #         \"filename\": data_train['filenames'][idx], \n",
    "    #         \"target\": str(data_train['target'][idx]), \n",
    "    #         \"target_names\": data_train['target_names'][data_train['target'][idx]]},\n",
    "    #     \"text\": input,\n",
    "    #     \"topics\": res,\n",
    "    # }\n",
    "    # print(json.dumps(result, ensure_ascii=False))\n",
    "    break\n",
    "\n",
    "# text = data_train['data'][0]\n",
    "# input = prompt1_with_special_token.replace(\"<text>\", text)\n",
    "# res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "# print(res)\n",
    "\n",
    "# input = prompt2.replace(\"<text>\", text)\n",
    "# res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/cascades.data\")\n",
    "\n",
    "# 统计信息\n",
    "# 有大约2263914条推文, 不太能每条都做检索\n",
    "# 每个用户有大约54.72条推文\n",
    "# 有41375个用户\n",
    "\n",
    "user_tweets_dict = {}\n",
    "for tag, cascades in cascade_dict.items():\n",
    "    # print(tag)\n",
    "    # print(cascades['content'][0])\n",
    "    for user, tweet in zip(cascades['user'], cascades['content']):\n",
    "        if user not in user_tweets_dict:\n",
    "            user_tweets_dict[user] = []\n",
    "        user_tweets_dict[user].append(tweet)\n",
    "\n",
    "print(len(user_tweets_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy1: 对每个用户的每条推文做话题分析\n",
    "\n",
    "# Strategy2: 对每个用户做话题分析\n",
    "\n",
    "prompt_template = \"We have several tweets from one particulr user on Twitter. \\\n",
    "We hope to extract some topics from these tweets, the number of the topics is no more than ten. \\n\\\n",
    "Remember you only have to keep the keywords of these extracted topics, i.e. Movies, Entertainment, etc. \\\n",
    "Put all topics in a list, and seperate each other with commas.\\n\\\n",
    "Here are the Tweets:\"\n",
    "\n",
    "for user, tweets in user_tweets_dict.items():\n",
    "    all_tweets = \"\\n\".join(tweets)\n",
    "    input = f\"{prompt_template} {all_tweets}\" + \"\\n\\nTopics:\"\n",
    "    res = call_openai_gpt(input, model_type=\"gpt-4-1106-preview\", stream=False)\n",
    "    print(res)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
