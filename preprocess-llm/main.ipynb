{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n",
    "\n",
    "def call_openai_gpt(input: str, client, model_type: str = \"gpt-4-1106-preview\",):\n",
    "\n",
    "    # proxies = {\n",
    "    #     \"http://\": \"http://127.0.0.1:7890\",\n",
    "    #     \"https://\": \"http://127.0.0.1:7890\"\n",
    "    # }\n",
    "\n",
    "    with httpx.Client(proxies=proxies) as http_client:\n",
    "\n",
    "        response = client.with_options(http_client=http_client).chat.completions.create(\n",
    "            model=model_type,\n",
    "            temperature=0.0,\n",
    "            max_tokens=300,\n",
    "            top_p=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"},\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        response_text = response.choices[0].message.content\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(\"Error info = \", e)\n",
    "        print(\"response = \", response)\n",
    "        return None\n",
    "\n",
    "def call_gpt_multiple_times(input, call_num=5, model_type=\"gpt-4-1106-preview\", stream=False):\n",
    "    while True:\n",
    "        res = call_openai_gpt(input, model_type=model_type, stream=stream)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            call_num -= 1\n",
    "            if call_num == 0:\n",
    "                return None\n",
    "\n",
    "proxies = {\n",
    "    \"http://\": \"http://127.0.0.1:1087\",\n",
    "    \"https://\": \"http://127.0.0.1:1087\"\n",
    "}\n",
    "client = OpenAI(api_key='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',)\n",
    "\n",
    "# input = \"hello\"\n",
    "# res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "# print(res)\n",
    "\n",
    "# with httpx.Client(proxies=proxies) as http_client:\n",
    "#     response = client.with_options(http_client=http_client).get(\"https://api.openai.com/v1/models\")\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971\n",
      "737162\n"
     ]
    }
   ],
   "source": [
    "# # 查看Weibo数据集的推文数据量级\n",
    "# # 总共73w条推文, 不过用户总量是4971, 按照每个用户粒度降采样应该能达到不错的覆盖率\n",
    "\n",
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/cascades.data\")\n",
    "\n",
    "# user_tweets_dict = {}\n",
    "# for tag, cascades in cascade_dict.items():\n",
    "#     for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['word'], cascades['label']):\n",
    "#         if user not in user_tweets_dict:\n",
    "#             user_tweets_dict[user] = []\n",
    "#         user_tweets_dict[user].append({\n",
    "#             \"tag\": tag,\n",
    "#             \"bertopic_label\": str(bertopic_label),\n",
    "#             \"user\": user,\n",
    "#             \"ts\": ts,\n",
    "#             \"text\": tweet,\n",
    "#         })\n",
    "\n",
    "# print(len(user_tweets_dict))\n",
    "# print(sum([len(tweets) for tweets in user_tweets_dict.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 降采样\n",
    "# 2. 找同类项传播话题类别标签\n",
    "\n",
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "user_tweets_dict = {}\n",
    "for tag, cascades in cascade_dict.items():\n",
    "    for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "        if user not in user_tweets_dict:\n",
    "            user_tweets_dict[user] = []\n",
    "        user_tweets_dict[user].append({\n",
    "            \"tag\": tag,\n",
    "            \"bertopic_label\": str(bertopic_label),\n",
    "            \"user\": user,\n",
    "            \"ts\": ts,\n",
    "            \"text\": tweet,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "9596\n",
      "7596\n",
      "2000\n",
      "18163\n",
      "104811\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# down sampling\n",
    "sampled_users = list(user_tweets.keys())\n",
    "print(len(sampled_users))\n",
    "print(len(user_tweets_dict))\n",
    "\n",
    "user_seeds = list(set((user_tweets_dict.keys())) - set(sampled_users))\n",
    "# user_seeds = list(user_tweets_dict.keys())\n",
    "print(len(user_seeds))\n",
    "user_seeds = random.sample(user_seeds, k=2000)\n",
    "print(len(user_seeds))\n",
    "\n",
    "user_tweets_dict_sample = {}\n",
    "user_tweets_dict_remain = {}\n",
    "for us in user_seeds:\n",
    "    unique_ = 0\n",
    "    for info in user_tweets_dict[us]:\n",
    "        info[\"unique_label\"] = unique_\n",
    "        unique_ += 1\n",
    "    user_tweets_dict_sample[us] = random.sample(user_tweets_dict[us], k=min(len(user_tweets_dict[us]), 10))\n",
    "    selected_unique_labels = []\n",
    "    for info in user_tweets_dict_sample[us]:\n",
    "        selected_unique_labels.append(info[\"unique_label\"])\n",
    "    user_tweets_dict_remain[us] = []\n",
    "    for info in user_tweets_dict[us]:\n",
    "        if info[\"unique_label\"] not in selected_unique_labels:\n",
    "            user_tweets_dict_remain[us].append(info)\n",
    "\n",
    "print(sum([len(v) for k, v in user_tweets_dict_sample.items()]))\n",
    "print(sum([len(v) for k, v in user_tweets_dict_remain.items()]))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/input_sample.jsonl\"\n",
    "\n",
    "def save_(filepath, data_dict):\n",
    "    with open(filepath, 'w') as f:\n",
    "        # for user, infos in user_tweets_dict.items():\n",
    "        for user, infos in data_dict.items():\n",
    "            for info in infos:\n",
    "                result = {\n",
    "                    \"meta_info\": {\n",
    "                        \"tag\": info[\"tag\"],\n",
    "                        \"bertopic_label\": info[\"bertopic_label\"],\n",
    "                        \"user\": info[\"user\"],\n",
    "                        \"ts\": info[\"ts\"],\n",
    "                    },\n",
    "                    \"text\": info[\"text\"],\n",
    "                }\n",
    "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm/input_sample.jsonl\"\n",
    "# remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm/input_sample_remain.jsonl\"\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample.jsonl\"\n",
    "remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample_remain.jsonl\"\n",
    "save_(save_filepath, user_tweets_dict_sample)\n",
    "save_(remain_filepath, user_tweets_dict_remain)\n",
    "\n",
    "def analyse_distribution(data):\n",
    "    for i in range(10):\n",
    "        print(np.percentile(data, i*10))\n",
    "\n",
    "# analyse_distribution([len(v) for k, v in user_tweets_dict.items()])\n",
    "# analyse_distribution([len(v) for k, v in user_tweets_dict_sample.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# read from down sampling results\n",
    "# apply sudo label assignment\n",
    "\n",
    "import regex\n",
    "import json\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/input_sample.jsonl\"\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1/generation_1.jsonl\"\n",
    "down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "regex_pattern = r\"\\[\\d+\\] ([\\w\\s]+)\"\n",
    "\n",
    "def read_from_topicgpt(filepath):\n",
    "    user_tweets = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            text = info[\"text\"]\n",
    "            mt_info = info[\"meta_info\"]\n",
    "            topic = info[\"responses\"]\n",
    "            topic = regex.compile(regex_pattern).findall(topic)\n",
    "            if len(topic) > 0: topic = topic[0]\n",
    "            else: topic = \"None\"\n",
    "            if mt_info[\"user\"] not in user_tweets: user_tweets[mt_info[\"user\"]] = []\n",
    "            user_tweets[mt_info[\"user\"]].append({\n",
    "                \"user\": mt_info[\"user\"],\n",
    "                \"ts\": mt_info[\"ts\"],\n",
    "                \"text\": text,\n",
    "                \"tag\": mt_info[\"tag\"],\n",
    "                \"bertopic_label\": mt_info[\"bertopic_label\"],\n",
    "                \"topicgpt_label\": topic,\n",
    "            })\n",
    "    return user_tweets\n",
    "\n",
    "user_tweets = read_from_topicgpt(down_sampling_filepath)\n",
    "print(len(user_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "# # find unsupervised infos\n",
    "# unsupervised_infos = {}\n",
    "\n",
    "# def is_in_list(item, list, return_elem=False):\n",
    "#     flag = False\n",
    "#     elem = None\n",
    "#     for e in list:\n",
    "#         if e[\"user\"] == item[\"user\"] and \\\n",
    "#             e[\"ts\"] == item[\"ts\"] and \\\n",
    "#             e[\"text\"] == item[\"text\"] and \\\n",
    "#             e[\"tag\"] == item[\"tag\"] and \\\n",
    "#             e[\"bertopic_label\"] == item[\"bertopic_label\"]:\n",
    "#             flag = True\n",
    "#             elem = e\n",
    "#         if flag: break\n",
    "#     if not return_elem: return flag\n",
    "#     else: return elem\n",
    "\n",
    "# for user, infos in user_tweets.items():\n",
    "#     full_infos = user_tweets_dict[user]\n",
    "#     # print(len(full_infos))\n",
    "#     unsampled_infos = []\n",
    "#     for info in full_infos:\n",
    "#         if is_in_list(info, infos): continue\n",
    "#         unsampled_infos.append(info)\n",
    "#     unsupervised_infos[user] = unsampled_infos\n",
    "\n",
    "# # analyse_distribution([len(v) for k, v in unsupervised_infos.items()])\n",
    "# print(len(user_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Use SBert to assign sudo labels\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "\n",
    "# sudo_user_infos_mp = {}\n",
    "\n",
    "# for user, infos in user_tweets.items():\n",
    "#     sudo_user_infos_mp[user] = []\n",
    "\n",
    "#     supervised_texts = [info[\"text\"] for info in infos]\n",
    "#     supervised_embs = sbert.encode(supervised_texts, convert_to_tensor=True)\n",
    "\n",
    "#     unsupervised_texts = [info[\"text\"] for info in unsupervised_infos[user]]\n",
    "#     if len(unsupervised_texts) == 0: continue\n",
    "#     unsupervised_embs = sbert.encode(unsupervised_texts, convert_to_tensor=True)\n",
    "    \n",
    "#     cosine_scores = util.cos_sim(supervised_embs, unsupervised_embs).cpu().transpose(0, 1)\n",
    "#     for i, info in enumerate(unsupervised_infos[user]):\n",
    "#         # print(cosine_scores[i].max())\n",
    "#         if cosine_scores[i].max() < 0.4: continue\n",
    "#         sudo_user_infos_mp[user].append({\n",
    "#             \"user\": info[\"user\"],\n",
    "#             \"ts\": info[\"ts\"],\n",
    "#             \"text\": info[\"text\"],\n",
    "#             \"tag\": info[\"tag\"],\n",
    "#             \"bertopic_label\": info[\"bertopic_label\"],\n",
    "#             # \"topicgpt_label\": info[\"topicgpt_label\"],\n",
    "#             \"sudo_label\": infos[cosine_scores[i].argmax()][\"topicgpt_label\"],\n",
    "#         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256 986 5464 4337\n"
     ]
    }
   ],
   "source": [
    "# total_tweet_num = sum([len(v) for k, v in user_tweets_dict.items()])\n",
    "# supervised_tweet_num = sum([len(v) for k, v in user_tweets.items()])\n",
    "# unsupervised_tweet_num = sum([len(v) for k, v in unsupervised_infos.items()])\n",
    "\n",
    "# a = sum([len(v) for k, v in sudo_user_infos_mp.items()])\n",
    "# print(total_tweet_num, supervised_tweet_num, unsupervised_tweet_num, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Travel\n",
      "RT @Kosalendradas: An ingenious clock in #Sanskrit which with every hour reminds of 12 fundamentals of ancient #Vedic philosophy arranged a… RT @Kosalendradas: An ingenious clock in #Sanskrit which with every hour reminds of 12 fundamentals of ancient #Vedic philosophy arranged a…\n",
      "Religion International Relations\n",
      "RT @ipsnaithani: #Divinity from #Devprayag \n",
      "#Uttarakhand My #homestate \n",
      "\n",
      "~\"Hope &amp; Optimism\"  \n",
      "for people to heal &amp; return from #Quarantines… RT @ipsnaithani: #Divinity from #Devprayag \n",
      "#Uttarakhand My #homestate \n",
      "\n",
      "~\"Hope &amp; Optimism\"  \n",
      "for people to heal &amp; return from #Quarantines…\n",
      "4335 0 4337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test Coverage\n",
    "\n",
    "# newer_topicgpt_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1_6991/generation_1.jsonl\"\n",
    "# valid_user_tweets_mp = read_from_topicgpt(newer_topicgpt_filepath)\n",
    "\n",
    "# def test_coverage(user_tweets_mp, sudo_user_infos_mp):\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     missing = 0\n",
    "#     for user, infos in sudo_user_infos_mp.items():\n",
    "#         for info in infos:\n",
    "#             total += 1\n",
    "#             ret = is_in_list(info, user_tweets_mp[user], return_elem=True)\n",
    "#             if ret is None:\n",
    "#                 missing += 1\n",
    "#                 continue\n",
    "#             print(info[\"sudo_label\"], ret[\"topicgpt_label\"])\n",
    "#             print(info[\"text\"], ret[\"text\"])\n",
    "#             if info[\"sudo_label\"] == ret[\"topicgpt_label\"]:\n",
    "#                 correct += 1\n",
    "#     print(missing, correct, total)\n",
    "#     return correct / total\n",
    "\n",
    "# test_coverage(valid_user_tweets_mp, sudo_user_infos_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Use trained bertopic model to assign sudo labels\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "# from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# # Get labeled data\n",
    "# data = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "# docs = data['data']\n",
    "# y = data['target']\n",
    "\n",
    "# # Skip over dimensionality reduction, replace cluster model with classifier,\n",
    "# # and reduce frequent words while we are at it.\n",
    "# empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "# clf = LogisticRegression()\n",
    "# ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# # Create a fully supervised BERTopic instance\n",
    "# topic_model= BERTopic(\n",
    "#     umap_model=empty_dimensionality_model,\n",
    "#     hdbscan_model=clf,\n",
    "#     ctfidf_model=ctfidf_model\n",
    "# )\n",
    "# topics, probs = topic_model.fit_transform(docs, y=y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
