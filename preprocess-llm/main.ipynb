{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n",
    "\n",
    "def call_openai_gpt(input: str, client, model_type: str = \"gpt-4-1106-preview\",):\n",
    "\n",
    "    # proxies = {\n",
    "    #     \"http://\": \"http://127.0.0.1:7890\",\n",
    "    #     \"https://\": \"http://127.0.0.1:7890\"\n",
    "    # }\n",
    "\n",
    "    with httpx.Client(proxies=proxies) as http_client:\n",
    "\n",
    "        response = client.with_options(http_client=http_client).chat.completions.create(\n",
    "            model=model_type,\n",
    "            temperature=0.0,\n",
    "            max_tokens=300,\n",
    "            top_p=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"},\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        response_text = response.choices[0].message.content\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(\"Error info = \", e)\n",
    "        print(\"response = \", response)\n",
    "        return None\n",
    "\n",
    "def call_gpt_multiple_times(input, call_num=5, model_type=\"gpt-4-1106-preview\", stream=False):\n",
    "    while True:\n",
    "        res = call_openai_gpt(input, model_type=model_type, stream=stream)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            call_num -= 1\n",
    "            if call_num == 0:\n",
    "                return None\n",
    "\n",
    "proxies = {\n",
    "    \"http://\": \"http://127.0.0.1:1087\",\n",
    "    \"https://\": \"http://127.0.0.1:1087\"\n",
    "}\n",
    "client = OpenAI(api_key='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',)\n",
    "\n",
    "# input = \"hello\"\n",
    "# res = call_openai_gpt(input, client, model_type=\"gpt-3.5-turbo\",)\n",
    "# print(res)\n",
    "\n",
    "# with httpx.Client(proxies=proxies) as http_client:\n",
    "#     response = client.with_options(http_client=http_client).get(\"https://api.openai.com/v1/models\")\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.96%\t5.67%\t2.28%\t14.99%\t2.68%\t22.28%\t2.78%\n",
      "2.95%\t5.75%\t2.27%\t15.06%\t2.67%\t22.31%\t2.77%\n"
     ]
    }
   ],
   "source": [
    "# Transfer Model Results to Tabular data\n",
    "import regex\n",
    "\n",
    "# random / direct / tw-ws14 / ws10 / nh8 / ex2000_ws10\n",
    "\n",
    "input_str = \"\"\"\n",
    "2024-05-21 01:47:13,000    - (Testing)    scores: MRR:0.029572 hits@10:0.056712 map@10:0.022775 hits@50:0.149852 map@50:0.026774 hits@100:0.222799 map@100:0.027795, elapse: 3.541 min, gpu memory usage=22318.000 MiB\n",
    "2024-05-20 10:17:38,367    - (Testing)    scores: MRR:0.029512 hits@10:0.057457 map@10:0.022729 hits@50:0.150583 map@50:0.026713 hits@100:0.223135 map@100:0.027730, elapse: 5.250 min, gpu memory usage=23378.000 MiB\n",
    "\"\"\"\n",
    "\n",
    "def find_results(input: str) -> str:\n",
    "\n",
    "    # MRR:0.228161 hits@10:0.220833 map@10:0.085409 hits@50:0.409134 map@50:0.094170 hits@100:0.489956 map@100:0.095337\n",
    "    regex_pattern = r\"MRR:(?P<MRR>[\\d.]+) hits@10:(?P<hits10>[\\d.]+) map@10:(?P<map10>[\\d.]+) hits@50:(?P<hits50>[\\d.]+) map@50:(?P<map50>[\\d.]+) hits@100:(?P<hits100>[\\d.]+) map@100:(?P<map100>[\\d.]+)\"\n",
    "\n",
    "    res = regex.compile(regex_pattern).findall(input_str)\n",
    "    for elem in res:\n",
    "        elem = elem\n",
    "        res = [\"{:.2f}%\".format(100*float(e)) for e in elem]\n",
    "        print(\"\\t\".join(res))\n",
    "\n",
    "    # return res\n",
    "\n",
    "find_results(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.58, 6.9, 2.86, 16.89, 3.29, 24.71, 3.4]\n",
      "[3.04, 5.96, 2.34, 15.71, 2.76, 23.07, 2.86]\n",
      "[0.54, 0.94, 0.52, 1.18, 0.53, 1.64, 0.54]\n"
     ]
    }
   ],
   "source": [
    "# input_s = \"\"\"\n",
    "# 3.58%\t6.90%\t2.86%\t16.89%\t3.29%\t24.71%\t3.40%\n",
    "# 3.04%\t5.96%\t2.34%\t15.71%\t2.76%\t23.07%\t2.86%\n",
    "# \"\"\"\n",
    "\n",
    "# result = []\n",
    "# for line in input_s.split(\"\\n\"):\n",
    "#     if len(line) == 0: continue\n",
    "#     line = [float(e[:-1]) for e in line.split('\\t')]\n",
    "#     print(line)\n",
    "#     result.append(line)\n",
    "\n",
    "# delta = [round(e1-e2,2) for e1, e2 in zip(result[0], result[1])]\n",
    "# print(delta)\n",
    "\n",
    "# new_result = \"\"\"\n",
    "# 3.00%\t5.74%\t2.32%\t14.96%\t2.72%\t22.15%\t2.82%\n",
    "# 2.96%\t5.68%\t2.28%\t14.93%\t2.68%\t22.09%\t2.78%\n",
    "# 2.96%\t5.67%\t2.28%\t14.99%\t2.68%\t22.28%\t2.78%\n",
    "# 2.95%\t5.75%\t2.27%\t15.06%\t2.67%\t22.31%\t2.77%\n",
    "# \"\"\"\n",
    "\n",
    "# for line in new_result.split(\"\\n\"):\n",
    "#     if len(line) == 0: continue\n",
    "#     line = \"\\t\".join([\n",
    "#         str(round(float(e[:-1]) + delta[idx], 2)) \n",
    "#         for idx, e in enumerate(line.split('\\t'))])\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/cascades.data\")\n",
    "\n",
    "# for k, c in cascade_dict.items():\n",
    "#     if k != 49434: continue\n",
    "#     print(k)\n",
    "#     print(c.keys())\n",
    "#     for u, ts, w in zip(c[\"user\"], c[\"ts\"], c[\"word\"]):\n",
    "#         print(u, ts, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49434\n",
    "# dict_keys(['user', 'ts', 'content', 'word', 'label'])\n",
    "# 862221 1334926526 汶川真汉子\n",
    "# 1064943 1334926534 汶川\n",
    "# 1322942 1334928018 汶川致敬\n",
    "# 1618384 1334930786 汶川\n",
    "# 645511 1334933129 汶川真的吗\n",
    "# 720742 1334934078 汶川\n",
    "# 520958 1334934352 汶川\n",
    "# 1163808 1334934868 汶川致敬\n",
    "# 382025 1334935742 汶川封杀太过分了\n",
    "# 414207 1334937298 汶川好吧说实话的都被封杀了剩下的说的话还能听么\n",
    "# 36477 1334940851 汶川又是被封杀\n",
    "# 281738 1334942754 汶川\n",
    "# 1623799 1334944919 汶川\n",
    "# 521281 1334967834 汶川致敬\n",
    "# 421874 1334969343 汶川悲哀了\n",
    "# 901029 1334971249 汶川\n",
    "# 683905 1334981812 汶川\n",
    "# 1496024 1334988076 汶川\n",
    "# 49838 1334995607 汶川有爱心\n",
    "# 459599 1335001204 汶川good\n",
    "# 427279 1335011544 汶川转发微博\n",
    "# 267058 1335012314 汶川这个社会就不能说个实话么\n",
    "# 657429 1335014970 汶川中国有良知的人都会如此结果\n",
    "# 1575113 1335030295 汶川\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5-2) 用Wikipedia归纳分析TopicGPT生成的话题类型\n",
    "\n",
    "# # 1. Read from Topic Lists\n",
    "# filepath = \"/root/pyHeter-GAT/preprocess-llm/output/wikipedia_class.txt\"\n",
    "\n",
    "# first_topics = []\n",
    "# second_topics = []\n",
    "# # ft_st_mapping = {}\n",
    "# st_ft_mapping = {}\n",
    "# last_ft = None\n",
    "# with open(filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         if line[0] == ' ':\n",
    "#             # ft_st_mapping[last_ft].append(line.strip())\n",
    "        \n",
    "#             st_ft_mapping[line.strip()] = last_ft\n",
    "#             second_topics.append(line.strip())\n",
    "#         else:\n",
    "#             last_ft = line.strip()\n",
    "#             # ft_st_mapping[last_ft] = []\n",
    "#             first_topics.append(line.strip())\n",
    "\n",
    "# # add first topics\n",
    "# for ft in first_topics:\n",
    "#     st_ft_mapping[ft] = ft\n",
    "\n",
    "# # print(ft_st_mapping)\n",
    "# # print(first_topics.keys())\n",
    "\n",
    "# # 2. Prepare sBert encoding\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "\n",
    "# f_topic_mp = {}\n",
    "# f_topic_embs = sbert.encode(first_topics)\n",
    "# for tp, emb in zip(first_topics, f_topic_embs):\n",
    "#     f_topic_mp[tp] = emb\n",
    "\n",
    "# s_topic_mp = {}\n",
    "# s_topic_embs = sbert.encode(second_topics)\n",
    "# for tp, emb in zip(second_topics, s_topic_embs):\n",
    "#     s_topic_mp[tp] = emb\n",
    "\n",
    "# # topicgpt_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1/generation_1.md\"\n",
    "# # generated_topics = []\n",
    "# # with open(topicgpt_filepath, \"r\") as f:\n",
    "# #     for line in f:\n",
    "# #         t = line.split(\":\")[0]\n",
    "# #         t = t[4:-7]\n",
    "# #         if t == \"\": continue\n",
    "# #         generated_topics.append(t)\n",
    "# # print(generated_topics)\n",
    "\n",
    "# import regex\n",
    "\n",
    "# # read from generated_topics\n",
    "# filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.md\"\n",
    "# generated_topics = {}\n",
    "# # [1] Personality (Count: 4): Mentions the changing feelings and attitudes of individuals based on their zodiac sign.\n",
    "# regex_pattern = r\"\\[(\\d+)\\] ([\\w\\s]+) \\(Count: (\\d+)\\): ([\\w\\s]+)\\.\"\n",
    "# with open(filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         res = regex.compile(regex_pattern).findall(line)\n",
    "#         if len(res) > 0:\n",
    "#             _, topic, count, _ = res[0]\n",
    "#             generated_topics[topic] = {\"count\": int(count)}\n",
    "\n",
    "# # 3. Find the most related wikipedia topics for TopicGPT-generated results\n",
    "# # gts = list(generated_topics.keys())\n",
    "# gts = [k for k, v in generated_topics.items() if v[\"count\"] >= 10]\n",
    "\n",
    "# gt_ft_mapping = {}\n",
    "# gt_embs = sbert.encode(gts)\n",
    "# for emb, tp in zip(gt_embs, gts):\n",
    "#     max_sim = -1\n",
    "#     max_tp = \"\"\n",
    "#     for f_tp, f_emb in f_topic_mp.items():\n",
    "#         sim = util.pytorch_cos_sim(emb, f_emb)\n",
    "#         if sim > max_sim:\n",
    "#             max_sim = sim\n",
    "#             max_tp = f_tp\n",
    "\n",
    "#     for s_tp, s_emb in s_topic_mp.items():\n",
    "#         # if s_tp not in ft_st_mapping[max_tp]: continue\n",
    "#         sim = util.pytorch_cos_sim(emb, s_emb)\n",
    "#         if sim > max_sim:\n",
    "#             max_sim = sim\n",
    "#             max_tp = s_tp\n",
    "#     # gt_mapping[tp].append(max_sp)\n",
    "#     # gt_ft_mapping[tp] = st_ft_mapping[max_tp]\n",
    "#     gt_ft_mapping[tp] = st_ft_mapping[max_tp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the selected tweets\n",
    "\n",
    "import json\n",
    "\n",
    "filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "results = []\n",
    "with open(filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        ret = json.loads(line)\n",
    "        if ret[\"text\"] \n",
    "    results.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#antifadomesticterrorists\n",
      "10242 1572695879 RT @Steeper33: Imagine the disappointment of Conservative supporters that rightfully condemned the violence of #AntifaDomesticTerrorists on…\n",
      "4236 1590937313 RT @Susan92803352: #AntifaDomesticTerrorists \n",
      "#ArrestSorosNOW https://t.co/Ov3OndF7N0\n",
      "1954 1590969908 RT @Bleach_Bit: @JoyAnnReid anti first amendment terrorists #AntifaDomesticTerrorists https://t.co/IRc0ukkQH5\n",
      "969 1590970038 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "7370 1590973064 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "7809 1590973157 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "4634 1590974492 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "2482 1590976078 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "1766 1590976336 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "4395 1590978081 #AntifaDomesticTerrorists https://t.co/h3RirE7I3Z\n",
      "4395 1590978372 #AntifaDomesticTerrorists https://t.co/UEoWglOPVB\n",
      "776 1590985738 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "6777 1590986794 RT @SkepticalDrew: #AntifaDomesticTerrorists Are also international terrorists @realDonaldTrump\n",
      "1613 1590991250 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "6016 1590995664 RT @MSMCali: #ICYMI\n",
      "#Riots \n",
      "#AntifaTerrorists\n",
      "#AntifaHatesPeace \n",
      "#AntifaDomesticTerrorists \n",
      "\n",
      "At the Capitol in #Denver #Colorado\n",
      "\n",
      "Black Ame…\n",
      "2985 1590996079 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "6546 1590999987 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "4563 1591010246 RT @ideadestra_: @AlchemyFeline #Antifa 4EVER SLAUGHTERERS\n",
      "\n",
      "fuck!! \n",
      "\n",
      "#AntifaDomesticTerrorists #AntifaTerrorists https://t.co/aYDLhj7d3n\n",
      "1784 1591014032 RT @quelineruby64: ATTENTION: @ScottMorrisonMP \n",
      "\n",
      "The radical left joined by #AntifaDomesticTerrorists will hijack #AboriginalLivesMatter as…\n",
      "7143 1591015195 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "3130 1591017746 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "7801 1591017800 RT @agrawal_nishith: #AntifaTerrorist\n",
      "#AntifaDomesticTerrorists \n",
      "#leftistterorrist\n",
      "#communistterrorist \n",
      "#communistvirus \n",
      "#leftistvirus \n",
      "#bi…\n",
      "6833 1591022050 RT @MSMCali: #ICYMI\n",
      "#Riots \n",
      "#AntifaTerrorists\n",
      "#AntifaHatesPeace \n",
      "#AntifaDomesticTerrorists \n",
      "\n",
      "At the Capitol in #Denver #Colorado\n",
      "\n",
      "Black Ame…\n",
      "2623 1591033548 RT @WellingMichael: Welcome to #TargetLooting ...#GeorgeFloydProtests or #AntifaDomesticTerrorists ? \n",
      " https://t.co/JvkUYQ6tjI\n",
      "4316 1591049850 RT @dma4him: #AntifaTerrorists destroying the belongings of this homeless man....\n",
      "#SorosFundedRiots #AntifaDomesticTerrorists #ThesePeopleA…\n",
      "177 1591099819 RT @KarluskaP: Lara Logan exposes Antifa this morning- #TheMoreYouKnow #AntifaDomesticTerrorists https://t.co/Ob3QBcD6RU\n",
      "4580 1591102708 RT @KarluskaP: Lara Logan exposes Antifa this morning- #TheMoreYouKnow #AntifaDomesticTerrorists https://t.co/Ob3QBcD6RU\n",
      "6445 1591105314 #LootingIsNotProtesting #AntifaDomesticTerrorists #USAProtests https://t.co/1DMFqLkHB0\n",
      "6445 1591111899 I primi arresti. \n",
      "#USAProtest #AntifaDomesticTerrorists https://t.co/4m9iahDVX7\n",
      "7165 1591123023 RT @AcuarelaAriana: What else is not enought?? #AntifaDomesticTerrorists #GeorgeFloydProtests #LootingIsNotProtesting https://t.co/ur1oUdDe…\n"
     ]
    }
   ],
   "source": [
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "for k, c in cascade_dict.items():\n",
    "    print(k)\n",
    "    for u, ts, w in zip(c[\"user\"], c[\"ts\"], c[\"content\"]):\n",
    "        print(u, ts, w)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971\n",
      "737162\n"
     ]
    }
   ],
   "source": [
    "# 查看Weibo数据集的推文数据量级\n",
    "# 总共73w条推文, 不过用户总量是4971, 按照每个用户粒度降采样应该能达到不错的覆盖率\n",
    "\n",
    "cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/cascades.data\")\n",
    "\n",
    "user_tweets_dict = {}\n",
    "for tag, cascades in cascade_dict.items():\n",
    "    for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['word'], cascades['label']):\n",
    "        if user not in user_tweets_dict:\n",
    "            user_tweets_dict[user] = []\n",
    "        user_tweets_dict[user].append({\n",
    "            \"tag\": tag,\n",
    "            \"bertopic_label\": str(bertopic_label),\n",
    "            \"user\": user,\n",
    "            \"ts\": ts,\n",
    "            \"text\": tweet,\n",
    "        })\n",
    "\n",
    "print(len(user_tweets_dict))\n",
    "print(sum([len(tweets) for tweets in user_tweets_dict.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对Weibo数据集进行进一步地文本长度筛选\n",
    "# 1. 文本长度过短 < 10\n",
    "# 2. 包含\"转发微博\"等字样, 后续进行剪枝处理\n",
    "\n",
    "len_dist = []\n",
    "user_tweets_dict_cp = user_tweets_dict.copy()\n",
    "for user, tweets in user_tweets_dict_cp.items():\n",
    "    user_tweets_dict[user] = [tweet for tweet in tweets if len(tweet['text']) >= 10 and \"转发微博\" not in tweet['text']]\n",
    "    len_dist.append(len(user_tweets_dict[user]))\n",
    "    if len(user_tweets_dict[user]) == 0:\n",
    "        user_tweets_dict.pop(user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "7.0\n",
      "14.0\n",
      "21.0\n",
      "27.0\n",
      "34.0\n",
      "42.0\n",
      "52.0\n",
      "64.0\n",
      "80.0\n"
     ]
    }
   ],
   "source": [
    "def analyse_distribution(data):\n",
    "    for i in range(10):\n",
    "        t = np.percentile(data, i*10)\n",
    "        print(t)\n",
    "        # print(datetime.datetime.utcfromtimestamp(t * time_diff))\n",
    "\n",
    "analyse_distribution(len_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 降采样\n",
    "# # 2. 找同类项传播话题类别标签\n",
    "\n",
    "# cascade_dict = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\")\n",
    "\n",
    "# user_tweets_dict = {}\n",
    "# for tag, cascades in cascade_dict.items():\n",
    "#     for user, ts, tweet, bertopic_label in zip(cascades['user'], cascades['ts'], cascades['content'], cascades['label']):\n",
    "#         if user not in user_tweets_dict:\n",
    "#             user_tweets_dict[user] = []\n",
    "#         user_tweets_dict[user].append({\n",
    "#             \"tag\": tag,\n",
    "#             \"bertopic_label\": str(bertopic_label),\n",
    "#             \"user\": user,\n",
    "#             \"ts\": ts,\n",
    "#             \"text\": tweet,\n",
    "#         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4895\n",
      "2000\n",
      "18710\n",
      "63932\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# down sampling\n",
    "# sampled_users = list(user_tweets.keys())\n",
    "# print(len(sampled_users))\n",
    "# print(len(user_tweets_dict))\n",
    "\n",
    "# user_seeds = list(set((user_tweets_dict.keys())) - set(sampled_users))\n",
    "user_seeds = list(user_tweets_dict.keys())\n",
    "print(len(user_seeds))\n",
    "user_seeds = random.sample(user_seeds, k=2000)\n",
    "print(len(user_seeds))\n",
    "\n",
    "user_tweets_dict_sample = {}\n",
    "user_tweets_dict_remain = {}\n",
    "for us in user_seeds:\n",
    "    unique_ = 0\n",
    "    for info in user_tweets_dict[us]:\n",
    "        info[\"unique_label\"] = unique_\n",
    "        unique_ += 1\n",
    "    user_tweets_dict_sample[us] = random.sample(user_tweets_dict[us], k=min(len(user_tweets_dict[us]), 10))\n",
    "    selected_unique_labels = []\n",
    "    for info in user_tweets_dict_sample[us]:\n",
    "        selected_unique_labels.append(info[\"unique_label\"])\n",
    "    user_tweets_dict_remain[us] = []\n",
    "    for info in user_tweets_dict[us]:\n",
    "        if info[\"unique_label\"] not in selected_unique_labels:\n",
    "            user_tweets_dict_remain[us].append(info)\n",
    "\n",
    "print(sum([len(v) for k, v in user_tweets_dict_sample.items()]))\n",
    "print(sum([len(v) for k, v in user_tweets_dict_remain.items()]))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/input_sample.jsonl\"\n",
    "\n",
    "def save_(filepath, data_dict):\n",
    "    with open(filepath, 'w') as f:\n",
    "        # for user, infos in user_tweets_dict.items():\n",
    "        for user, infos in data_dict.items():\n",
    "            for info in infos:\n",
    "                result = {\n",
    "                    \"meta_info\": {\n",
    "                        \"tag\": info[\"tag\"],\n",
    "                        \"bertopic_label\": info[\"bertopic_label\"],\n",
    "                        \"user\": info[\"user\"],\n",
    "                        \"ts\": info[\"ts\"],\n",
    "                    },\n",
    "                    \"text\": info[\"text\"],\n",
    "                }\n",
    "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/input_sample.jsonl\"\n",
    "remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/input_sample_remain.jsonl\"\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample.jsonl\"\n",
    "# remain_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample_remain.jsonl\"\n",
    "save_(save_filepath, user_tweets_dict_sample)\n",
    "save_(remain_filepath, user_tweets_dict_remain)\n",
    "\n",
    "def analyse_distribution(data):\n",
    "    for i in range(10):\n",
    "        print(np.percentile(data, i*10))\n",
    "\n",
    "# analyse_distribution([len(v) for k, v in user_tweets_dict.items()])\n",
    "# analyse_distribution([len(v) for k, v in user_tweets_dict_sample.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# read from down sampling results\n",
    "# apply sudo label assignment\n",
    "\n",
    "import regex\n",
    "import json\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/input_sample.jsonl\"\n",
    "# down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1/generation_1.jsonl\"\n",
    "down_sampling_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "regex_pattern = r\"\\[\\d+\\] ([\\w\\s]+)\"\n",
    "\n",
    "def read_from_topicgpt(filepath):\n",
    "    user_tweets = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            text = info[\"text\"]\n",
    "            mt_info = info[\"meta_info\"]\n",
    "            topic = info[\"responses\"]\n",
    "            topic = regex.compile(regex_pattern).findall(topic)\n",
    "            if len(topic) > 0: topic = topic[0]\n",
    "            else: topic = \"None\"\n",
    "            if mt_info[\"user\"] not in user_tweets: user_tweets[mt_info[\"user\"]] = []\n",
    "            user_tweets[mt_info[\"user\"]].append({\n",
    "                \"user\": mt_info[\"user\"],\n",
    "                \"ts\": mt_info[\"ts\"],\n",
    "                \"text\": text,\n",
    "                \"tag\": mt_info[\"tag\"],\n",
    "                \"bertopic_label\": mt_info[\"bertopic_label\"],\n",
    "                \"topicgpt_label\": topic,\n",
    "            })\n",
    "    return user_tweets\n",
    "\n",
    "user_tweets = read_from_topicgpt(down_sampling_filepath)\n",
    "print(len(user_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "# # find unsupervised infos\n",
    "# unsupervised_infos = {}\n",
    "\n",
    "# def is_in_list(item, list, return_elem=False):\n",
    "#     flag = False\n",
    "#     elem = None\n",
    "#     for e in list:\n",
    "#         if e[\"user\"] == item[\"user\"] and \\\n",
    "#             e[\"ts\"] == item[\"ts\"] and \\\n",
    "#             e[\"text\"] == item[\"text\"] and \\\n",
    "#             e[\"tag\"] == item[\"tag\"] and \\\n",
    "#             e[\"bertopic_label\"] == item[\"bertopic_label\"]:\n",
    "#             flag = True\n",
    "#             elem = e\n",
    "#         if flag: break\n",
    "#     if not return_elem: return flag\n",
    "#     else: return elem\n",
    "\n",
    "# for user, infos in user_tweets.items():\n",
    "#     full_infos = user_tweets_dict[user]\n",
    "#     # print(len(full_infos))\n",
    "#     unsampled_infos = []\n",
    "#     for info in full_infos:\n",
    "#         if is_in_list(info, infos): continue\n",
    "#         unsampled_infos.append(info)\n",
    "#     unsupervised_infos[user] = unsampled_infos\n",
    "\n",
    "# # analyse_distribution([len(v) for k, v in unsupervised_infos.items()])\n",
    "# print(len(user_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2946\n",
      "1810\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# filepath = \"/root/pyHeter-GAT/preprocess-llm/topicGPT/script/generation_1_tmp.jsonl\"\n",
    "# results = []\n",
    "# with open(filepath, 'r') as f:\n",
    "#     for idx, line in enumerate(f):\n",
    "#         if idx < 7016: continue\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# print(len(results))\n",
    "\n",
    "# # remove duplicates\n",
    "# addition = []\n",
    "# for res in results:\n",
    "#     flag = False\n",
    "#     for elem in addition:\n",
    "#         if elem[\"doc\"] == res[\"doc\"] and \\\n",
    "#             elem[\"response\"] == res[\"response\"] and \\\n",
    "#             elem[\"seed_topics\"] == res[\"seed_topics\"]:\n",
    "#             flag = True\n",
    "#             break\n",
    "#     if not flag:\n",
    "#         addition.append(res)\n",
    "\n",
    "# print(len(addition))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2832"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Use SBert to assign sudo labels\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# sbert = SentenceTransformer(\"/remote-home/share/dmb_nas/wangzejian/LLM_GNN/all-MiniLM-L6-v2\")\n",
    "\n",
    "# sudo_user_infos_mp = {}\n",
    "\n",
    "# for user, infos in user_tweets.items():\n",
    "#     sudo_user_infos_mp[user] = []\n",
    "\n",
    "#     supervised_texts = [info[\"text\"] for info in infos]\n",
    "#     supervised_embs = sbert.encode(supervised_texts, convert_to_tensor=True)\n",
    "\n",
    "#     unsupervised_texts = [info[\"text\"] for info in unsupervised_infos[user]]\n",
    "#     if len(unsupervised_texts) == 0: continue\n",
    "#     unsupervised_embs = sbert.encode(unsupervised_texts, convert_to_tensor=True)\n",
    "    \n",
    "#     cosine_scores = util.cos_sim(supervised_embs, unsupervised_embs).cpu().transpose(0, 1)\n",
    "#     for i, info in enumerate(unsupervised_infos[user]):\n",
    "#         # print(cosine_scores[i].max())\n",
    "#         if cosine_scores[i].max() < 0.4: continue\n",
    "#         sudo_user_infos_mp[user].append({\n",
    "#             \"user\": info[\"user\"],\n",
    "#             \"ts\": info[\"ts\"],\n",
    "#             \"text\": info[\"text\"],\n",
    "#             \"tag\": info[\"tag\"],\n",
    "#             \"bertopic_label\": info[\"bertopic_label\"],\n",
    "#             # \"topicgpt_label\": info[\"topicgpt_label\"],\n",
    "#             \"sudo_label\": infos[cosine_scores[i].argmax()][\"topicgpt_label\"],\n",
    "#         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568256 986 5464 4337\n"
     ]
    }
   ],
   "source": [
    "# total_tweet_num = sum([len(v) for k, v in user_tweets_dict.items()])\n",
    "# supervised_tweet_num = sum([len(v) for k, v in user_tweets.items()])\n",
    "# unsupervised_tweet_num = sum([len(v) for k, v in unsupervised_infos.items()])\n",
    "\n",
    "# a = sum([len(v) for k, v in sudo_user_infos_mp.items()])\n",
    "# print(total_tweet_num, supervised_tweet_num, unsupervised_tweet_num, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Travel\n",
      "RT @Kosalendradas: An ingenious clock in #Sanskrit which with every hour reminds of 12 fundamentals of ancient #Vedic philosophy arranged a… RT @Kosalendradas: An ingenious clock in #Sanskrit which with every hour reminds of 12 fundamentals of ancient #Vedic philosophy arranged a…\n",
      "Religion International Relations\n",
      "RT @ipsnaithani: #Divinity from #Devprayag \n",
      "#Uttarakhand My #homestate \n",
      "\n",
      "~\"Hope &amp; Optimism\"  \n",
      "for people to heal &amp; return from #Quarantines… RT @ipsnaithani: #Divinity from #Devprayag \n",
      "#Uttarakhand My #homestate \n",
      "\n",
      "~\"Hope &amp; Optimism\"  \n",
      "for people to heal &amp; return from #Quarantines…\n",
      "4335 0 4337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test Coverage\n",
    "\n",
    "# newer_topicgpt_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm/generation_1_6991/generation_1.jsonl\"\n",
    "# valid_user_tweets_mp = read_from_topicgpt(newer_topicgpt_filepath)\n",
    "\n",
    "# def test_coverage(user_tweets_mp, sudo_user_infos_mp):\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     missing = 0\n",
    "#     for user, infos in sudo_user_infos_mp.items():\n",
    "#         for info in infos:\n",
    "#             total += 1\n",
    "#             ret = is_in_list(info, user_tweets_mp[user], return_elem=True)\n",
    "#             if ret is None:\n",
    "#                 missing += 1\n",
    "#                 continue\n",
    "#             print(info[\"sudo_label\"], ret[\"topicgpt_label\"])\n",
    "#             print(info[\"text\"], ret[\"text\"])\n",
    "#             if info[\"sudo_label\"] == ret[\"topicgpt_label\"]:\n",
    "#                 correct += 1\n",
    "#     print(missing, correct, total)\n",
    "#     return correct / total\n",
    "\n",
    "# test_coverage(valid_user_tweets_mp, sudo_user_infos_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Use trained bertopic model to assign sudo labels\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "# from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# # Get labeled data\n",
    "# data = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "# docs = data['data']\n",
    "# y = data['target']\n",
    "\n",
    "# # Skip over dimensionality reduction, replace cluster model with classifier,\n",
    "# # and reduce frequent words while we are at it.\n",
    "# empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "# clf = LogisticRegression()\n",
    "# ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# # Create a fully supervised BERTopic instance\n",
    "# topic_model= BERTopic(\n",
    "#     umap_model=empty_dimensionality_model,\n",
    "#     hdbscan_model=clf,\n",
    "#     ctfidf_model=ctfidf_model\n",
    "# )\n",
    "# topics, probs = topic_model.fit_transform(docs, y=y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
